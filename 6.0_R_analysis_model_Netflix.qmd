---
title: "Netflix Data"
author: "Shane Corrie"
format: html
editor: visual
---

```{r}
library(tidyverse)
library(scales)
library(ggplot2)
```

Import data

```{r}


# Set the file path to your CSV file
file_path <- "netflix_data.csv"

# Import the CSV file into a data frame
netflix_data <- read.csv(file_path)

# Check the structure of the imported data frame
str(netflix_data)

# Optionally, view the first few rows of the data frame
head(netflix_data)

```
Quick looks at data

```{r}

```

```{r}

netflix_data |>
  summarize(min(release_year), max(release_year))


```

```{r}

netflix_data |>
  count(rating, sort = TRUE)
```

CLEANING 

need to clean the duration column, we separate on spaces to split the number from the "seasons", "season' or"mins" descriptor. We include convert = true to convert the result to a number

also going to add the "mature" feature here so we can use it later

```{r}

library(lubridate)

netflix_data <- netflix_data |>
  # split the duration column, isolate the string and the number
  separate(duration, c("duration", "duration_units"), sep = " ", convert = TRUE) |>
  # replace "season", used exclusively for tv shows with 1 season, with "seasons" to avoid dupes later
  # add "mature" feature for later use
  mutate(duration_units = str_replace(duration_units, "season", "seasons"),
         mature = rating %in% c("TV-MA", "R", "NC-17"))

```

want to reformat the date_added column for future use

```{r}
netflix_data$date_added <- as.Date(netflix_data$date_added, format = "%d-%b-%y")

# Print the first few rows to verify the changes
head(netflix_data)
```

adding a year added column for later use

```{r}
netflix_data <- netflix_data |>
  mutate(year_added = year(netflix_data$date_added))
```

Analyze / Visualize Data

Content Sources

```{r}

netflix_data |>
  count(release_year)

netflix_data |>
  count(year_added)

```


```{r}

# Plot histogram using ggplot2
ggplot(netflix_data, aes(x = release_year, fill = type)) + 
  geom_histogram(binwidth = 5, position = "dodge", color = "black") +
  facet_wrap(~ type, ncol = 2) +
  labs(title = "Release Dates of Netflix Content",
       x = "Release Year",
       y = "Frequency",
       fill = "Type")


#add a cutoff date; no content in dataset with date_added subsequent to 2021
ggplot(netflix_data, aes(x = pmin(year(date_added), 2022), fill = type)) + 
  geom_histogram(binwidth = 1, position = "dodge", color = "black") +
  facet_wrap(~ type, ncol = 2) +
  labs(title = "Date Content Added to Netflix",
       x = "Year Added",
       y = "Frequency",
       fill = "Type")

```
The netflix content is skewed towards being modern, the oldest release date is 1925 but most content has a release date after 1980. Netflix has a greater number of older movies among its offerings than older tv_shows. 


The Date Content Added to Netflix visual looks consistent with expectations that Netflix started streaming movies around 2008 and tv a few years later. Our data appears to have been collected in 2021, and per Kaggle the data is "as of mid-2021." 
```{r}



```

Are movies getting longer over time?

```{r}
netflix_data |>
  filter(type == "Movie") |>
  mutate(decade = 10 * (release_year %/% 10)) |>
  ggplot(aes(decade, duration, group = decade)) + 
  geom_boxplot()
```

Movies are not getting longer over time, rather they have been coming in at around 100 minutes long since the 70's. In fact it would seem that the 60's was the time of epic movies.

Taking a look at duration by Genre, first movies then TV Shows

```{r}

netflix_data |>
  separate_rows(listed_in, sep = ",") |>
  mutate(listed_in = str_trim(listed_in),  # Trim whitespace; was getting duplicates prior to trimming
         listed_in = str_to_title(listed_in)) |>
  group_by(type, genre = listed_in) |>
  summarize(n = n(), mean_duration = mean(duration)) |>
  arrange(desc(n)) |>
  filter(type=="Movie") |>
  filter(genre != "Movies") |>
  mutate(genre = fct_reorder(genre, mean_duration)) |>
  ggplot(aes(mean_duration, genre)) +
  geom_col()
  
```
Classic movies are the longest on average, this would make sense given the age of epic long movies released in the 60's. Stand up and children's shows being on the short side makes sense too. I was surprised to see the documentaries genre with a duration much shorter than average, those documentaries can feel quite long sometimes


create a summarize_titles function to automate creation of mean_year and mean_duration (done manually above w movies)

```{r}

summarize_titles <- function(tbl) {
  tbl |>
#    group_by(type, genre = listed_in) |>
    summarize(n = n(), 
            mean_duration = mean(duration),
            mean_year = mean(release_year)) |>
  arrange(desc(n))
}
```

```{r}
# TV shows here, movies is above
netflix_data |>
  separate_rows(listed_in, sep = ",") |>
  mutate(listed_in = str_trim(listed_in),  
         listed_in = str_to_title(listed_in)) |>
  group_by(type, genre = listed_in) |>
  summarize_titles() |>
  filter(type=="TV Show") |>
  filter(genre != "Movies") |>
  mutate(genre = fct_reorder(genre, mean_duration)) |>
  ggplot(aes(mean_duration, genre)) +
  geom_col()
```
In TV Shows the mean duration is represented in seasons, not minutes like with movies above. 


Date added

```{r}
netflix_data |>
  filter(!is.na(date_added)) |>
  arrange(date_added) |>
  select(type, title, date_added)
  
```

```{r}
# are additions changing in type over time
netflix_data |>
  filter(!is.na(date_added)) |>
  mutate(year_added = pmax(year(date_added), 2015)) |>
  count(year_added, type) |>
  ggplot(aes(year_added, n, fill= type)) +
  geom_area()


```

Content and country

```{r}
# Contributions to Netflix movies and shows by country

netflix_data |>
  filter(!is.na(country)) |>
  count(country = fct_lump(country,15), type, sort =TRUE) |>
  # fct_reorder to return country sorted 
  ggplot(aes(n, fct_reorder(country, n), fill = type)) +
  geom_col()

```

```{r}
#what is the average length of a movie in each country

netflix_data |>
  filter(!is.na(country)) |>
  filter(!is.na(duration)) |>
  filter(type == "Movie") |>
  group_by(country) |>
  summarize_titles()



result_table <- netflix_data |>
  filter(!is.na(country)) |>
  filter(type == "Movie") |>
  filter(country != "") |>
  filter(!is.na(duration)) |>
  group_by(country) |>
  filter(n() > 75) |> 
  summarize_titles() 
  

plot_country_mean_duration <- ggplot(result_table, aes(x = country, y = mean_duration)) +
  geom_bar(stat = "identity", fill = "orange") +
  labs(title = "Mean Duration by Country",
       x = "Country",
       y = "Mean Duration") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

plot_country_mean_duration
```

compare mean with median

```{r}
netflix_data |>
  filter(!is.na(country)) |>
  filter(type == "Movie") |>
  group_by(country) |>
  summarize(n = n(), 
            median_duration = median(duration),
            median_year = median(release_year)) |>
  arrange(desc(n))



```

RATING

where do the most "R", "NC-17" or "TV-MA" movies and shows get produced

```{r}

library(knitr)
library(dplyr)

netflix_data |>
  filter(rating %in% c("R", "TV-MA", "NC-17")) |>
  filter(country != "") |>
  count(country, sort =TRUE) |> 
  kable(col.width = "1cm")  # Adjust the column width as needed
```

is the percentage of movies and shows with certain rating types changing over time

```{r}
netflix_data %>%
  mutate(year_added = pmax(year(date_added), 2015)) |>
  filter(!is.na(date_added)) |>
  filter(!is.na(rating)) |>
  # rating data gets a little erratic after 2021 
  filter(year_added <= 2021) |>
  group_by(type) |>
  mutate(rating = fct_lump(rating, 4))  |>
  count(type, year_added, rating) |>
  group_by(type, year_added) |>
  mutate(percent = n / sum(n)) |>
  ggplot(aes(year_added, percent, fill = rating)) +
  geom_area() +
  facet_wrap(~type)
# U.S. movies get R and PG-13 ratings while foreign movies are rated with the same system as shows
```

It looks as if Netflix pivoted around 2017 from adding depth to its TV-14 category and more recently began expanding its TV-MA offerings. Similarly, in movies, rated R movies are increasingly among the content.

```{r}

library(dplyr)
library(ggplot2)

# Filter the data for movies and remove missing values
movies_data <- netflix_data |>
  filter(type == "Movie" & country != "" & !is.na(country) & !is.na(rating))

# Count the number of movies for each country
country_counts <- movies_data |>
  count(country, sort = TRUE)

# Select the top x countries based on the count of movies
top_countries <- country_counts |>
  slice_head(n = 10)

# Filter the data for the selected top countries
selected_movies_data <- movies_data |>
  filter(country %in% top_countries$country)

# Calculate the percentage of movies rated "mature" in each selected country
percentage_mature <- selected_movies_data |>
  group_by(country) |>
  summarize(percent_mature = mean(mature == TRUE) * 100)

# Create the plot
ggplot(percentage_mature, aes(x = country, y = percent_mature)) +
  geom_bar(stat = "identity", fill = "skyblue", width = 0.5) +
  labs(title = "Percentage of Movies Rated 'Mature' by Top 10 Countries",
       x = "Country",
       y = "Percentage of Movies Rated 'Mature'")

```
Interesting to see Spain's content is so likely to be rated as Mature. 

COUNTRY

the visual below tells us a little about Netflix's content sourcing strategy


```{r}

netflix_data |>
  filter(!is.na(country)) |>
  filter(type == "Movie") |>
  group_by(country) |>
  summarize_titles()


netflix_data |>
  filter(!is.na(country)) |>
  filter(country != "") |>
  count(country = fct_lump(country,16),
        type,
        sort = TRUE ) |>
  mutate(country = fct_reorder(country, n)) |>
  ggplot(aes(n, country, fill = type)) +
  geom_col() +
  labs(x = "Titles Released by Country")


```

Our analysis indicates Spanish content is likely to be mature, and Spain is the 7th largest contributor country to Netflix based on total titles contributed. Its a decent sample size, so either Netflix is disproportionately selecting mature Spanish content or Spanish content tends to be on the mature side. India, UK, and Japan also are among top contributors based on total titles. 

EXPLORING THE DESCRIPTIONS

What descriptive words are most associated with TV Shows and Movies



```{r}

library(tidytext)
library(snakecase)

# what are the most common words in description
netflix_data |>
  filter(!is.na(description)) |>
  # splits the text in the desc col into indiv words (tokens) and creates new row for each word, associating it with the corresponding orig row's data.
  unnest_tokens(word, description) |> 
  count(word, sort = TRUE)
  
# filter out filler words
netflix_data |>
  filter(!is.na(description)) |>
  unnest_tokens(word, description) |>
  #applies a preset filter to adjust out boring / less meaningful words
  anti_join(stop_words, by = "word") |>
  # add distinction between words in movies and tv_shows via type
  count(type, word, sort = TRUE)


netflix_data |>
  filter(!is.na(description)) |>
  unnest_tokens(word, description) |>
  anti_join(stop_words, by = "word") |>
  count(type, word, sort = TRUE) |>
  mutate(type = to_snake_case(type)) |>
  # add two additional columns: movie and tv_show, where the count of words for each word-type combination is populated
  spread(type, n, fill = 0) |>
  # add a total column to sort on, the sum of words mentioned in movies and tv
  mutate(total = movie + tv_show) |>
  arrange(desc(total))
  
  # Visualize: what words are more common in tv_shows vs movies
  netflix_data |>
  filter(!is.na(description)) |>
  unnest_tokens(word, description) |>
  anti_join(stop_words, by = "word") |>
  count(type, word, sort = TRUE) |>
  mutate(type = to_snake_case(type)) |>
  spread(type, n, fill = 0) |>
  mutate(total = movie + tv_show) |>
  arrange(desc(total)) |>
  # take the top x words
  head(45) |>
  ggplot(aes(movie, tv_show)) + 
  geom_point() + 
  # apply word as the point
  geom_text(aes(label = word), vjust = 1, hjust = 1) + 
  scale_x_log10() +
  scale_y_log10()
  
```

The above looks more useful to determine words that are often in both movies and tv_shows. In the example above: "life" is common to both, "series" is common to tv_show, and "Documentary" is common to movies. 

Maybe a better way to look at what words are more common in movies vs tv_shows is to see what is the ratio in which a word is mentioned, as shown below. Essentially we want to know is a particular word over-represented within the description of tv_shows as compared to the descriptions of movies, and vice versa. We will use bind_log_odds from the tidylo library to bind the log odds to our terms.


```{r}

library(tidylo)

# what words are more common in tv_shows vs movies
words_unnested <- netflix_data |>
filter(!is.na(description)) |>
# parses descriptions and returns tokenized words 
unnest_tokens(word, description) |>
#applies a preset filter to adjust out boring / less meaningful words
anti_join(stop_words, by = "word")
  
words_unnested |>
count(type, word, sort = TRUE) |>
mutate(type = to_snake_case(type)) |>
spread(type, n, fill = 0) |>
mutate(total = movie + tv_show) |>
arrange(desc(total)) |>
head(100) |>
ggplot(aes(movie, tv_show)) + 
geom_point() + 
geom_text(aes(label = word), vjust = 1, hjust = 1) + 
scale_x_log10() +
scale_y_log10()
  
# what is the relative ratio of the words
words_unnested |>
  count(type, word) |>
  bind_log_odds(type, word, n) |>
  arrange(desc(log_odds_weighted))

# to visualize the above
# this should be the words that are over-represented in movies vs tv_shows and vice versa
words_unnested |>
  count(type, word) |>
  bind_log_odds(type, word, n) |>
  arrange(desc(log_odds_weighted)) |>
  group_by(type) |>
  top_n(10, log_odds_weighted) |>
  ungroup() |>
  mutate(word = fct_reorder(word, log_odds_weighted)) |>
  ggplot(aes(log_odds_weighted, word)) + 
  geom_col() + 
  facet_wrap(~ type, scales = "free_y")



```

What words tend to appear with other words (using the widyr package)

```{r}
library(widyr)

#returning words that are often found together
words_unnested |>
  distinct(type, title, word) |>
  # remove the really rare words
  add_count(word, name = "word_total") |>
  # add words that appear in at least 20 titles
  filter(word_total >= 20) |>
  # what words appear next to each other
  pairwise_cor(word, title, sort = TRUE)


# adjust the corr threshold for inclusion
words_unnested |>
  distinct(type, title, word) |>
  # remove the really rare words
  add_count(word, name = "word_total") |>
  # add words that appear in at least 20 titles
  filter(word_total >= 20) |>
  # what words appear next to each other
  pairwise_cor(word, title, sort = TRUE) |>
  # increase the corr threshold for a word being included
  filter(correlation >= .2)
  
  
# words related to a specific word
words_unnested |>
  distinct(type, title, word) |>
  # remove the really rare words
  add_count(word, name = "word_total") |>
  # add words that appear in at least 20 titles
  filter(word_total >= 20) |>
  # what words appear next to each other
  pairwise_cor(word, title, sort = TRUE)|>
  # what words appear next to my word of interest
  filter(item1 == "crime")



```

I think the really neat function here is the ability to return words associated with whatever we pick as our word of interest. For example, in the above we return all the words that are most likely to be found next to "crime." We return words like "gangster", "cops", and "corruption". This seems like something we could potentially build a recommendation engine around. 


Are certain keywords associated with certain genres? 

```{r}
words_unnested |>
  # retains only unique combinations 
  distinct(type, title, word, genre = listed_in) |>
  separate_rows(genre, sep = ", ") |>
  # pick the top 9 genres, for display in facet_wrap
  filter(fct_lump(genre, 9) != "Other") |>
  count(genre, word) |>
  # calculates and binds log-odds values for each word within each genre
  bind_log_odds(genre, word, n)

 
#same as above w filter added
words_unnested |>
  distinct(type, title, word, genre = listed_in) |>
  # use only fairly common words, words used across different genres
  add_count(word, name = "word_total") |>
  filter(word_total >= 50) |>
  separate_rows(genre, sep = ", ") |>
  filter(fct_lump(genre, 9) != "Other") |>
  count(genre, word) |>
  bind_log_odds(genre, word, n)





# to visualize 

word_genre_log_odds <- words_unnested |>
  distinct(type, title, word, genre = listed_in) |>
  # use only fairly common words, words used across different genres
  add_count(word, name = "word_total") |>
  filter(word_total >= 50) |>
  separate_rows(genre, sep = ", ") |>
  filter(fct_lump(genre, 9) != "Other") |>
  count(genre, word) |>
  bind_log_odds(genre, word, n)


word_genre_log_odds |>
  group_by(genre) |>
  top_n(10, log_odds_weighted) |>
  ungroup() |>
  mutate(word = reorder_within(word, log_odds_weighted, genre)) |>
  ggplot(aes(log_odds_weighted, word)) + 
  geom_col() + 
  facet_wrap(~ genre, scales = "free_y") + 
  scale_y_reordered() +
  labs(x = "words mentioned according to genre")
  
  
  
```
The visual above illustrates words that are over-represented in a particular genre. We still can see for example, "evil" is pretty common among several of the genres we have selected for display here, we should note that there are a lot of genres were you won't see "evil" at all, which is why it stands out here. 



Regression

can we predict the probability that something is mature from the description?

```{r}
library(glmnet)
library(broom)

# create a label column
word_ratings <- words_unnested |> 
  count(type, title, rating, word) |>
  filter(!is.na(rating)) |>
  mutate(mature = rating %in% c("TV-MA", "R", "NC-17")) |>
  add_count(word, name = "word_total") |>
  filter(word_total >= 30)

word_matrix <- word_ratings |>
  cast_sparse(title, word, n)

# the thing we will predict
y <- word_ratings$mature[match(rownames(word_matrix), word_ratings$title)]

#predict w cross validation

mod <- cv.glmnet(word_matrix, y, family = "binomial")
plot(mod)

# a visual of description words that would push the label in one direction or the other
mod$glmnet.fit |>
  #pass our fitted model to broom's tidy function
  tidy() |>
  # set a lambda to push our error down, we are being more selective with our terms here 
  filter(lambda == mod$lambda.1se) |>
  # take the top 30 for our visual
  top_n(30, abs(estimate)) |>
  # to arrange the visual in desc order
  mutate(term = fct_reorder(term, estimate)) |>
  ggplot(aes(estimate, term)) + 
  geom_col()
```
Above we illustrate which words from the descriptions of the shows/movies are the most effective predictors of mature content, which words are either adding to or reducing from the likelihood of the content being rated mature.

Maybe we want to check if other features, besides words in the description, are predictive of content being mature or not. We could add other features such as title, director, cast, or genre. 

```{r}
# to grab some other features for inclusion with our model
other_features <- netflix_data |>
  select(title, director, cast, genre = listed_in) |>
  # Reshape the data from wide to long format using gather()
  # This gathers the selected columns (director, cast, genre) into two new columns: feature_type and feature
  gather(feature_type, feature, director, cast, genre) |>
  # Filter out rows where the feature (e.g., director, cast, genre) is NA
  filter(!is.na(feature)) |>
  # Split the feature column by separating each entry based on the separator ", "
  separate_rows(feature, sep = ", ") |>
  # Convert feature_type to title case (e.g., "director" becomes "Director"); avoid dupes
  mutate(feature_type = str_to_title(feature_type)) |>
  # Combine feature_type and feature into a single column separated by ": ", e.g. Director: Spielberg
  # to create a new column that contains both the type of feature (feature_type) and the specific feature itself (feature), separated by ": ".
  # For example, if feature_type contains values like "director", "cast", or "genre", and feature contains the specific names or titles associated with those types (e.g., the names of directors, cast members, or genres), then uniting them like this would result in entries like "director: Christopher Nolan", "cast: Leonardo DiCaprio", or "genre: Action".
  unite(feature, feature_type, feature, sep = ": ") |>
  # Count the occurrences of each feature and add it as a new column named feature_count
  add_count(feature, name = "feature_count") |>
   # Filter out rows where feature_count is less than 10
  filter(feature_count >= 10)

# Create a feature matrix by combining word ratings and other features
feature_matrix <- word_ratings |>                                 # Pipe operator (%>%) is used to pass the result of the previous line to the next function
   mutate(feature = paste("Description:", word)) |>               # Add a prefix "Description:" to each word and create a new column named 'feature'
   bind_rows(other_features) |>                                   # Combine 'word_ratings' and 'other_features' data frames row-wise
   cast_sparse(title, feature)                                    # Convert the data frame to a sparse matrix format based on 'title' and 'feature'

# Define the target variable (thing we will predict)
# The target variable 'y' indicates whether a movie is rated as 'mature' or not. 
# It's matched with the rows in 'feature_matrix' based on the movie titles.
y <- netflix_data$mature[match(rownames(feature_matrix), netflix_data$title)]  


# Predict using cross-validation
mod <- cv.glmnet(feature_matrix, y, family = "binomial")          # Fit a logistic regression model using cross-validation

plot(mod)                                                         # Plot the cross-validation results

mod$glmnet.fit |>                                                 # Extract the fitted model object
  tidy() |>                                                       # Convert the model object to a tidy data frame
  filter(lambda == mod$lambda.1se) |>                             # Filter for coefficients at the optimal lambda value
  top_n(30, abs(estimate)) |>                                     # Select the top 30 coefficients based on their absolute values
  mutate(term = fct_reorder(term, estimate)) |>                   # Reorder terms based on their estimates for better visualization
  ggplot(aes(estimate, term)) +                                   # Create a ggplot object with estimates on the x-axis and terms on the y-axis
  geom_col()                                                      # Add a column plot to visualize the coefficients

```
Adding some fill values on feature_type to clean this visual up and make the info easier to view

```{r}

other_features <- netflix_data |>
  # pick our desired features
  select(title, director, cast, genre = listed_in) |>
  # 
  gather(feature_type, feature, director, cast, genre) |>
  filter(!is.na(feature)) |>
  separate_rows(feature, sep = ", ") |>
  mutate(feature_type = str_to_title(feature_type)) |>
  unite(feature, feature_type, feature, sep = ": ") |>
  add_count(feature, name = "feature_count") |>
  filter(feature_count >= 10)

 feature_matrix <- word_ratings |>
   mutate(feature = paste("Description:", word)) |>
   bind_rows(other_features) |>
   cast_sparse(title, feature)

# the thing we will predict
y <- netflix_data$mature[match(rownames(feature_matrix), netflix_data$title)]

#predict w cross validation

mod <- cv.glmnet(feature_matrix, y, family = "binomial")

plot(mod)

mod$glmnet.fit |>
  tidy() |>
  separate(term, c("feature_type", "feature"), sep = ": ") |>
  filter(lambda == mod$lambda.1se) |>
  top_n(30, abs(estimate)) |>
  mutate(feature = fct_reorder(feature, estimate)) |>
  ggplot(aes(estimate, feature, fill = feature_type)) + 
  geom_col()
```

The visual illustrates the best predictors. It is intuitive that genres such as Children & Family Movies, stand-up comedy, and Kids' TV are excellent predictors. 

However, before visualizing it, I would not have guessed that casting would be such a good predictor. Names like David Attenborough and Oprah Winfrey that people instantly recognize as very wholesome are present. In hindsight it does make sense, we might expect Oprah to be reluctant to create content that is not consistent with her wholesome image. 

Summary
 
The Netflix data is great because everyone loves Netflix, and most people are at least semi-familiar with what we are looking at before we even dive in. 

Overall, this data allowed for some neat analysis, we dug into the content created by country and the different durations of movies. We were also able to illustrate which words in the content descriptions were most associated with each genre. We built a function to return a list of words most often associated with any given term we want. We were also able to create a logistic regression model to predict which description words were likely to be attached to mature content. Finally, we included other features in our model alongside the descriptions such as title, director, cast, and genre, to also determine their ability to predict whether content would be mature or not.  

If we had information about how often each title was viewed, that might have been the most interesting data to me.But we did not. In this case, and in my opinion, the verbal description of each title is the most interesting piece of information included in this dataset. It was really fun to be able to work with these descriptions. 