{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e114c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Fetching articles...\n",
      "üìä Creating aggregated dataframe...\n",
      "üîé Fetching price data in single query\n",
      "‚ö†Ô∏è Price data fetch failed: not all arguments converted during string formatting\n",
      "üî† Extracting top keywords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentiment keyword extraction: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 909/909 [00:10<00:00, 84.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Adding time dimensions...\n",
      "üìä Exploding TF-IDF keywords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TF-IDF keyword explosion: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 909/909 [18:38<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Saving outputs...\n",
      "‚úÖ All outputs saved for Tableau üéâ\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from wordcloud import STOPWORDS\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import time\n",
    "\n",
    "# ---------- CONFIG ---------- #\n",
    "DB_PARAMS = \"postgresql+psycopg2://postgres:Ilpmnl!69gg@localhost:5432/twt_snt\"\n",
    "END_DATE = time.strftime('%Y-%m-%d', time.gmtime(time.time()))\n",
    "START_DATE = (datetime.now() - timedelta(days=45)).strftime('%Y-%m-%d')\n",
    "engine = create_engine(DB_PARAMS)\n",
    "\n",
    "# ---------- FUNCTIONS ---------- #\n",
    "def fetch_articles(start_date, end_date, engine):\n",
    "    query = \"\"\"\n",
    "        SELECT title, summary, url, date, pulled_date, published_date, term,\n",
    "               vader_neg, vader_neu, vader_pos, vader_compound,\n",
    "               roberta_neg, roberta_pos\n",
    "        FROM articles_tbl\n",
    "        WHERE date BETWEEN %s AND %s\n",
    "        ORDER BY date DESC;\n",
    "    \"\"\"\n",
    "    df = pd.read_sql(query, engine, params=(start_date, end_date))\n",
    "    df['date'] = pd.to_datetime(df['date'])  # KEEP datetime64[ns]\n",
    "    return df\n",
    "\n",
    "def fetch_price_data(start_date, end_date, terms, engine):\n",
    "    query = \"\"\"\n",
    "        SELECT date, term, open, high, low, close, adj_close, volume,\n",
    "               close_ma_7, close_ma_21, close_ma_50, close_ma_100, close_ma_200\n",
    "        FROM yahoo_price_tbl\n",
    "        WHERE term = ANY(%s) AND date BETWEEN %s AND %s;\n",
    "    \"\"\"\n",
    "    terms_list = terms if isinstance(terms, list) else [terms]\n",
    "    df = pd.read_sql(query, engine, params=(terms_list, start_date, end_date))\n",
    "    df['date'] = pd.to_datetime(df['date'])  # KEEP datetime64[ns]\n",
    "    return df\n",
    "\n",
    "def include_price_data(grouped_df, start_date, end_date, engine):\n",
    "    terms = grouped_df['term'].unique().tolist()\n",
    "    if not terms:\n",
    "        return grouped_df\n",
    "    print(\"üîé Fetching price data in single query\")\n",
    "    try:\n",
    "        price_df = fetch_price_data(start_date, end_date, terms, engine)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Price data fetch failed: {e}\")\n",
    "        return grouped_df\n",
    "    if price_df.empty:\n",
    "        print(\"‚ö†Ô∏è No price data found for any terms.\")\n",
    "        return grouped_df\n",
    "    return grouped_df.merge(price_df, on=['date', 'term'], how='left')\n",
    "\n",
    "def assign_sentiment_labels(df):\n",
    "    conditions = [\n",
    "        df['vader_compound'] > 0.05,\n",
    "        df['vader_compound'] < -0.05\n",
    "    ]\n",
    "    choices = ['Positive', 'Negative']\n",
    "    df['sentiment_label'] = np.select(conditions, choices, default='Neutral')\n",
    "    return df\n",
    "\n",
    "def calculate_article_metrics(df):\n",
    "    df['abs_vader_compound'] = df['vader_compound'].abs()\n",
    "    article_counts = df.groupby(['date', 'term']).size().reset_index(name='article_count')\n",
    "    sentiment_dist = (df.groupby(['date', 'term'])['sentiment_label']\n",
    "                      .value_counts(normalize=True)\n",
    "                      .unstack(fill_value=0)\n",
    "                      .mul(100)\n",
    "                      .reset_index())\n",
    "    sentiment_cols = ['Positive', 'Neutral', 'Negative']\n",
    "    sentiment_dist['majority_sentiment'] = sentiment_dist[sentiment_cols].idxmax(axis=1)\n",
    "    df = df.merge(article_counts, on=['date', 'term'])\n",
    "    df = df.merge(sentiment_dist, on=['date', 'term'])\n",
    "    df[sentiment_cols] = df[sentiment_cols].fillna(0)\n",
    "    df['impact_score'] = df['article_count'] * df['abs_vader_compound']\n",
    "    return df\n",
    "\n",
    "def create_grouped_df(df):\n",
    "    grouped = df.groupby(['date', 'pulled_date', 'term'], as_index=False).agg({\n",
    "        'title': 'first', 'summary': 'first', 'url': 'first',\n",
    "        'vader_compound': 'mean', 'article_count': 'first',\n",
    "        'Positive': 'first', 'Neutral': 'first', 'Negative': 'first',\n",
    "        'abs_vader_compound': 'max', 'impact_score': 'sum',\n",
    "        'vader_neg': 'first', 'vader_neu': 'first', 'vader_pos': 'first',\n",
    "        'roberta_neg': 'first', 'roberta_pos': 'first',\n",
    "        'majority_sentiment': 'first'\n",
    "    })\n",
    "    grouped['sentiment_3d_MA'] = grouped['vader_compound'].rolling(3, min_periods=1).mean()\n",
    "    grouped['sentiment_momentum'] = grouped['vader_compound'] - grouped['sentiment_3d_MA']\n",
    "    grouped['trend_reversal'] = np.where(grouped['sentiment_momentum'].diff() < 0, 'Reversal', 'No Change')\n",
    "    return grouped\n",
    "\n",
    "def perform_clustering(df):\n",
    "    kmeans = KMeans(n_clusters=4, random_state=42, n_init='auto')\n",
    "    features = df[['vader_compound', 'article_count']].fillna(0)\n",
    "    df['sentiment_cluster'] = kmeans.fit_predict(features)\n",
    "    label_map = {\n",
    "        0: \"Low Volume, Positive Sentiment\",\n",
    "        1: \"High Volume, Positive Sentiment\",\n",
    "        2: \"High Volume, Negative Sentiment\",\n",
    "        3: \"Low Volume, Negative Sentiment\"\n",
    "    }\n",
    "    df['cluster_label'] = df['sentiment_cluster'].map(label_map).fillna(\"Mixed Sentiment\")\n",
    "    return df\n",
    "\n",
    "def is_valid_word(word):\n",
    "    return len(word) > 2 and word.lower() not in STOPWORDS and not re.search(r'^\\d+$', word)\n",
    "\n",
    "def extract_sentiment_keywords(df, text_column='combined_text', top_n=5):\n",
    "    sentiments = ['Positive', 'Negative', 'Neutral']\n",
    "    rows = []\n",
    "    grouped = df.groupby(['date', 'term'])\n",
    "    for (date, term), group in tqdm(grouped, desc=\"Sentiment keyword extraction\"):\n",
    "        row = {'date': date, 'term': term}\n",
    "        for sentiment in sentiments:\n",
    "            text = ' '.join(group[group['sentiment_label'] == sentiment][text_column].dropna().astype(str)).lower()\n",
    "            words = [re.sub(r'[^a-zA-Z]', '', w) for w in text.split() if is_valid_word(w)]\n",
    "            top = [w for w, _ in Counter(words).most_common(top_n)]\n",
    "            row[f'top_keywords_{sentiment.lower()}'] = ', '.join(top)\n",
    "        rows.append(row)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def tfidf_keywords_exploded_with_sentiment(df, text_column='combined_text', top_n=10):\n",
    "    rows = []\n",
    "    grouped = df.groupby(['date', 'term'])\n",
    "    for (date, term), group in tqdm(grouped, desc=\"TF-IDF keyword explosion\"):\n",
    "        corpus = group[text_column].dropna().astype(str)\n",
    "        if corpus.empty:\n",
    "            continue\n",
    "        try:\n",
    "            vec = TfidfVectorizer(\n",
    "                stop_words='english',\n",
    "                max_features=1000,\n",
    "                token_pattern=r'(?u)\\b[a-zA-Z][a-zA-Z0-9_-]{2,}\\b'\n",
    "            )\n",
    "            X = vec.fit_transform(corpus)\n",
    "            keywords = vec.get_feature_names_out()\n",
    "            tfidf_scores = X.sum(axis=0).A1\n",
    "            score_df = pd.DataFrame({'keyword': keywords, 'tfidf_score': tfidf_scores})\n",
    "            score_df = score_df[score_df['keyword'].apply(is_valid_word)]\n",
    "\n",
    "            full_text = ' '.join(corpus).lower().split()\n",
    "            word_counts = Counter(full_text)\n",
    "            score_df['keyword_count'] = score_df['keyword'].map(word_counts).fillna(0).astype(int)\n",
    "\n",
    "            sentiments = []\n",
    "            for kw in score_df['keyword']:\n",
    "                pos = group.loc[group['sentiment_label'] == 'Positive', text_column].dropna().str.contains(kw, case=False, na=False).sum()\n",
    "                neg = group.loc[group['sentiment_label'] == 'Negative', text_column].dropna().str.contains(kw, case=False, na=False).sum()\n",
    "                neu = group.loc[group['sentiment_label'] == 'Neutral', text_column].dropna().str.contains(kw, case=False, na=False).sum()\n",
    "                total = max(pos, neg, neu)\n",
    "                sentiments.append('Positive' if total == pos else 'Negative' if total == neg else 'Neutral')\n",
    "\n",
    "            score_df['sentiment_label'] = sentiments\n",
    "            score_df['date'] = pd.to_datetime(date)\n",
    "            score_df['term'] = term\n",
    "            score_df['week'] = score_df['date'].dt.isocalendar().week\n",
    "            score_df['month'] = score_df['date'].dt.to_period('M').astype(str)\n",
    "\n",
    "            rows.append(score_df.sort_values('tfidf_score', ascending=False).head(top_n))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è TF-IDF failed for ({date}, {term}): {str(e)[:100]}\")\n",
    "    return pd.concat(rows, ignore_index=True) if rows else pd.DataFrame()\n",
    "\n",
    "def add_additional_metrics(df):\n",
    "    df['sentiment_disagreement'] = ((df['vader_compound'] > 0) & (df['roberta_neg'] > df['roberta_pos'])) | \\\n",
    "                                   ((df['vader_compound'] < 0) & (df['roberta_pos'] > df['roberta_neg']))\n",
    "    df['sentiment_spike'] = (df['vader_compound'].diff().abs() > 0.5).astype(int)\n",
    "    df['volume_spike'] = (df['article_count'] > df['article_count'].rolling(7).mean() + 2 * df['article_count'].rolling(7).std()).astype(int)\n",
    "    df['vader_day_change'] = df['vader_compound'].diff()\n",
    "    df['impact_day_change'] = df['impact_score'].diff()\n",
    "    df['sentiment_consistent'] = df['majority_sentiment'].eq(df['majority_sentiment'].shift(1)) & df['majority_sentiment'].eq(df['majority_sentiment'].shift(2))\n",
    "    return df\n",
    "\n",
    "def flag_daily_top_terms(df):\n",
    "    daily_top = df.groupby(['date', 'term'])['impact_score'].sum().reset_index()\n",
    "    daily_top = daily_top.sort_values(['date', 'impact_score'], ascending=[True, False])\n",
    "    daily_top = daily_top.groupby('date').head(10)\n",
    "    daily_top['daily_top10_flag'] = True\n",
    "    return df.merge(daily_top[['date', 'term', 'daily_top10_flag']], on=['date', 'term'], how='left').fillna(False)\n",
    "\n",
    "def merge_most_impactful_article(df, article_df):\n",
    "    impactful = article_df.loc[article_df.groupby(['date', 'term'])['impact_score'].idxmax()]\n",
    "    df = df.merge(impactful[['date', 'term', 'summary', 'url', 'sentiment_label']],\n",
    "                  on=['date', 'term'], how='left', suffixes=('', '_impactful'))\n",
    "    df['impactful_summary_with_url'] = df.apply(\n",
    "        lambda x: f\"{x['summary']}\\n\\n{x['url']}\" if pd.notnull(x['summary']) else \"\", axis=1)\n",
    "    return df.rename(columns={'sentiment_label_impactful': 'impactful_article_sentiment'})\n",
    "\n",
    "# ---------- MAIN ---------- #\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        print(\"üöÄ Fetching articles...\")\n",
    "        articles_df = fetch_articles(START_DATE, END_DATE, engine)\n",
    "        articles_df = assign_sentiment_labels(articles_df)\n",
    "        articles_df = calculate_article_metrics(articles_df)\n",
    "        articles_df['combined_text'] = articles_df['title'].fillna('') + ' ' + \\\n",
    "                                       articles_df['summary'].fillna('') + ' ' + \\\n",
    "                                       articles_df['url'].fillna('')\n",
    "\n",
    "        print(\"üìä Creating aggregated dataframe...\")\n",
    "        grouped_df = create_grouped_df(articles_df)\n",
    "        grouped_df = perform_clustering(grouped_df)\n",
    "        grouped_df = include_price_data(grouped_df, START_DATE, END_DATE, engine)\n",
    "        grouped_df = add_additional_metrics(grouped_df)\n",
    "        grouped_df = flag_daily_top_terms(grouped_df)\n",
    "        grouped_df = merge_most_impactful_article(grouped_df, articles_df)\n",
    "\n",
    "        print(\"üî† Extracting top keywords...\")\n",
    "        keywords_df = extract_sentiment_keywords(articles_df, text_column='combined_text', top_n=5)\n",
    "        grouped_df = grouped_df.merge(keywords_df, on=['date', 'term'], how='left')\n",
    "\n",
    "        print(\"üìÖ Adding time dimensions...\")\n",
    "        grouped_df['week'] = grouped_df['date'].dt.isocalendar().week\n",
    "        grouped_df['month'] = grouped_df['date'].dt.to_period('M').astype(str)\n",
    "        grouped_df['weekday'] = grouped_df['date'].dt.day_name()\n",
    "\n",
    "        print(\"üìä Exploding TF-IDF keywords...\")\n",
    "        exploded_keywords_df = tfidf_keywords_exploded_with_sentiment(articles_df, text_column='combined_text', top_n=10)\n",
    "        exploded_keywords_df.to_csv(\"tfidf_keywords_exploded.csv\", index=False)\n",
    "\n",
    "        print(\"üìÇ Saving outputs...\")\n",
    "        # Convert to ISO string for Tableau compatibility\n",
    "        #articles_df['date'] = articles_df['date'].dt.strftime('%Y-%m-%d')\n",
    "        #grouped_df['date'] = grouped_df['date'].dt.strftime('%Y-%m-%d')\n",
    "        #exploded_keywords_df['date'] = exploded_keywords_df['date'].dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        \n",
    "        grouped_df['term'] = grouped_df['term'].str.strip().str.upper()\n",
    "        articles_df['term'] = articles_df['term'].str.strip().str.upper()\n",
    "        \n",
    "        grouped_df['date'] = pd.to_datetime(grouped_df['date']).dt.date\n",
    "  \n",
    "        articles_df.to_csv(\"article_level_sentiment.csv\", index=False)\n",
    "        grouped_df.to_csv(\"aggregated_news_per_term_per_day.csv\", index=False)\n",
    "\n",
    "        print(\"‚úÖ All outputs saved for Tableau üéâ\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "    finally:\n",
    "        engine.dispose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8fcafc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date                                  object\n",
      "pulled_date                   datetime64[ns]\n",
      "term                                  object\n",
      "title                                 object\n",
      "summary                               object\n",
      "url                                   object\n",
      "vader_compound                       float64\n",
      "article_count                          int64\n",
      "Positive                             float64\n",
      "Neutral                              float64\n",
      "Negative                             float64\n",
      "abs_vader_compound                   float64\n",
      "impact_score                         float64\n",
      "vader_neg                            float64\n",
      "vader_neu                            float64\n",
      "vader_pos                            float64\n",
      "roberta_neg                          float64\n",
      "roberta_pos                          float64\n",
      "majority_sentiment                    object\n",
      "sentiment_3d_MA                      float64\n",
      "sentiment_momentum                   float64\n",
      "trend_reversal                        object\n",
      "sentiment_cluster                      int32\n",
      "cluster_label                         object\n",
      "sentiment_disagreement                  bool\n",
      "sentiment_spike                        int32\n",
      "volume_spike                           int32\n",
      "vader_day_change                      object\n",
      "impact_day_change                     object\n",
      "sentiment_consistent                    bool\n",
      "daily_top10_flag                        bool\n",
      "summary_impactful                     object\n",
      "url_impactful                         object\n",
      "sentiment_label                       object\n",
      "impactful_summary_with_url            object\n",
      "top_keywords_positive                 object\n",
      "top_keywords_negative                 object\n",
      "top_keywords_neutral                  object\n",
      "week                                  UInt32\n",
      "month                                 object\n",
      "weekday                               object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(grouped_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc074e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad820a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c9a950",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
