{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed73a097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Date: 2025-04-23 00:00:00\n",
      "Extended Start Date: 2025-04-02 00:00:00\n",
      "End Date: 2025-06-08 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# âœ… Automatically set `end_date` to today's date\n",
    "end_date = time.strftime('%Y-%m-%d', time.gmtime(time.time()))  # Today's date as a string\n",
    "\n",
    "# âœ… Automatically set `start_date` to 60 days before `end_date`\n",
    "start_date = (datetime.today() - timedelta(days=45)).strftime('%Y-%m-%d')\n",
    "\n",
    "# âœ… Calculate the extended start date for sentiment analysis (21 days before `start_date`)\n",
    "extended_start_date = (pd.to_datetime(start_date) - pd.Timedelta(days=21)).strftime('%Y-%m-%d')\n",
    "\n",
    "# âœ… Convert start, extended start, and end dates to `Timestamp` for consistency\n",
    "start_date = pd.to_datetime(start_date)\n",
    "end_date = pd.to_datetime(end_date)\n",
    "extended_start_date = pd.to_datetime(extended_start_date)\n",
    "\n",
    "# âœ… Debug print to check the generated dates\n",
    "print(f\"Start Date: {start_date}\")\n",
    "print(f\"Extended Start Date: {extended_start_date}\")\n",
    "print(f\"End Date: {end_date}\")\n",
    "\n",
    "\n",
    "\n",
    "grp_terms = ['SOL', 'KAS', 'LINK', 'ADA', 'MATIC', 'AMZN', 'MSFT', 'AVAX', 'AAPL', 'GME', 'NVDA','JPM','DOGE' \n",
    "             'GOOGL','ETH','DXY', 'TSMC', 'CVX', 'COIN', 'POPCAT', 'SUI', 'HNT', 'NFLX', 'WIF', 'DIS', 'BTC', 'TSLA']\n",
    "\n",
    "# Initialize a dictionary to collect DataFrames for each term\n",
    "combined_data_dict = {}\n",
    "# Create dictionaries to store different types of data\n",
    "price_data_dict = {}  # Stores the raw price data\n",
    "technical_indicators_dict = {}  # Stores the extended data with technical indicators\n",
    "filtered_data_dict = {}  # Stores the filtered data after applying the technical indicators\n",
    "\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    'dbname': 'twt_snt',\n",
    "    'user': 'postgres',\n",
    "    'password': 'Ilpmnl!69gg',\n",
    "    'host': 'localhost',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "# Function to fetch tweets from the database\n",
    "def fetch_tweets(start_date, end_date, term):\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_params)\n",
    "        cursor = conn.cursor()\n",
    "        query = \"\"\"\n",
    "            SELECT * FROM twt_tbl\n",
    "            WHERE term = %s AND date BETWEEN %s AND %s\n",
    "        \"\"\"\n",
    "        cursor.execute(query, (term, extended_start_date, end_date))\n",
    "        rows = cursor.fetchall()\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        df = pd.DataFrame(rows, columns=columns)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching tweets: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "# Function to fetch moving averages from the database\n",
    "def fetch_moving_averages(extended_start_date, end_date, term):\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_params)\n",
    "        cursor = conn.cursor()\n",
    "        query = \"\"\"\n",
    "            SELECT date, term, combined_compound_ma_7, combined_compound_ma_21, \n",
    "                   combined_compound_ma_50, combined_compound_ma_100, combined_compound_ma_200, combined_compound\n",
    "            FROM snt_ma_blend_tbl\n",
    "            WHERE term = %s AND date BETWEEN %s AND %s\n",
    "        \"\"\"\n",
    "        cursor.execute(query, (term, extended_start_date, end_date))\n",
    "        rows = cursor.fetchall()\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        df = pd.DataFrame(rows, columns=columns)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching moving averages: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "# Function to fetch data from the database\n",
    "def fetch_price_data(start_date, end_date, term):\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_params)\n",
    "        cursor = conn.cursor()\n",
    "        query = \"\"\"\n",
    "            SELECT date, term, open, high, low, close, adj_close, volume,\n",
    "            close_ma_7, close_ma_21, close_ma_50, close_ma_100, close_ma_200\n",
    "            FROM yahoo_price_tbl\n",
    "            WHERE term = %s AND date BETWEEN %s AND %s\n",
    "        \"\"\"\n",
    "        cursor.execute(query, (term, start_date, end_date))\n",
    "        rows = cursor.fetchall()\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        df = pd.DataFrame(rows, columns=columns)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching price data: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "def fetch_predictions_data(start_date, end_date, term):\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_params)\n",
    "        cursor = conn.cursor()\n",
    "        query = \"\"\"\n",
    "            SELECT prediction_date, term, prediction\n",
    "            FROM predictions_tbl\n",
    "            WHERE term = %s AND prediction_date BETWEEN %s AND %s\n",
    "        \"\"\"\n",
    "        cursor.execute(query, (term, start_date, end_date))\n",
    "        rows = cursor.fetchall()\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        df = pd.DataFrame(rows, columns=columns)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching predictions data: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        \n",
    "def fetch_bollinger_data(start_date, end_date, term):\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_params)\n",
    "        cursor = conn.cursor()\n",
    "        query = \"\"\"\n",
    "            SELECT * FROM bollinger_tbl\n",
    "            WHERE term = %s AND date BETWEEN %s AND %s\n",
    "        \"\"\"\n",
    "        cursor.execute(query, (term, start_date, end_date))\n",
    "        rows = cursor.fetchall()\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        df = pd.DataFrame(rows, columns=columns)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching predictions data: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "\n",
    "def fetch_signals(start_date, end_date, terms):\n",
    "    # Ensure terms is always a list, even if it's a single term\n",
    "    terms = [terms] if isinstance(terms, str) else terms\n",
    "    db_params = {\n",
    "        'dbname': 'twt_snt',\n",
    "        'user': 'postgres',\n",
    "        'password': 'Ilpmnl!69gg',\n",
    "        'host': 'localhost',\n",
    "        'port': '5432'\n",
    "    }\n",
    "    conn = psycopg2.connect(**db_params)\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        query = \"\"\"\n",
    "            SELECT *\n",
    "            FROM signal_cnt_tbl\n",
    "            WHERE term = ANY(%s) AND date BETWEEN %s AND %s\n",
    "        \"\"\"\n",
    "        # Pass terms as a list to form a PostgreSQL array\n",
    "        cursor.execute(query, (terms, start_date, end_date))\n",
    "        rows = cursor.fetchall()\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        df = pd.DataFrame(rows, columns=columns)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching signals: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "# Functions to calculate technical indicators (as given)\n",
    "def calculate_rsi(series, period=14):\n",
    "    delta = series.diff(1)\n",
    "    gain = delta.where(delta > 0, 0.0)\n",
    "    loss = -delta.where(delta < 0, 0.0)\n",
    "    avg_gain = gain.rolling(window=period, min_periods=1).mean()\n",
    "    avg_loss = loss.rolling(window=period, min_periods=1).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def calculate_stochastic_rsi(df, rsi_column, window=14):\n",
    "    rsi_min = df[rsi_column].rolling(window=window, min_periods=1).min()\n",
    "    rsi_max = df[rsi_column].rolling(window=window, min_periods=1).max()\n",
    "    stoch_rsi = (df[rsi_column] - rsi_min) / (rsi_max - rsi_min)\n",
    "    return stoch_rsi * 100\n",
    "\n",
    "def calculate_mfi(df, window=14):\n",
    "    typical_price = (df['high'] + df['low'] + df['close']) / 3\n",
    "    money_flow = typical_price * df['volume']\n",
    "    positive_flow = (money_flow.where(typical_price > typical_price.shift(1), 0)).rolling(window=window).sum()\n",
    "    negative_flow = (money_flow.where(typical_price < typical_price.shift(1), 0)).rolling(window=window).sum()\n",
    "    mfi = 100 - (100 / (1 + positive_flow / negative_flow))\n",
    "    \n",
    "    # Normalize MFI to range [-100, 100]\n",
    "    mfi_normalized = 2 * mfi - 100\n",
    "    return mfi_normalized\n",
    "\n",
    "def calculate_sfi(df, window=14):\n",
    "    # Fill missing volume data with the most recent non-zero or non-null value\n",
    "    df['volume'] = df['volume'].fillna(method='ffill')\n",
    "    \n",
    "    typical_sentiment = df['combined_compound_ma_7']\n",
    "    sentiment_flow = typical_sentiment * df['volume']\n",
    "    positive_flow = (sentiment_flow.where(typical_sentiment > typical_sentiment.shift(1), 0)).rolling(window=window).sum()\n",
    "    negative_flow = (sentiment_flow.where(typical_sentiment < typical_sentiment.shift(1), 0)).rolling(window=window).sum()\n",
    "    sfi = 100 - (100 / (1 + positive_flow / negative_flow))\n",
    "    \n",
    "    # Normalize SFI to range [-100, 100]\n",
    "    sfi_normalized = 2 * sfi - 100\n",
    "    return sfi_normalized\n",
    "\n",
    "def calculate_macd(series, short_window=12, long_window=26, signal_window=9):\n",
    "    short_ema = series.ewm(span=short_window, adjust=False).mean()\n",
    "    long_ema = series.ewm(span=long_window, adjust=False).mean()\n",
    "    macd = short_ema - long_ema\n",
    "    signal = macd.ewm(span=signal_window, adjust=False).mean()\n",
    "    return macd, signal\n",
    "\n",
    "\n",
    "def calculate_bollinger_bands(df, ma_column, window=20, num_std_dev=2, band_type='price'):\n",
    "    df[f'{band_type}_MA'] = df[ma_column]\n",
    "    df[f'{band_type}_STD'] = df[ma_column].rolling(window=window).std()\n",
    "    df[f'{band_type}_Upper_Band'] = df[f'{band_type}_MA'] + (df[f'{band_type}_STD'] * num_std_dev)\n",
    "    df[f'{band_type}_Lower_Band'] = df[f'{band_type}_MA'] - (df[f'{band_type}_STD'] * num_std_dev)\n",
    "    return df\n",
    "\n",
    "def scale_features_to_price(df, columns_to_scale, reference_column):\n",
    "    scaled_columns = {}\n",
    "    for col in columns_to_scale:\n",
    "        scaler = MinMaxScaler(feature_range=(df[reference_column].min(), df[reference_column].max()))\n",
    "        scaled_columns[f'scaled_{col}'] = scaler.fit_transform(df[[col]]).flatten()\n",
    "    \n",
    "    # Convert scaled_columns dictionary to a DataFrame\n",
    "    scaled_df = pd.DataFrame(scaled_columns, index=df.index)\n",
    "    \n",
    "    # Concatenate the original DataFrame with the scaled DataFrame\n",
    "    return pd.concat([df, scaled_df], axis=1)\n",
    "\n",
    "def calculate_boll_upper_advanced(boll_upper_price, boll_lower_sent, boll_upper_sent, boll_lower_price):\n",
    "    if boll_upper_price >= boll_lower_sent and boll_upper_sent >= boll_lower_price:\n",
    "        # When the bands overlap, take the minimum of the upper bounds, but ensure it's above the lower bound\n",
    "        return max(min(boll_upper_price, boll_upper_sent), boll_lower_price)\n",
    "    else:\n",
    "        # When there's no overlap, choose the upper band that is closer to the other band's lower boundary\n",
    "        if abs(boll_upper_price - boll_lower_sent) < abs(boll_upper_sent - boll_lower_price):\n",
    "            return max(boll_upper_price, boll_lower_price)\n",
    "        else:\n",
    "            return max(boll_upper_sent, boll_lower_price)\n",
    "\n",
    "def calculate_boll_lower_advanced(boll_lower_price, boll_upper_sent, boll_lower_sent, boll_upper_price):\n",
    "    if boll_lower_price <= boll_upper_sent and boll_lower_sent <= boll_upper_price:\n",
    "        # When the bands overlap, take the maximum of the lower bounds, but ensure it's below the upper bound\n",
    "        return min(max(boll_lower_price, boll_lower_sent), boll_upper_price)\n",
    "    else:\n",
    "        # When there's no overlap, choose the lower band that is closer to the other band's upper boundary\n",
    "        if abs(boll_lower_price - boll_upper_sent) < abs(boll_lower_sent - boll_upper_price):\n",
    "            return min(boll_lower_price, boll_upper_price)\n",
    "        else:\n",
    "            return min(boll_lower_sent, boll_upper_price)\n",
    "        \n",
    "def normalize_column(df, column):\n",
    "    min_val = df[column].min()\n",
    "    max_val = df[column].max()\n",
    "    return ((df[column] - min_val) / (max_val - min_val)) * 100\n",
    "        \n",
    "    \n",
    "# Function to find divergence\n",
    "def find_MACD_price_divergence(df):\n",
    "    divergence = ['None']  # Start with 'None' for the first row since no comparison can be made\n",
    "    for i in range(1, len(df)):\n",
    "        if df['close'].iloc[i] < df['close'].iloc[i-1] and df['MACD'].iloc[i] > df['MACD'].iloc[i-1]:\n",
    "            divergence.append('Bullish MACD Price Divergence')\n",
    "        elif df['close'].iloc[i] > df['close'].iloc[i-1] and df['MACD'].iloc[i] < df['MACD'].iloc[i-1]:\n",
    "            divergence.append('Bearish MACD Price Divergence')\n",
    "        else:\n",
    "            divergence.append('None')\n",
    "    return divergence\n",
    "\n",
    "# Function to find divergence\n",
    "def find_MACD_sentiment_divergence(df):\n",
    "    divergence = ['None']  # Start with 'None' for the first row since no comparison can be made\n",
    "    for i in range(1, len(df)):\n",
    "        if df['close'].iloc[i] < df['close'].iloc[i-1] and df['Sentiment_MACD'].iloc[i] > df['Sentiment_MACD'].iloc[i-1]:\n",
    "            divergence.append('Bullish MACD Sentiment Divergence')\n",
    "        elif df['close'].iloc[i] > df['close'].iloc[i-1] and df['Sentiment_MACD'].iloc[i] < df['Sentiment_MACD'].iloc[i-1]:\n",
    "            divergence.append('Bearish MACD Sentiment Divergence')\n",
    "        else:\n",
    "            divergence.append('None')\n",
    "    return divergence\n",
    "\n",
    "\n",
    "# Function to find divergence\n",
    "def find_RSI_price_divergence(df):\n",
    "    divergence = ['None']  # Start with 'None' for the first row since no comparison can be made\n",
    "    for i in range(1, len(df)):\n",
    "        if df['close'].iloc[i] < df['close'].iloc[i-1] and df['RSI'].iloc[i] > df['RSI'].iloc[i-1]:\n",
    "            divergence.append('Bullish RSI Price Divergence')\n",
    "        elif df['close'].iloc[i] > df['close'].iloc[i-1] and df['RSI'].iloc[i] < df['RSI'].iloc[i-1]:\n",
    "            divergence.append('Bearish RSI Price Divergence')\n",
    "        else:\n",
    "            divergence.append('None')\n",
    "    return divergence\n",
    "\n",
    "\n",
    "# Function to find divergence\n",
    "def find_RSI_sentiment_divergence(df):\n",
    "    divergence = ['None']  # Start with 'None' for the first row since no comparison can be made\n",
    "    for i in range(1, len(df)):\n",
    "        if df['close'].iloc[i] < df['close'].iloc[i-1] and df['Sentiment_RSI'].iloc[i] > df['Sentiment_RSI'].iloc[i-1]:\n",
    "            divergence.append('Bullish RSI Sentiment Divergence')\n",
    "        elif df['close'].iloc[i] > df['close'].iloc[i-1] and df['Sentiment_RSI'].iloc[i] < df['Sentiment_RSI'].iloc[i-1]:\n",
    "            divergence.append('Bearish RSI Sentiment Divergence')\n",
    "        else:\n",
    "            divergence.append('None')\n",
    "    return divergence\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3138c395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing term: SOL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:365: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff'] = combined_data_df['close_ma_7'] - combined_data_df['scaled_combined_compound_ma_7']\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:368: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff_std'] = (combined_data_df['ma_7_diff'] - mean_difference) / std_difference\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:375: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:380: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:381: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” DEBUG CHECK â€” Row presence for: \n",
      "âœ… extended_price_df: FOUND\n",
      "âœ… ma_db_df: FOUND\n",
      "âœ… signals_df: FOUND\n",
      "âœ… pred_db_df: FOUND\n",
      "âœ… combined_data_df (final): FOUND\n",
      "Processing term: KAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:365: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff'] = combined_data_df['close_ma_7'] - combined_data_df['scaled_combined_compound_ma_7']\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:368: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff_std'] = (combined_data_df['ma_7_diff'] - mean_difference) / std_difference\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:375: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:380: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:381: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” DEBUG CHECK â€” Row presence for: \n",
      "âœ… extended_price_df: FOUND\n",
      "âœ… ma_db_df: FOUND\n",
      "âœ… signals_df: FOUND\n",
      "âœ… pred_db_df: FOUND\n",
      "âœ… combined_data_df (final): FOUND\n",
      "Processing term: LINK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:365: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff'] = combined_data_df['close_ma_7'] - combined_data_df['scaled_combined_compound_ma_7']\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:368: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff_std'] = (combined_data_df['ma_7_diff'] - mean_difference) / std_difference\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:375: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:380: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:381: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” DEBUG CHECK â€” Row presence for: \n",
      "âœ… extended_price_df: FOUND\n",
      "âœ… ma_db_df: FOUND\n",
      "âœ… signals_df: FOUND\n",
      "âœ… pred_db_df: FOUND\n",
      "âœ… combined_data_df (final): FOUND\n",
      "Processing term: ADA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:365: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff'] = combined_data_df['close_ma_7'] - combined_data_df['scaled_combined_compound_ma_7']\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:368: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff_std'] = (combined_data_df['ma_7_diff'] - mean_difference) / std_difference\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:375: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:380: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:381: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” DEBUG CHECK â€” Row presence for: \n",
      "âœ… extended_price_df: FOUND\n",
      "âœ… ma_db_df: FOUND\n",
      "âœ… signals_df: FOUND\n",
      "âœ… pred_db_df: FOUND\n",
      "âœ… combined_data_df (final): FOUND\n",
      "Processing term: MATIC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:365: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff'] = combined_data_df['close_ma_7'] - combined_data_df['scaled_combined_compound_ma_7']\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:368: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff_std'] = (combined_data_df['ma_7_diff'] - mean_difference) / std_difference\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:375: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:380: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:381: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” DEBUG CHECK â€” Row presence for: \n",
      "âœ… extended_price_df: FOUND\n",
      "âœ… ma_db_df: FOUND\n",
      "âœ… signals_df: FOUND\n",
      "âœ… pred_db_df: FOUND\n",
      "âœ… combined_data_df (final): FOUND\n",
      "Processing term: AMZN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:368: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff_std'] = (combined_data_df['ma_7_diff'] - mean_difference) / std_difference\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:375: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:380: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:381: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” DEBUG CHECK â€” Row presence for: \n",
      "âœ… extended_price_df: âŒ MISSING\n",
      "âœ… ma_db_df: âŒ MISSING\n",
      "âœ… signals_df: FOUND\n",
      "âœ… pred_db_df: âŒ MISSING\n",
      "âœ… combined_data_df (final): âŒ MISSING\n",
      "Processing term: MSFT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:368: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff_std'] = (combined_data_df['ma_7_diff'] - mean_difference) / std_difference\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:375: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:380: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:381: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” DEBUG CHECK â€” Row presence for: \n",
      "âœ… extended_price_df: âŒ MISSING\n",
      "âœ… ma_db_df: âŒ MISSING\n",
      "âœ… signals_df: FOUND\n",
      "âœ… pred_db_df: âŒ MISSING\n",
      "âœ… combined_data_df (final): âŒ MISSING\n",
      "Processing term: AVAX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:365: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff'] = combined_data_df['close_ma_7'] - combined_data_df['scaled_combined_compound_ma_7']\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:368: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff_std'] = (combined_data_df['ma_7_diff'] - mean_difference) / std_difference\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:375: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:380: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:381: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” DEBUG CHECK â€” Row presence for: \n",
      "âœ… extended_price_df: FOUND\n",
      "âœ… ma_db_df: FOUND\n",
      "âœ… signals_df: FOUND\n",
      "âœ… pred_db_df: FOUND\n",
      "âœ… combined_data_df (final): FOUND\n",
      "Processing term: AAPL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:368: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff_std'] = (combined_data_df['ma_7_diff'] - mean_difference) / std_difference\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:375: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:380: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:381: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” DEBUG CHECK â€” Row presence for: \n",
      "âœ… extended_price_df: âŒ MISSING\n",
      "âœ… ma_db_df: âŒ MISSING\n",
      "âœ… signals_df: FOUND\n",
      "âœ… pred_db_df: âŒ MISSING\n",
      "âœ… combined_data_df (final): âŒ MISSING\n",
      "Processing term: GME\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:368: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff_std'] = (combined_data_df['ma_7_diff'] - mean_difference) / std_difference\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:375: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:380: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:381: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” DEBUG CHECK â€” Row presence for: \n",
      "âœ… extended_price_df: âŒ MISSING\n",
      "âœ… ma_db_df: âŒ MISSING\n",
      "âœ… signals_df: FOUND\n",
      "âœ… pred_db_df: âŒ MISSING\n",
      "âœ… combined_data_df (final): âŒ MISSING\n",
      "Processing term: NVDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:368: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff_std'] = (combined_data_df['ma_7_diff'] - mean_difference) / std_difference\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:375: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:380: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:381: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” DEBUG CHECK â€” Row presence for: \n",
      "âœ… extended_price_df: âŒ MISSING\n",
      "âœ… ma_db_df: âŒ MISSING\n",
      "âœ… signals_df: FOUND\n",
      "âœ… pred_db_df: âŒ MISSING\n",
      "âœ… combined_data_df (final): âŒ MISSING\n",
      "Processing term: JPM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:368: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff_std'] = (combined_data_df['ma_7_diff'] - mean_difference) / std_difference\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:375: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:380: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:381: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” DEBUG CHECK â€” Row presence for: \n",
      "âœ… extended_price_df: âŒ MISSING\n",
      "âœ… ma_db_df: âŒ MISSING\n",
      "âœ… signals_df: FOUND\n",
      "âœ… pred_db_df: âŒ MISSING\n",
      "âœ… combined_data_df (final): âŒ MISSING\n",
      "Processing term: DOGEGOOGL\n",
      "No price data found for term: DOGEGOOGL\n",
      "Processing term: ETH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:365: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff'] = combined_data_df['close_ma_7'] - combined_data_df['scaled_combined_compound_ma_7']\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:368: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff_std'] = (combined_data_df['ma_7_diff'] - mean_difference) / std_difference\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:375: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:380: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:381: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” DEBUG CHECK â€” Row presence for: \n",
      "âœ… extended_price_df: FOUND\n",
      "âœ… ma_db_df: FOUND\n",
      "âœ… signals_df: FOUND\n",
      "âœ… pred_db_df: FOUND\n",
      "âœ… combined_data_df (final): FOUND\n",
      "Processing term: DXY\n",
      "No price data found for term: DXY\n",
      "Processing term: TSMC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:368: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff_std'] = (combined_data_df['ma_7_diff'] - mean_difference) / std_difference\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:375: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:380: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:381: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” DEBUG CHECK â€” Row presence for: \n",
      "âœ… extended_price_df: âŒ MISSING\n",
      "âœ… ma_db_df: âŒ MISSING\n",
      "âœ… signals_df: FOUND\n",
      "âœ… pred_db_df: âŒ MISSING\n",
      "âœ… combined_data_df (final): âŒ MISSING\n",
      "Processing term: CVX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:368: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff_std'] = (combined_data_df['ma_7_diff'] - mean_difference) / std_difference\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:375: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:380: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:381: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” DEBUG CHECK â€” Row presence for: \n",
      "âœ… extended_price_df: âŒ MISSING\n",
      "âœ… ma_db_df: âŒ MISSING\n",
      "âœ… signals_df: FOUND\n",
      "âœ… pred_db_df: âŒ MISSING\n",
      "âœ… combined_data_df (final): âŒ MISSING\n",
      "Processing term: COIN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:368: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff_std'] = (combined_data_df['ma_7_diff'] - mean_difference) / std_difference\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:375: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:380: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:381: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” DEBUG CHECK â€” Row presence for: \n",
      "âœ… extended_price_df: âŒ MISSING\n",
      "âœ… ma_db_df: âŒ MISSING\n",
      "âœ… signals_df: FOUND\n",
      "âœ… pred_db_df: âŒ MISSING\n",
      "âœ… combined_data_df (final): âŒ MISSING\n",
      "Processing term: POPCAT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:365: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff'] = combined_data_df['close_ma_7'] - combined_data_df['scaled_combined_compound_ma_7']\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:368: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff_std'] = (combined_data_df['ma_7_diff'] - mean_difference) / std_difference\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:375: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:380: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:381: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” DEBUG CHECK â€” Row presence for: \n",
      "âœ… extended_price_df: FOUND\n",
      "âœ… ma_db_df: FOUND\n",
      "âœ… signals_df: FOUND\n",
      "âœ… pred_db_df: FOUND\n",
      "âœ… combined_data_df (final): FOUND\n",
      "Processing term: SUI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:365: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff'] = combined_data_df['close_ma_7'] - combined_data_df['scaled_combined_compound_ma_7']\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:368: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff_std'] = (combined_data_df['ma_7_diff'] - mean_difference) / std_difference\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:375: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:380: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:381: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” DEBUG CHECK â€” Row presence for: \n",
      "âœ… extended_price_df: FOUND\n",
      "âœ… ma_db_df: FOUND\n",
      "âœ… signals_df: FOUND\n",
      "âœ… pred_db_df: FOUND\n",
      "âœ… combined_data_df (final): FOUND\n",
      "Processing term: HNT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:365: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff'] = combined_data_df['close_ma_7'] - combined_data_df['scaled_combined_compound_ma_7']\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:368: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff_std'] = (combined_data_df['ma_7_diff'] - mean_difference) / std_difference\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:375: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:380: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:381: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” DEBUG CHECK â€” Row presence for: \n",
      "âœ… extended_price_df: FOUND\n",
      "âœ… ma_db_df: âŒ MISSING\n",
      "âœ… signals_df: FOUND\n",
      "âœ… pred_db_df: âŒ MISSING\n",
      "âœ… combined_data_df (final): âŒ MISSING\n",
      "Processing term: NFLX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:368: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff_std'] = (combined_data_df['ma_7_diff'] - mean_difference) / std_difference\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:375: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:380: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:381: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” DEBUG CHECK â€” Row presence for: \n",
      "âœ… extended_price_df: âŒ MISSING\n",
      "âœ… ma_db_df: âŒ MISSING\n",
      "âœ… signals_df: FOUND\n",
      "âœ… pred_db_df: âŒ MISSING\n",
      "âœ… combined_data_df (final): âŒ MISSING\n",
      "Processing term: WIF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:365: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff'] = combined_data_df['close_ma_7'] - combined_data_df['scaled_combined_compound_ma_7']\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:368: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff_std'] = (combined_data_df['ma_7_diff'] - mean_difference) / std_difference\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:375: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:380: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:381: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” DEBUG CHECK â€” Row presence for: \n",
      "âœ… extended_price_df: FOUND\n",
      "âœ… ma_db_df: FOUND\n",
      "âœ… signals_df: FOUND\n",
      "âœ… pred_db_df: FOUND\n",
      "âœ… combined_data_df (final): FOUND\n",
      "Processing term: DIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:368: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff_std'] = (combined_data_df['ma_7_diff'] - mean_difference) / std_difference\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:375: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:380: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:381: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” DEBUG CHECK â€” Row presence for: \n",
      "âœ… extended_price_df: âŒ MISSING\n",
      "âœ… ma_db_df: âŒ MISSING\n",
      "âœ… signals_df: FOUND\n",
      "âœ… pred_db_df: âŒ MISSING\n",
      "âœ… combined_data_df (final): âŒ MISSING\n",
      "Processing term: BTC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:365: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff'] = combined_data_df['close_ma_7'] - combined_data_df['scaled_combined_compound_ma_7']\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:368: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff_std'] = (combined_data_df['ma_7_diff'] - mean_difference) / std_difference\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:375: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:380: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:381: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” DEBUG CHECK â€” Row presence for: \n",
      "âœ… extended_price_df: FOUND\n",
      "âœ… ma_db_df: FOUND\n",
      "âœ… signals_df: FOUND\n",
      "âœ… pred_db_df: FOUND\n",
      "âœ… combined_data_df (final): FOUND\n",
      "Processing term: TSLA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:368: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['ma_7_diff_std'] = (combined_data_df['ma_7_diff'] - mean_difference) / std_difference\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:375: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:380: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:381: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” DEBUG CHECK â€” Row presence for: \n",
      "âœ… extended_price_df: âŒ MISSING\n",
      "âœ… ma_db_df: âŒ MISSING\n",
      "âœ… signals_df: FOUND\n",
      "âœ… pred_db_df: âŒ MISSING\n",
      "âœ… combined_data_df (final): âŒ MISSING\n",
      "Final Combined DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_12668\\503078127.py:513: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  final_combined_data_df.reset_index(inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>term</th>\n",
       "      <th>combined_compound_ma_7</th>\n",
       "      <th>combined_compound_ma_21</th>\n",
       "      <th>combined_compound_ma_50</th>\n",
       "      <th>combined_compound_ma_100</th>\n",
       "      <th>combined_compound_ma_200</th>\n",
       "      <th>combined_compound</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>...</th>\n",
       "      <th>crossover_type_7</th>\n",
       "      <th>crossover_21</th>\n",
       "      <th>crossover_type_21</th>\n",
       "      <th>crossover_50</th>\n",
       "      <th>crossover_type_50</th>\n",
       "      <th>crossover_100</th>\n",
       "      <th>crossover_type_100</th>\n",
       "      <th>crossover_200</th>\n",
       "      <th>crossover_type_200</th>\n",
       "      <th>prev_close_up_down</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>2025-05-19</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.122077</td>\n",
       "      <td>0.111684</td>\n",
       "      <td>0.082183</td>\n",
       "      <td>0.058887</td>\n",
       "      <td>0.042653</td>\n",
       "      <td>0.070925</td>\n",
       "      <td>336.300</td>\n",
       "      <td>343.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>cross_down</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>2025-05-20</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.088690</td>\n",
       "      <td>0.100488</td>\n",
       "      <td>0.078510</td>\n",
       "      <td>0.057494</td>\n",
       "      <td>0.042115</td>\n",
       "      <td>-0.011468</td>\n",
       "      <td>347.870</td>\n",
       "      <td>354.9899</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>2025-05-21</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.077345</td>\n",
       "      <td>0.095290</td>\n",
       "      <td>0.077130</td>\n",
       "      <td>0.057213</td>\n",
       "      <td>0.042127</td>\n",
       "      <td>0.043311</td>\n",
       "      <td>344.430</td>\n",
       "      <td>347.3500</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>2025-05-22</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.104363</td>\n",
       "      <td>0.103484</td>\n",
       "      <td>0.081376</td>\n",
       "      <td>0.059751</td>\n",
       "      <td>0.043553</td>\n",
       "      <td>0.185414</td>\n",
       "      <td>331.900</td>\n",
       "      <td>347.2700</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>2025-05-23</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.137439</td>\n",
       "      <td>0.115591</td>\n",
       "      <td>0.087466</td>\n",
       "      <td>0.063255</td>\n",
       "      <td>0.045474</td>\n",
       "      <td>0.236668</td>\n",
       "      <td>337.920</td>\n",
       "      <td>343.1800</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>2025-05-24</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.140123</td>\n",
       "      <td>0.118554</td>\n",
       "      <td>0.089847</td>\n",
       "      <td>0.064936</td>\n",
       "      <td>0.046496</td>\n",
       "      <td>0.148176</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>2025-05-25</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.154439</td>\n",
       "      <td>0.125720</td>\n",
       "      <td>0.094064</td>\n",
       "      <td>0.067559</td>\n",
       "      <td>0.047997</td>\n",
       "      <td>0.197385</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>cross_up</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>2025-05-26</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.106047</td>\n",
       "      <td>0.110734</td>\n",
       "      <td>0.088841</td>\n",
       "      <td>0.065447</td>\n",
       "      <td>0.047131</td>\n",
       "      <td>-0.039128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>cross_down</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>2025-05-27</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.091131</td>\n",
       "      <td>0.104884</td>\n",
       "      <td>0.087176</td>\n",
       "      <td>0.065069</td>\n",
       "      <td>0.047123</td>\n",
       "      <td>0.046382</td>\n",
       "      <td>347.350</td>\n",
       "      <td>363.7900</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1102</th>\n",
       "      <td>2025-05-28</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.077562</td>\n",
       "      <td>0.098699</td>\n",
       "      <td>0.085202</td>\n",
       "      <td>0.064510</td>\n",
       "      <td>0.047021</td>\n",
       "      <td>0.036857</td>\n",
       "      <td>364.840</td>\n",
       "      <td>365.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>356.90</td>\n",
       "      <td>cross_down</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>2025-05-29</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.098564</td>\n",
       "      <td>0.104415</td>\n",
       "      <td>0.088197</td>\n",
       "      <td>0.066432</td>\n",
       "      <td>0.048161</td>\n",
       "      <td>0.161568</td>\n",
       "      <td>365.290</td>\n",
       "      <td>367.7100</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>2025-05-30</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.082017</td>\n",
       "      <td>0.097866</td>\n",
       "      <td>0.086008</td>\n",
       "      <td>0.065758</td>\n",
       "      <td>0.048004</td>\n",
       "      <td>0.032377</td>\n",
       "      <td>355.520</td>\n",
       "      <td>363.6800</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>346.46</td>\n",
       "      <td>cross_down</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>346.46</td>\n",
       "      <td>cross_down</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>2025-05-31</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.119919</td>\n",
       "      <td>0.110208</td>\n",
       "      <td>0.091797</td>\n",
       "      <td>0.069082</td>\n",
       "      <td>0.049851</td>\n",
       "      <td>0.233625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cross_up</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.141057</td>\n",
       "      <td>0.118777</td>\n",
       "      <td>0.096216</td>\n",
       "      <td>0.071763</td>\n",
       "      <td>0.051389</td>\n",
       "      <td>0.204469</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cross_up</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cross_up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107</th>\n",
       "      <td>2025-06-02</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.158819</td>\n",
       "      <td>0.127261</td>\n",
       "      <td>0.100760</td>\n",
       "      <td>0.074542</td>\n",
       "      <td>0.052988</td>\n",
       "      <td>0.212105</td>\n",
       "      <td>343.500</td>\n",
       "      <td>348.0200</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108</th>\n",
       "      <td>2025-06-03</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.161703</td>\n",
       "      <td>0.131179</td>\n",
       "      <td>0.103489</td>\n",
       "      <td>0.076439</td>\n",
       "      <td>0.054156</td>\n",
       "      <td>0.170356</td>\n",
       "      <td>346.595</td>\n",
       "      <td>355.4000</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>2025-06-04</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.143425</td>\n",
       "      <td>0.127307</td>\n",
       "      <td>0.102905</td>\n",
       "      <td>0.076680</td>\n",
       "      <td>0.054499</td>\n",
       "      <td>0.088593</td>\n",
       "      <td>345.095</td>\n",
       "      <td>345.6000</td>\n",
       "      <td>...</td>\n",
       "      <td>cross_down</td>\n",
       "      <td>332.05</td>\n",
       "      <td>cross_down</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.108588</td>\n",
       "      <td>0.116105</td>\n",
       "      <td>0.099030</td>\n",
       "      <td>0.075242</td>\n",
       "      <td>0.053997</td>\n",
       "      <td>0.004078</td>\n",
       "      <td>322.490</td>\n",
       "      <td>324.5499</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>284.70</td>\n",
       "      <td>cross_down</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>284.70</td>\n",
       "      <td>cross_up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>2025-06-06</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.078489</td>\n",
       "      <td>0.104476</td>\n",
       "      <td>0.094683</td>\n",
       "      <td>0.073518</td>\n",
       "      <td>0.053342</td>\n",
       "      <td>-0.011810</td>\n",
       "      <td>298.830</td>\n",
       "      <td>305.5000</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112</th>\n",
       "      <td>2025-06-07</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.061670</td>\n",
       "      <td>0.095998</td>\n",
       "      <td>0.091410</td>\n",
       "      <td>0.072285</td>\n",
       "      <td>0.052923</td>\n",
       "      <td>0.011215</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows Ã— 190 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  term  combined_compound_ma_7  combined_compound_ma_21  \\\n",
       "1093 2025-05-19  TSLA                0.122077                 0.111684   \n",
       "1094 2025-05-20  TSLA                0.088690                 0.100488   \n",
       "1095 2025-05-21  TSLA                0.077345                 0.095290   \n",
       "1096 2025-05-22  TSLA                0.104363                 0.103484   \n",
       "1097 2025-05-23  TSLA                0.137439                 0.115591   \n",
       "1098 2025-05-24  TSLA                0.140123                 0.118554   \n",
       "1099 2025-05-25  TSLA                0.154439                 0.125720   \n",
       "1100 2025-05-26  TSLA                0.106047                 0.110734   \n",
       "1101 2025-05-27  TSLA                0.091131                 0.104884   \n",
       "1102 2025-05-28  TSLA                0.077562                 0.098699   \n",
       "1103 2025-05-29  TSLA                0.098564                 0.104415   \n",
       "1104 2025-05-30  TSLA                0.082017                 0.097866   \n",
       "1105 2025-05-31  TSLA                0.119919                 0.110208   \n",
       "1106 2025-06-01  TSLA                0.141057                 0.118777   \n",
       "1107 2025-06-02  TSLA                0.158819                 0.127261   \n",
       "1108 2025-06-03  TSLA                0.161703                 0.131179   \n",
       "1109 2025-06-04  TSLA                0.143425                 0.127307   \n",
       "1110 2025-06-05  TSLA                0.108588                 0.116105   \n",
       "1111 2025-06-06  TSLA                0.078489                 0.104476   \n",
       "1112 2025-06-07  TSLA                0.061670                 0.095998   \n",
       "\n",
       "      combined_compound_ma_50  combined_compound_ma_100  \\\n",
       "1093                 0.082183                  0.058887   \n",
       "1094                 0.078510                  0.057494   \n",
       "1095                 0.077130                  0.057213   \n",
       "1096                 0.081376                  0.059751   \n",
       "1097                 0.087466                  0.063255   \n",
       "1098                 0.089847                  0.064936   \n",
       "1099                 0.094064                  0.067559   \n",
       "1100                 0.088841                  0.065447   \n",
       "1101                 0.087176                  0.065069   \n",
       "1102                 0.085202                  0.064510   \n",
       "1103                 0.088197                  0.066432   \n",
       "1104                 0.086008                  0.065758   \n",
       "1105                 0.091797                  0.069082   \n",
       "1106                 0.096216                  0.071763   \n",
       "1107                 0.100760                  0.074542   \n",
       "1108                 0.103489                  0.076439   \n",
       "1109                 0.102905                  0.076680   \n",
       "1110                 0.099030                  0.075242   \n",
       "1111                 0.094683                  0.073518   \n",
       "1112                 0.091410                  0.072285   \n",
       "\n",
       "      combined_compound_ma_200  combined_compound     open      high  ...  \\\n",
       "1093                  0.042653           0.070925  336.300  343.0000  ...   \n",
       "1094                  0.042115          -0.011468  347.870  354.9899  ...   \n",
       "1095                  0.042127           0.043311  344.430  347.3500  ...   \n",
       "1096                  0.043553           0.185414  331.900  347.2700  ...   \n",
       "1097                  0.045474           0.236668  337.920  343.1800  ...   \n",
       "1098                  0.046496           0.148176      NaN       NaN  ...   \n",
       "1099                  0.047997           0.197385      NaN       NaN  ...   \n",
       "1100                  0.047131          -0.039128      NaN       NaN  ...   \n",
       "1101                  0.047123           0.046382  347.350  363.7900  ...   \n",
       "1102                  0.047021           0.036857  364.840  365.0000  ...   \n",
       "1103                  0.048161           0.161568  365.290  367.7100  ...   \n",
       "1104                  0.048004           0.032377  355.520  363.6800  ...   \n",
       "1105                  0.049851           0.233625      NaN       NaN  ...   \n",
       "1106                  0.051389           0.204469      NaN       NaN  ...   \n",
       "1107                  0.052988           0.212105  343.500  348.0200  ...   \n",
       "1108                  0.054156           0.170356  346.595  355.4000  ...   \n",
       "1109                  0.054499           0.088593  345.095  345.6000  ...   \n",
       "1110                  0.053997           0.004078  322.490  324.5499  ...   \n",
       "1111                  0.053342          -0.011810  298.830  305.5000  ...   \n",
       "1112                  0.052923           0.011215      NaN       NaN  ...   \n",
       "\n",
       "      crossover_type_7  crossover_21 crossover_type_21  crossover_50  \\\n",
       "1093        cross_down           NaN               nan           NaN   \n",
       "1094               nan           NaN               nan           NaN   \n",
       "1095               nan           NaN               nan           NaN   \n",
       "1096               nan           NaN               nan           NaN   \n",
       "1097               nan           NaN               nan           NaN   \n",
       "1098               nan           NaN               nan           NaN   \n",
       "1099          cross_up           NaN               nan           NaN   \n",
       "1100        cross_down           NaN               nan           NaN   \n",
       "1101               nan           NaN               nan           NaN   \n",
       "1102               nan        356.90        cross_down           NaN   \n",
       "1103               nan           NaN               nan           NaN   \n",
       "1104               nan           NaN               nan        346.46   \n",
       "1105               nan           NaN               nan           NaN   \n",
       "1106               nan           NaN          cross_up           NaN   \n",
       "1107               nan           NaN               nan           NaN   \n",
       "1108               nan           NaN               nan           NaN   \n",
       "1109        cross_down        332.05        cross_down           NaN   \n",
       "1110               nan           NaN               nan        284.70   \n",
       "1111               nan           NaN               nan           NaN   \n",
       "1112               nan           NaN               nan           NaN   \n",
       "\n",
       "      crossover_type_50  crossover_100  crossover_type_100  crossover_200  \\\n",
       "1093                nan            NaN                 nan            NaN   \n",
       "1094                nan            NaN                 nan            NaN   \n",
       "1095                nan            NaN                 nan            NaN   \n",
       "1096                nan            NaN                 nan            NaN   \n",
       "1097                nan            NaN                 nan            NaN   \n",
       "1098                nan            NaN                 nan            NaN   \n",
       "1099                nan            NaN                 nan            NaN   \n",
       "1100                nan            NaN                 nan            NaN   \n",
       "1101                nan            NaN                 nan            NaN   \n",
       "1102                nan            NaN                 nan            NaN   \n",
       "1103                nan            NaN                 nan            NaN   \n",
       "1104         cross_down            NaN                 nan         346.46   \n",
       "1105           cross_up            NaN                 nan            NaN   \n",
       "1106                nan            NaN                 nan            NaN   \n",
       "1107                nan            NaN                 nan            NaN   \n",
       "1108                nan            NaN                 nan            NaN   \n",
       "1109                nan            NaN                 nan            NaN   \n",
       "1110         cross_down            NaN                 nan         284.70   \n",
       "1111                nan            NaN                 nan            NaN   \n",
       "1112                nan            NaN                 nan            NaN   \n",
       "\n",
       "      crossover_type_200  prev_close_up_down  \n",
       "1093                 nan                   0  \n",
       "1094                 nan                   1  \n",
       "1095                 nan                   0  \n",
       "1096                 nan                   1  \n",
       "1097                 nan                   0  \n",
       "1098                 nan                   0  \n",
       "1099                 nan                   0  \n",
       "1100                 nan                   0  \n",
       "1101                 nan                   0  \n",
       "1102                 nan                   0  \n",
       "1103                 nan                   1  \n",
       "1104          cross_down                   0  \n",
       "1105                 nan                   0  \n",
       "1106            cross_up                   0  \n",
       "1107                 nan                   0  \n",
       "1108                 nan                   1  \n",
       "1109                 nan                   0  \n",
       "1110            cross_up                   0  \n",
       "1111                 nan                   1  \n",
       "1112                 nan                   0  \n",
       "\n",
       "[20 rows x 190 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define dictionaries to store different results\n",
    "moving_averages_dict = {}\n",
    "scaled_features_dict = {}\n",
    "combined_data_dict = {}\n",
    "\n",
    "# Iterate over each term in the group\n",
    "for grp_term in grp_terms:\n",
    "    print(f\"Processing term: {grp_term}\")\n",
    "\n",
    "    # Step 1: Fetch price data for the extended range\n",
    "    extended_price_df = fetch_price_data(extended_start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'), grp_term)\n",
    "    \n",
    "    if extended_price_df is None or extended_price_df.empty:\n",
    "        print(f\"No price data found for term: {grp_term}\")\n",
    "        continue\n",
    "\n",
    "    # Step 1.1: Set 'date' as the index and ensure it's unique\n",
    "    extended_price_df['date'] = pd.to_datetime(extended_price_df['date'])\n",
    "    extended_price_df.set_index('date', inplace=True)\n",
    "    extended_price_df = extended_price_df.loc[~extended_price_df.index.duplicated(keep='first')]\n",
    "\n",
    "    # Step 2: Fetch moving averages data\n",
    "    ma_db_df = fetch_moving_averages(start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'), grp_term)\n",
    "    \n",
    "    # Step 2.1: Fetch prediction data\n",
    "    pred_db_df = fetch_predictions_data(start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'), grp_term)\n",
    "    \n",
    "    # Step 2.2: Fetch Bollinger data\n",
    "    boll_db_df = fetch_bollinger_data(start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'), grp_term)\n",
    "\n",
    "    signals_df = fetch_signals(start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'), grp_term)\n",
    "    \n",
    "    if ma_db_df is not None and not ma_db_df.empty:\n",
    "        ma_db_df['date'] = pd.to_datetime(ma_db_df['date'])\n",
    "        ma_db_df.set_index('date', inplace=True)\n",
    "\n",
    "        # Drop the 'term' column from extended_price_df to avoid duplication during the join\n",
    "        extended_price_df.drop(columns=['term'], errors='ignore', inplace=True)\n",
    "\n",
    "        # Join moving average data to extended price data\n",
    "        combined_data_df = ma_db_df.join(extended_price_df, how='left')\n",
    "        \n",
    "        # Store results\n",
    "        moving_averages_dict[grp_term] = combined_data_df.copy()\n",
    "        \n",
    "        \n",
    "        # Assuming signals_df has been fetched and contains a 'date' column\n",
    "\n",
    "        # Check and prepare signals_df\n",
    "        if signals_df is not None and not signals_df.empty:\n",
    "            # Convert 'date' to datetime and set as index\n",
    "            signals_df['date'] = pd.to_datetime(signals_df['date'])\n",
    "            signals_df.set_index('date', inplace=True)\n",
    "            signals_df.drop(columns=['term'], errors='ignore', inplace=True)  # Drop 'term' if not needed or duplicate\n",
    "\n",
    "            # Rename the index to 'date' for consistency, if you prefer\n",
    "            signals_df.index.rename('date', inplace=True)\n",
    "\n",
    "            # Join signals data to extended price data\n",
    "            combined_data_df = combined_data_df.join(signals_df, how='left')\n",
    "\n",
    "            #print(\"Signals data integrated into combined DataFrame.\")\n",
    "        else:\n",
    "            print(\"No signals data available or DataFrame is empty.\")\n",
    "\n",
    "        # Now combined_data_df includes the signals data\n",
    "        \n",
    "        \n",
    "        # If prediction data is available, process and join it\n",
    "        if pred_db_df is not None and not pred_db_df.empty:\n",
    "            # Convert prediction_date to datetime and set as index\n",
    "            pred_db_df['prediction_date'] = pd.to_datetime(pred_db_df['prediction_date'])\n",
    "            pred_db_df.set_index('prediction_date', inplace=True)\n",
    "            pred_db_df.drop(columns=['term'], errors='ignore', inplace=True)\n",
    "            # Rename the index to 'date' for consistency, if you prefer\n",
    "            pred_db_df.index.rename('date', inplace=True)\n",
    "\n",
    "            # Now join prediction data to combined_data_df\n",
    "            combined_data_df = combined_data_df.join(pred_db_df, how='left')\n",
    "            \n",
    "            \n",
    "            \n",
    "        # Ensure the index is a proper DatetimeIndex and sorted\n",
    "        combined_data_df.index = pd.to_datetime(combined_data_df.index)\n",
    "        combined_data_df = combined_data_df.sort_index()\n",
    "\n",
    "\n",
    "\n",
    "        # DATE RELATED ERROR CORRECTING\n",
    "        # Filter out rows with dates in the future\n",
    "        current_date = pd.Timestamp.now().normalize()\n",
    "        # combined_data_df = combined_data_df[combined_data_df.index <= current_date]\n",
    "        # Ensure the index is a proper `DatetimeIndex` and sorted\n",
    "        combined_data_df.index = pd.to_datetime(combined_data_df.index)\n",
    "        combined_data_df = combined_data_df.sort_index()    \n",
    "            \n",
    "        # Step 4: Calculate Technical Indicators\n",
    "        combined_data_df['RSI'] = calculate_rsi(combined_data_df['close'])\n",
    "        combined_data_df['Stochastic_RSI'] = calculate_stochastic_rsi(combined_data_df, 'RSI')\n",
    "        combined_data_df['MFI'] = calculate_mfi(combined_data_df)\n",
    "        combined_data_df['SFI'] = calculate_sfi(combined_data_df)\n",
    "        combined_data_df['MACD'], combined_data_df['MACD_Signal'] = calculate_macd(combined_data_df['close'])\n",
    "        \n",
    "        # Step 4.1: Calculate Sentiment Technical Indicators\n",
    "        # Calculate RSI for sentiment data using 'daily_avg_combined_compound'\n",
    "        combined_data_df['Sentiment_RSI'] = calculate_rsi(combined_data_df['combined_compound_ma_7'])\n",
    "        combined_data_df['Sentiment_Stochastic_RSI'] = calculate_stochastic_rsi(combined_data_df, 'Sentiment_RSI')\n",
    "        combined_data_df['Sentiment_MACD'], combined_data_df['Sentiment_MACD_Signal'] = calculate_macd(combined_data_df['combined_compound'])\n",
    "        \n",
    "        # Step 4.2-4.6: Calculate Sentiment Technical Indicators supporting features\n",
    "        # Adding stdev for RSI and Sentiment_RSI\n",
    "        #Step 4.21: Calculate the difference between RSI and Sentiment_RSI\n",
    "        combined_data_df['RSI_Difference'] = combined_data_df['RSI'] - combined_data_df['Sentiment_RSI']\n",
    "\n",
    "        # Step 4.22: Calculate the rolling standard deviation of this difference\n",
    "        # You can specify the window size (e.g., 14 days) for the rolling standard deviation\n",
    "        combined_data_df['RSI_Sentiment_STD'] = combined_data_df['RSI_Difference'].rolling(window=14).std().abs()\n",
    "        # Now, combined_data_df['RSI_Sentiment_STD'] contains the standard deviation between RSI and Sentiment_RSI\n",
    "        \n",
    "        # Step 4.23: Calculate the rolling mean of the RSI_Difference\n",
    "        combined_data_df['RSI_Difference_Mean'] = combined_data_df['RSI_Difference'].rolling(window=14).mean()\n",
    "\n",
    "        # Step 4.24: Calculate the number of standard deviations from the mean\n",
    "        combined_data_df['RSI_Difference_STD_Deviation'] = (\n",
    "            (combined_data_df['RSI_Difference'] - combined_data_df['RSI_Difference_Mean']) /\n",
    "            combined_data_df['RSI_Sentiment_STD']\n",
    "        )\n",
    "        # Now, combined_data_df['RSI_Difference_STD_Deviation'] contains the number of standard deviations from the mean for each day\n",
    "        \n",
    "        # Step 4.24: Identify if RSI_Difference_STD_Deviation is greater than 2\n",
    "        combined_data_df['RSI_STD_above_2'] = abs(combined_data_df['RSI_Difference_STD_Deviation']) > 2\n",
    "\n",
    "        # Step 4.25: execute the divergence function\n",
    "        combined_data_df['RSI_Price_Divergence'] = find_RSI_price_divergence(combined_data_df)\n",
    "        combined_data_df['RSI_Sentiment_Divergence'] = find_RSI_sentiment_divergence(combined_data_df)\n",
    "        \n",
    "        # Step 4.26: count the divergence recorded in the divergence function\n",
    "        # Calculate consecutive counts for RSI Price divergence, similar to what was done with MACD\n",
    "        combined_data_df['Consecutive_Count_RSI_Price_Divergence'] = (\n",
    "            combined_data_df['RSI_Price_Divergence']\n",
    "            .apply(lambda x: x if x != 'None' else None)\n",
    "            .groupby((combined_data_df['RSI_Price_Divergence'] != combined_data_df['RSI_Price_Divergence'].shift()).cumsum())\n",
    "            .cumcount()\n",
    "            .where(combined_data_df['RSI_Price_Divergence'] != 'None', 0)\n",
    "        )\n",
    "        \n",
    "        # Step 4.26A: Calculate consecutive counts for RSI Sentiment divergence, similar to what is done with MACD\n",
    "        combined_data_df['Consecutive_Count_RSI_Sentiment_Divergence'] = (\n",
    "            combined_data_df['RSI_Sentiment_Divergence']\n",
    "            .apply(lambda x: x if x != 'None' else None)\n",
    "            .groupby((combined_data_df['RSI_Sentiment_Divergence'] != combined_data_df['RSI_Sentiment_Divergence'].shift()).cumsum())\n",
    "            .cumcount()\n",
    "            .where(combined_data_df['RSI_Sentiment_Divergence'] != 'None', 0)\n",
    "        )\n",
    "        \n",
    "        # Step 4.27 calculate RSI_Trend_Reversal variable\n",
    "        combined_data_df['RSI_Overbought'] = (combined_data_df['RSI'] > 70) & (combined_data_df['Sentiment_RSI'] > 70)\n",
    "        combined_data_df['RSI_Oversold'] = (combined_data_df['RSI'] < 30) & (combined_data_df['Sentiment_RSI'] < 30)\n",
    "\n",
    "        # Create conditions for divergence\n",
    "        combined_data_df['Bearish_Divergence'] = (combined_data_df['RSI_Price_Divergence'] == 'Bearish RSI Price Divergence') & (combined_data_df['RSI_Sentiment_Divergence'] == 'Bearish RSI Sentiment Divergence')\n",
    "        combined_data_df['Bullish_Divergence'] = (combined_data_df['RSI_Price_Divergence'] == 'Bullish RSI Price Divergence') & (combined_data_df['RSI_Sentiment_Divergence'] == 'Bullish RSI Sentiment Divergence')\n",
    "\n",
    "        # Create a new column for RSI Trend Reversal based on overbought/oversold levels and divergence\n",
    "        combined_data_df['RSI_Trend_Reversal'] = np.where(\n",
    "            (combined_data_df['RSI_Overbought'] & combined_data_df['Bearish_Divergence']),\n",
    "            'Likely Downward Reversal',  # Bearish reversal when both RSI and Sentiment_RSI are overbought and bearish divergence occurs\n",
    "            np.where(\n",
    "                (combined_data_df['RSI_Oversold'] & combined_data_df['Bullish_Divergence']),\n",
    "                'Likely Upward Reversal',  # Bullish reversal when both RSI and Sentiment_RSI are oversold and bullish divergence occurs\n",
    "                'No Reversal'  # Default value when no reversal condition is met\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Step 4.3: Calculate the rolling mean of the stoch RSI_Difference\n",
    "        # Apply smoothing to the Sentiment_Stochastic_RSI using a moving average or EMA; Here we use a 2-period EMA for smoothing\n",
    "        smoothing_window = 2\n",
    "        combined_data_df['Smoothed_Sentiment_Stochastic_RSI'] = combined_data_df['Sentiment_Stochastic_RSI'].ewm(span=smoothing_window, adjust=False).mean()\n",
    "\n",
    "        # Adding stdev for RSI and Sentiment_RSI\n",
    "        # Step 4.31: Calculate the difference between Stochastic_RSI and Sentiment_Stochastic_RSI\n",
    "        combined_data_df['Stoch_RSI_Difference'] = combined_data_df['Stochastic_RSI'] - combined_data_df['Sentiment_Stochastic_RSI']\n",
    "\n",
    "        \n",
    "        # Step 4.32: Calculate the rolling standard deviation of this difference\n",
    "        # You can specify the window size (e.g., 14 days) for the rolling standard deviation\n",
    "        combined_data_df['Stoch_RSI_Sentiment_STD'] = combined_data_df['Stoch_RSI_Difference'].rolling(window=14).std().abs()\n",
    "\n",
    "        # Step 4.33: Calculate the rolling mean of the RSI_Difference\n",
    "        combined_data_df['Stoch_RSI_Difference_Mean'] = combined_data_df['Stoch_RSI_Difference'].rolling(window=14).mean()\n",
    "\n",
    "        # Step 4.34: Calculate the number of standard deviations from the mean\n",
    "        combined_data_df['Stoch_RSI_Difference_STD_Deviation'] = (\n",
    "            (combined_data_df['Stoch_RSI_Difference'] - combined_data_df['Stoch_RSI_Difference_Mean']) /\n",
    "            combined_data_df['Stoch_RSI_Sentiment_STD']\n",
    "        )\n",
    "        \n",
    "        # Step 4.35: Identify if Stoch_RSI_Difference_STD_Deviation is greater than 2\n",
    "        combined_data_df['Stoch_RSI_STD_above_2'] = abs(combined_data_df['Stoch_RSI_Difference_STD_Deviation']) > 2\n",
    "        \n",
    "        #  # Step 4.365: count how many periods stoch RSI is in extreme position\n",
    "        # Initialize the counter columns\n",
    "        combined_data_df['Stoch_RSI_Both_Extreme_Counter'] = 0\n",
    "\n",
    "        # Iterate through the DataFrame to update the counter\n",
    "        for i in range(1, len(combined_data_df)):\n",
    "            if combined_data_df.iloc[i]['Stochastic_RSI'] > 80 and combined_data_df.iloc[i]['Sentiment_Stochastic_RSI'] > 80:\n",
    "                if combined_data_df.iloc[i-1]['Stoch_RSI_Both_Extreme_Counter'] > 0:  # Continuation of a positive streak\n",
    "                    combined_data_df.iloc[i, combined_data_df.columns.get_loc('Stoch_RSI_Both_Extreme_Counter')] = combined_data_df.iloc[i-1]['Stoch_RSI_Both_Extreme_Counter'] + 1\n",
    "                else:  # Start of a new positive streak\n",
    "                    combined_data_df.iloc[i, combined_data_df.columns.get_loc('Stoch_RSI_Both_Extreme_Counter')] = 1\n",
    "            elif combined_data_df.iloc[i]['Stochastic_RSI'] < 20 and combined_data_df.iloc[i]['Sentiment_Stochastic_RSI'] < 20:\n",
    "                if combined_data_df.iloc[i-1]['Stoch_RSI_Both_Extreme_Counter'] < 0:  # Continuation of a negative streak\n",
    "                    combined_data_df.iloc[i, combined_data_df.columns.get_loc('Stoch_RSI_Both_Extreme_Counter')] = combined_data_df.iloc[i-1]['Stoch_RSI_Both_Extreme_Counter'] - 1\n",
    "                else:  # Start of a new negative streak\n",
    "                    combined_data_df.iloc[i, combined_data_df.columns.get_loc('Stoch_RSI_Both_Extreme_Counter')] = -1\n",
    "            else:\n",
    "                combined_data_df.iloc[i, combined_data_df.columns.get_loc('Stoch_RSI_Both_Extreme_Counter')] = 0  # Reset the counter if the condition is not met or both are zero\n",
    "\n",
    "            # Additional condition to reset counter if both values are exactly zero\n",
    "            if combined_data_df.iloc[i]['Stochastic_RSI'] == 0 and combined_data_df.iloc[i]['Sentiment_Stochastic_RSI'] == 0:\n",
    "                combined_data_df.iloc[i, combined_data_df.columns.get_loc('Stoch_RSI_Both_Extreme_Counter')] = 0\n",
    "        \n",
    "        \n",
    "        # Step 4.40 Calculate extra goodies for the MACD \n",
    "        # Scale Sentiment_MACD to the scale of MACD\n",
    "        macd_scaler = MinMaxScaler(feature_range=(combined_data_df['MACD'].min(), combined_data_df['MACD'].max()))\n",
    "        combined_data_df['scaled_Sentiment_MACD'] = macd_scaler.fit_transform(combined_data_df[['Sentiment_MACD']]).flatten()\n",
    "\n",
    "        # Step 4.41 Scale Sentiment_MACD_Signal to the scale of MACD_Signal\n",
    "        macd_signal_scaler = MinMaxScaler(feature_range=(combined_data_df['MACD_Signal'].min(), combined_data_df['MACD_Signal'].max()))\n",
    "        combined_data_df['scaled_Sentiment_MACD_Signal'] = macd_signal_scaler.fit_transform(combined_data_df[['Sentiment_MACD_Signal']]).flatten()\n",
    "\n",
    "        #Step 4.42 Calculate the MACD histogram for price data\n",
    "        combined_data_df['MACD_Histogram'] = combined_data_df['MACD'] - combined_data_df['MACD_Signal']\n",
    "\n",
    "        #Step 4.43 Calculate the MACD histogram for sentiment data\n",
    "        combined_data_df['Sentiment_MACD_Histogram'] = combined_data_df['scaled_Sentiment_MACD'] - combined_data_df['scaled_Sentiment_MACD_Signal']\n",
    "\n",
    "        #Step 4.44: Calculate the difference between MACD_Signal and Sentiment_MACD_Signal\n",
    "        combined_data_df['MACD_Signal_Difference'] = combined_data_df['MACD_Signal'] - combined_data_df['scaled_Sentiment_MACD_Signal']\n",
    "\n",
    "        #Step 4.45: Calculate the rolling standard deviation of this difference\n",
    "        combined_data_df['MACD_Signal_Sentiment_STD'] = combined_data_df['MACD_Signal_Difference'].rolling(window=14).std().abs()\n",
    "\n",
    "        #Step 4.46: Calculate the rolling mean of the MACD_Signal_Difference\n",
    "        combined_data_df['MACD_Signal_Difference_Mean'] = combined_data_df['MACD_Signal_Difference'].rolling(window=14).mean()\n",
    "\n",
    "        #Step 4.46: Calculate the number of standard deviations from the mean\n",
    "        combined_data_df['MACD_Signal_Difference_STD_Deviation'] = (\n",
    "            (combined_data_df['MACD_Signal_Difference'] - combined_data_df['MACD_Signal_Difference_Mean']) /\n",
    "            combined_data_df['MACD_Signal_Sentiment_STD']\n",
    "        )\n",
    "\n",
    "        # Step 4.47: Identify if MACD_Signal_Difference_STD_Deviation is greater than 2\n",
    "        macd_signal_condition_above_2 = abs(combined_data_df['MACD_Signal_Difference_STD_Deviation']) > 2\n",
    "\n",
    "        # Step 4.48: Record the MACD_Signal_Difference_STD_Deviation directly to the DataFrame\n",
    "        combined_data_df['MACD_Signal_trend_reversal'] = np.where(\n",
    "            macd_signal_condition_above_2,\n",
    "            combined_data_df['MACD_Signal_Difference_STD_Deviation'],\n",
    "            0\n",
    "        )\n",
    "\n",
    "        # Step 4.49: Identify if there is a cross between Sentiment_MACD_Signal and MACD_Signal and record it\n",
    "        # Capture positive and negative crosses for future analysis in Tableau\n",
    "        combined_data_df['MACD_Signal_Cross'] = np.where(\n",
    "            (combined_data_df['scaled_Sentiment_MACD_Signal'] > combined_data_df['MACD_Signal']) &\n",
    "            (combined_data_df['scaled_Sentiment_MACD_Signal'].shift(1) <= combined_data_df['MACD_Signal'].shift(1)),\n",
    "            1,  # Bullish cross\n",
    "            np.where(\n",
    "                (combined_data_df['scaled_Sentiment_MACD_Signal'] < combined_data_df['MACD_Signal']) &\n",
    "                (combined_data_df['scaled_Sentiment_MACD_Signal'].shift(1) >= combined_data_df['MACD_Signal'].shift(1)),\n",
    "                -1,  # Bearish cross\n",
    "                0  # No cross\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Step 4.491: Create a new column to capture the significance of the cross using both deviation and the MACD difference\n",
    "        combined_data_df['MACD_Cross_Significance'] = np.where(\n",
    "            combined_data_df['MACD_Signal_Cross'] != 0,\n",
    "            combined_data_df['MACD_Signal_trend_reversal'] * combined_data_df['MACD_Signal_Cross'],\n",
    "            0\n",
    "        )\n",
    "\n",
    "        # Step 4.492: Determine the direction of the Sentiment MACD Signal\n",
    "        combined_data_df['Sentiment_MACD_Signal_Direction'] = np.where(\n",
    "            combined_data_df['scaled_Sentiment_MACD_Signal'] > combined_data_df['scaled_Sentiment_MACD_Signal'].shift(1),\n",
    "            1,  # Upward direction\n",
    "            np.where(\n",
    "                combined_data_df['scaled_Sentiment_MACD_Signal'] < combined_data_df['scaled_Sentiment_MACD_Signal'].shift(1),\n",
    "                -1,  # Downward direction\n",
    "                0  # No change\n",
    "            )\n",
    "        )\n",
    "        \n",
    "\n",
    "        #Step 4.493 Find MACD Price divergence and store it in the DataFrame\n",
    "        combined_data_df['MACD_Price_Divergence'] = find_MACD_price_divergence(combined_data_df)\n",
    "        combined_data_df['Consecutive_Count_MACD_Price_Divergence'] = combined_data_df['MACD_Price_Divergence'].apply(lambda x: x if x != 'None' else None).groupby((combined_data_df['MACD_Price_Divergence'] != combined_data_df['MACD_Price_Divergence'].shift()).cumsum()).cumcount().where(combined_data_df['MACD_Price_Divergence'] != 'None', 0)\n",
    " \n",
    "        #Step 4.494 Find MACD sentiment divergence and store it in the DataFrame\n",
    "        combined_data_df['MACD_Sentiment_Divergence'] = find_MACD_sentiment_divergence(combined_data_df)\n",
    "        combined_data_df['Consecutive_Count_MACD_Sentiment_Divergence'] = combined_data_df['MACD_Sentiment_Divergence'].apply(lambda x: x if x != 'None' else None).groupby((combined_data_df['MACD_Sentiment_Divergence'] != combined_data_df['MACD_Sentiment_Divergence'].shift()).cumsum()).cumcount().where(combined_data_df['MACD_Sentiment_Divergence'] != 'None', 0)\n",
    " \n",
    "        # Step 7: Scale selected sentiment features to match the scale of the 'close' price\n",
    "        columns_to_scale = [\n",
    "            'combined_compound_ma_7', 'combined_compound_ma_21', 'combined_compound_ma_50',\n",
    "            'combined_compound_ma_100', 'combined_compound_ma_200', 'combined_compound'\n",
    "        ]\n",
    "        combined_data_df = scale_features_to_price(combined_data_df, columns_to_scale, 'close')\n",
    "        \n",
    "        # Step 5: Generate previous day and trend indicators\n",
    "        ma_columns = [col for col in combined_data_df.columns if 'combined_compound' in col or 'close_ma' in col or 'scaled_combined_compound' in col]\n",
    "\n",
    "        for ma_column in ma_columns:\n",
    "            combined_data_df[f'prev_{ma_column}'] = combined_data_df[ma_column].shift(1)\n",
    "            combined_data_df[f'{ma_column}_trend'] = (combined_data_df[ma_column] > combined_data_df[f'prev_{ma_column}']).astype(int)\n",
    "            combined_data_df[f'{ma_column}_pct_change'] = combined_data_df[ma_column].pct_change() * 100\n",
    "            combined_data_df[f'{ma_column}_direction_change_flag'] = combined_data_df[f'{ma_column}_trend'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
    "\n",
    "        # Step 6: Volume Moving Averages\n",
    "        ma_vol_df = combined_data_df[['volume']].copy()\n",
    "        for ma in [7, 20]:\n",
    "            ma_vol_df[f'{ma}_day_MA_Volume'] = ma_vol_df['volume'].rolling(window=ma, min_periods=1).mean()\n",
    "\n",
    "        # Join Volume moving averages\n",
    "        combined_data_df = combined_data_df.join(ma_vol_df, how='left', rsuffix='_vol')\n",
    "\n",
    "        # Fill NaN values for moving average columns\n",
    "        for column in combined_data_df.columns:\n",
    "            if '_MA' in column:\n",
    "                combined_data_df[column].fillna(method='ffill', inplace=True)\n",
    "                combined_data_df[column].fillna(method='bfill', inplace=True)\n",
    "           \n",
    "        # fill Close for Bollinger band display only\n",
    "        combined_data_df['close_fill'] = combined_data_df['close']    \n",
    "        fill_columns = ['close_fill']\n",
    "\n",
    "        for column in fill_columns:\n",
    "                combined_data_df[column].fillna(method='ffill', inplace=True)\n",
    "                combined_data_df[column].fillna(method='bfill', inplace=True)\n",
    "        \n",
    "\n",
    "        # Create High Volume Flags\n",
    "        combined_data_df['High_Volume_7'] = (combined_data_df['volume'] > combined_data_df['7_day_MA_Volume']).astype(int)\n",
    "        combined_data_df['High_Volume_20'] = (combined_data_df['volume'] > combined_data_df['20_day_MA_Volume']).astype(int)\n",
    "\n",
    "        # Apply forward fill and backfill for numerical values\n",
    "        # combined_data_df.fillna(method='ffill', inplace=True)\n",
    "        #combined_data_df.fillna(method='bfill', inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "        # Step 8: Add trend columns for scaled values\n",
    "        combined_data_df['3_day_avg_combined_compound_ma_7'] = combined_data_df['combined_compound_ma_7'].rolling(window=3).mean()\n",
    "        combined_data_df['3_day_avg_combined_compound_ma_7_prev'] = combined_data_df['3_day_avg_combined_compound_ma_7'].shift(1)\n",
    "        combined_data_df['3_day_avg_combined_compound_ma_7_trend'] = (combined_data_df['3_day_avg_combined_compound_ma_7'] > combined_data_df['3_day_avg_combined_compound_ma_7_prev']).astype(int)\n",
    "\n",
    "        # Step 9: Calculate differences between scaled values and close moving averages\n",
    "        if 'close_ma_7' in combined_data_df.columns and 'scaled_combined_compound_ma_7' in combined_data_df.columns:\n",
    "            combined_data_df['ma_7_diff'] = combined_data_df['close_ma_7'] - combined_data_df['scaled_combined_compound_ma_7']\n",
    "            mean_difference = combined_data_df['ma_7_diff'].mean()\n",
    "            std_difference = combined_data_df['ma_7_diff'].std()\n",
    "            combined_data_df['ma_7_diff_std'] = (combined_data_df['ma_7_diff'] - mean_difference) / std_difference\n",
    "        else:\n",
    "            print(f\"Required columns not found for {grp_term}. Skipping difference calculations.\")\n",
    "            \n",
    "            \n",
    "        # Use the extended data for Bollinger Band calculation\n",
    "        # Step 2: Calculate the standard deviation for sentiment using 'combined_compound_ma_7' over the extended data range\n",
    "        combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
    "\n",
    "        # Step 3: Calculate the sentiment Upper and Lower Bollinger Bands using the extended data\n",
    "        sentiment_ma_column = 'combined_compound_ma_7'\n",
    "        num_std_dev = 2\n",
    "        combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
    "        combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n",
    "\n",
    "        # Step 4: Ensure sentiment_Upper_Band is always above sentiment_Lower_Band\n",
    "        mask = combined_data_df['sentiment_Upper_Band'] < combined_data_df['sentiment_Lower_Band']\n",
    "        combined_data_df.loc[mask, ['sentiment_Upper_Band', 'sentiment_Lower_Band']] = combined_data_df.loc[mask, ['sentiment_Lower_Band', 'sentiment_Upper_Band']].values\n",
    "\n",
    "        # Step 5: Calculate the price Bollinger Bands using the close price moving average ('close_ma_21') over the extended data range\n",
    "        price_ma_column = 'close_ma_21'\n",
    "        combined_data_df['price_STD'] = combined_data_df[price_ma_column].rolling(window=20).std()\n",
    "\n",
    "        # Calculate Upper and Lower Bollinger Bands for the price\n",
    "        combined_data_df['price_Upper_Band'] = combined_data_df[price_ma_column] + (combined_data_df['price_STD'] * num_std_dev)\n",
    "        combined_data_df['price_Lower_Band'] = combined_data_df[price_ma_column] - (combined_data_df['price_STD'] * num_std_dev)\n",
    "\n",
    "        # Step 6: Calculate sentiment divergence based on aligned data\n",
    "        # This is done on the extended data, and later we'll trim the result\n",
    "        sentiment_divergence = combined_data_df[['sentiment_Upper_Band', 'sentiment_Lower_Band']].sub(\n",
    "            combined_data_df[['price_Upper_Band', 'price_Lower_Band']].values\n",
    "        )\n",
    "        sentiment_divergence['divergence'] = sentiment_divergence.abs().sum(axis=1)\n",
    "\n",
    "        # Add sentiment divergence to the DataFrame\n",
    "        combined_data_df['sentiment_divergence'] = sentiment_divergence['divergence']\n",
    "\n",
    "        # Step 7: Calculate the adjusted overlap upper and lower bands\n",
    "        combined_data_df['boll_upper_overlap_band'] = combined_data_df.apply(\n",
    "            lambda row: calculate_boll_upper_advanced(\n",
    "                row['price_Upper_Band'], \n",
    "                row['sentiment_Lower_Band'], \n",
    "                row['sentiment_Upper_Band'], \n",
    "                row['price_Lower_Band']\n",
    "            ), axis=1\n",
    "        )\n",
    "\n",
    "        combined_data_df['boll_lower_overlap_band'] = combined_data_df.apply(\n",
    "            lambda row: calculate_boll_lower_advanced(\n",
    "                row['price_Lower_Band'], \n",
    "                row['sentiment_Upper_Band'], \n",
    "                row['sentiment_Lower_Band'], \n",
    "                row['price_Upper_Band']\n",
    "            ), axis=1\n",
    "        )\n",
    "        \n",
    "        # Crossovers\n",
    "        # Calculate the combined normalized scores for all moving averages\n",
    "        for ma in [7, 21, 50, 100, 200]:\n",
    "            sentiment_ma_col = f'combined_compound_ma_{ma}'\n",
    "            price_ma_col = f'close_ma_{ma}'\n",
    "\n",
    "            combined_data_df[sentiment_ma_col].fillna(method='ffill', inplace=True)\n",
    "            combined_data_df[price_ma_col].fillna(method='ffill', inplace=True)\n",
    "\n",
    "            normalized_sentiment_col = f'normalized_sentiment_{ma}'\n",
    "            normalized_price_col = f'normalized_price_{ma}'\n",
    "\n",
    "            combined_data_df[normalized_sentiment_col] = normalize_column(combined_data_df, sentiment_ma_col)\n",
    "            combined_data_df[normalized_price_col] = normalize_column(combined_data_df, price_ma_col)\n",
    "\n",
    "            # Calculate combined normalized score using weights\n",
    "            combined_data_df[f'combined_normalized_score_{ma}'] = (\n",
    "                combined_data_df[normalized_sentiment_col] * 0.8 +\n",
    "                combined_data_df[normalized_price_col] * 0.2\n",
    "            )\n",
    "        # Iterate over each moving average period and calculate crossovers dynamically\n",
    "        for ma in [7, 21, 50, 100, 200]:\n",
    "            # Dynamic column names\n",
    "            normalized_sentiment_col = f'normalized_sentiment_{ma}'\n",
    "            normalized_price_col = f'normalized_price_{ma}'\n",
    "            crossover_column = f'crossover_{ma}'\n",
    "            crossover_type_column = f'crossover_type_{ma}'\n",
    "\n",
    "            # Calculate the crossover points for each moving average using normalized columns and store the 'Close' value\n",
    "            combined_data_df[crossover_column] = np.where(\n",
    "                (combined_data_df[normalized_sentiment_col] > combined_data_df[normalized_price_col]) & \n",
    "                (combined_data_df[normalized_sentiment_col].shift(1) <= combined_data_df[normalized_price_col].shift(1)) |\n",
    "                (combined_data_df[normalized_sentiment_col] < combined_data_df[normalized_price_col]) & \n",
    "                (combined_data_df[normalized_sentiment_col].shift(1) >= combined_data_df[normalized_price_col].shift(1)), \n",
    "                combined_data_df['close'], \n",
    "                np.nan\n",
    "            )\n",
    "\n",
    "            # Define the crossover type (up or down)\n",
    "            combined_data_df[crossover_type_column] = np.where(\n",
    "                (combined_data_df[normalized_sentiment_col] > combined_data_df[normalized_price_col]) & \n",
    "                (combined_data_df[normalized_sentiment_col].shift(1) <= combined_data_df[normalized_price_col].shift(1)),\n",
    "                'cross_up',\n",
    "                np.where(\n",
    "                    (combined_data_df[normalized_sentiment_col] < combined_data_df[normalized_price_col]) & \n",
    "                    (combined_data_df[normalized_sentiment_col].shift(1) >= combined_data_df[normalized_price_col].shift(1)),\n",
    "                    'cross_down',\n",
    "                    np.nan\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        combined_data_df['prev_close_up_down'] = combined_data_df['close'].diff().apply(lambda x: 1 if x > 0 else 0)\n",
    "        \n",
    "        \n",
    "        # === DEBUG BLOCK: Check for 2025-xx-xx in all relevant DataFrames ===\n",
    "        debug_date = end_date\n",
    "\n",
    "        print(\"\\nðŸ” DEBUG CHECK â€” Row presence for: \")\n",
    "\n",
    "        for label, df in {\n",
    "            \"extended_price_df\": extended_price_df,\n",
    "            \"ma_db_df\": ma_db_df,\n",
    "            \"signals_df\": signals_df,\n",
    "            \"pred_db_df\": pred_db_df,\n",
    "            \"combined_data_df (final)\": combined_data_df\n",
    "        }.items():\n",
    "            if df is not None and not df.empty:\n",
    "                try:\n",
    "                    df.index = pd.to_datetime(df.index)  # ensure datetime index\n",
    "                    has_row = debug_date in df.index\n",
    "                    print(f\"âœ… {label}: {'FOUND' if has_row else 'âŒ MISSING'}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ {label}: Error checking â€” {e}\")\n",
    "            else:\n",
    "                print(f\"âš ï¸ {label} is None or empty.\")\n",
    "\n",
    "        # Step 8: Trim the DataFrame to fit the original date window (start_date to end_date)\n",
    "        combined_data_df = combined_data_df.loc[start_date:end_date]\n",
    "\n",
    "        # Step 11: Remove duplicate columns if they exist\n",
    "        combined_data_df = combined_data_df.loc[:, ~combined_data_df.columns.duplicated()]\n",
    "\n",
    "        # Step 12: Store the final DataFrame in the dictionary\n",
    "        moving_averages_dict[grp_term] = combined_data_df.copy()\n",
    "\n",
    "# Step 13: Concatenate all DataFrames from the dictionary after processing each term\n",
    "final_combined_data_df = pd.concat(moving_averages_dict.values(), axis=0)\n",
    "\n",
    "# Optionally, reset index if you need to work with the 'date' column directly\n",
    "final_combined_data_df.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "# Display the final DataFrame or inspect it as needed\n",
    "print(\"Final Combined DataFrame:\")\n",
    "display(final_combined_data_df.tail(20))\n",
    "\n",
    "# Save the final combined DataFrame to a CSV file\n",
    "final_combined_data_df.to_csv('final_combined_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171c232c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd310fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157b2c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb01194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
