{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3dbe07a",
   "metadata": {},
   "source": [
    "### ğŸš€ Part 1: Database Setup & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "99ba6adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2025-06-08'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from scipy.stats import pearsonr\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Database Connection\n",
    "DB_PARAMS = \"postgresql://postgres:Ilpmnl!69gg@localhost:5432/twt_snt\"\n",
    "engine = create_engine(DB_PARAMS)\n",
    "\n",
    "# Dynamic Rolling Period\n",
    "window_length = 45\n",
    "\n",
    "\n",
    "# Get current UTC date\n",
    "utc_now = datetime.now(timezone.utc)\n",
    "\n",
    "end_date = utc_now.strftime('%Y-%m-%d')\n",
    "start_date = (utc_now - timedelta(days=window_length)).strftime('%Y-%m-%d')\n",
    "run_date = utc_now.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "# Terms & Moving Averages\n",
    "TERMS = [ \n",
    "       'ETH', 'SOL', 'KAS', 'LINK', 'ADA', 'MATIC', 'AVAX', 'DOGE',  'BTC', 'POPCAT',  'SUI', 'HNT', 'WIF',\n",
    "#        'TSLA', 'GOOGL', 'TSMC', 'CVX', 'COIN', 'NFLX', 'DIS','AMZN', 'MSFT', 'AAPL', 'GME', 'NVDA', 'JPM'\n",
    "#, 'DXY'\n",
    " \n",
    "        ]\n",
    "\n",
    "MOVING_AVERAGES = [7, 21, 50, 100, 200]\n",
    "DEFAULT_WEIGHTS = {'twitter': 0.96, 'reddit': 0.02, 'news': 0.02}\n",
    "end_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe990f6b",
   "metadata": {},
   "source": [
    "### ğŸš€ Part 2: Helper Functions (Data Fetching, Date Standardization, Confidence Interval Calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e60e3cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data from PostgreSQL\n",
    "def fetch_data(query):\n",
    "    try:\n",
    "        # Add connection recycling\n",
    "        engine.dispose()\n",
    "        return pd.read_sql(query, engine)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error executing query: {e}\")\n",
    "        # Return a DataFrame with expected columns\n",
    "        return pd.DataFrame(columns=['date', 'term', 'vader_compound', 'roberta_pos', 'roberta_neg', \n",
    "                                   'combined_compound', 'source', 'url', 'content_type'])\n",
    "\n",
    "# Standardize date column to YYYY-MM-DD format\n",
    "def standardize_datetime(df, column='date'):\n",
    "    if column in df.columns:\n",
    "        df[column] = pd.to_datetime(df[column], errors='coerce', utc=True).dt.date\n",
    "    return df\n",
    "\n",
    "# Calculate Pearson Correlation Confidence Interval\n",
    "def calculate_confidence_interval(correlation, n, confidence_level=0.95):\n",
    "    if n <= 3 or correlation in [-1, 1]:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    z = np.arctanh(correlation)\n",
    "    se = 1 / np.sqrt(n - 3)\n",
    "    z_critical = stats.norm.ppf((1 + confidence_level) / 2)\n",
    "    \n",
    "    lower = np.tanh(z - z_critical * se)\n",
    "    upper = np.tanh(z + z_critical * se)\n",
    "    return lower, upper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a14238",
   "metadata": {},
   "source": [
    "### ğŸš€ Part 3: Sentiment Aggregation & Moving Average Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "411c7757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Sentiment Data\n",
    "def aggregate_sentiment(start_date, end_date, term, source_weights, ma_list):\n",
    "    # SQL Queries\n",
    "    twitter_query = f\"\"\"\n",
    "        SELECT date, term, vader_compound, roberta_pos, roberta_neg \n",
    "        FROM twt_tbl \n",
    "        WHERE term = '{term}' AND date BETWEEN '{start_date}' AND '{end_date}'\n",
    "    \"\"\"\n",
    "    # Combined reddit posts + comments query\n",
    "    reddit_query = f\"\"\"\n",
    "        -- Posts\n",
    "        SELECT \n",
    "            date_created AS date, \n",
    "            db_term AS term, \n",
    "            vader_compound, \n",
    "            roberta_pos, \n",
    "            roberta_neg,\n",
    "            url,\n",
    "            'post' AS content_type\n",
    "        FROM reddit_tbl \n",
    "        WHERE db_term = '{term}' AND date_created BETWEEN '{start_date}' AND '{end_date}'\n",
    "        \n",
    "        UNION ALL\n",
    "        \n",
    "        -- Comments\n",
    "        SELECT \n",
    "            c.date_created AS date, \n",
    "            r.db_term AS term, \n",
    "            c.vader_compound, \n",
    "            c.roberta_pos, \n",
    "            c.roberta_neg,\n",
    "            c.post_url AS url,\n",
    "            'comment' AS content_type\n",
    "        FROM reddit_comments_tbl c\n",
    "        JOIN reddit_tbl r ON c.post_url = r.url\n",
    "        WHERE r.db_term = '{term}' AND c.date_created BETWEEN '{start_date}' AND '{end_date}'\n",
    "    \"\"\"\n",
    "    \n",
    "    articles_query = f\"\"\"\n",
    "        SELECT published_date AS date, term, vader_compound \n",
    "        FROM articles_tbl \n",
    "        WHERE term = '{term}' AND published_date BETWEEN '{start_date}' AND '{end_date}'\n",
    "    \"\"\"\n",
    "\n",
    "    # Fetch Data\n",
    "    twitter_df = fetch_data(twitter_query)\n",
    "    reddit_df = fetch_data(reddit_query)\n",
    "    articles_df = fetch_data(articles_query)\n",
    "\n",
    "    # Standardize dates\n",
    "    for df in [twitter_df, reddit_df, articles_df]:\n",
    "        standardize_datetime(df, 'date')\n",
    "\n",
    "    # Process Twitter\n",
    "    if not twitter_df.empty:\n",
    "        twitter_df['roberta_compound'] = twitter_df['roberta_pos'] - twitter_df['roberta_neg']\n",
    "        twitter_df['combined_compound'] = (twitter_df['vader_compound'] * 0.5) + (twitter_df['roberta_compound'] * 0.5)\n",
    "        twitter_df['source'] = 'twitter'\n",
    "        twitter_df['source_weight'] = source_weights['twitter']\n",
    "\n",
    "    # Process Reddit (posts + comments)\n",
    "    if not reddit_df.empty:\n",
    "        reddit_df['roberta_compound'] = reddit_df['roberta_pos'] - reddit_df['roberta_neg']\n",
    "        reddit_df['combined_compound'] = (reddit_df['vader_compound'] * 0.5) + (reddit_df['roberta_compound'] * 0.5)\n",
    "        reddit_df['source'] = 'reddit'\n",
    "        \n",
    "        # Different weights for posts vs comments\n",
    "        reddit_df['source_weight'] = np.where(\n",
    "            reddit_df['content_type'] == 'post',\n",
    "            source_weights['reddit'] * 0.6,  # Higher weight for posts\n",
    "            source_weights['reddit'] * 0.4    # Lower weight for comments\n",
    "        )\n",
    "    # Process Articles (VADER only)\n",
    "    if not articles_df.empty:\n",
    "        articles_df['combined_compound'] = articles_df['vader_compound']\n",
    "\n",
    "    # Add source labels\n",
    "    twitter_df['source'] = 'twitter'\n",
    "    reddit_df['source'] = 'reddit'\n",
    "    articles_df['source'] = 'news'\n",
    "\n",
    "    # Merge all into one DataFrame\n",
    "    sentiment_df = pd.concat([twitter_df, reddit_df, articles_df], ignore_index=True)\n",
    "    sentiment_df = sentiment_df[['date', 'term', 'source', 'combined_compound']].dropna()\n",
    "\n",
    "    # Daily averages before weighting\n",
    "    sentiment_df = sentiment_df.groupby(['date', 'source']).agg({'combined_compound': 'mean'}).reset_index()\n",
    "\n",
    "    # Pivot original sentiment (unweighted)\n",
    "    original_sentiment_df = sentiment_df.pivot(index='date', columns='source', values='combined_compound').reset_index()\n",
    "\n",
    "    # Apply weights\n",
    "    sentiment_df['weight'] = sentiment_df['source'].map(source_weights)\n",
    "    sentiment_df['weighted_sentiment'] = sentiment_df['combined_compound'] * sentiment_df['weight']\n",
    "\n",
    "    # Pivot weighted sentiment\n",
    "    weighted_sentiment_df = sentiment_df.pivot(index='date', columns='source', values='weighted_sentiment').reset_index()\n",
    "\n",
    "    # Merge original + weighted\n",
    "    sentiment_df = pd.merge(original_sentiment_df, weighted_sentiment_df, on='date', suffixes=('_raw', '_weighted'))\n",
    "\n",
    "    # Replace zeros with NaN (no data cases)\n",
    "    for source in ['twitter', 'reddit', 'news']:\n",
    "        for suffix in ['raw', 'weighted']:\n",
    "            col = f\"{source}_{suffix}\"\n",
    "            if col in sentiment_df.columns:\n",
    "                sentiment_df[col] = sentiment_df[col].replace(0, np.nan)\n",
    "\n",
    "    # Fill missing columns with NaN\n",
    "    for source in ['twitter', 'reddit', 'news']:\n",
    "        for suffix in ['raw', 'weighted']:\n",
    "            col = f\"{source}_{suffix}\"\n",
    "            if col not in sentiment_df.columns:\n",
    "                sentiment_df[col] = np.nan\n",
    "\n",
    "    # Final combined weighted sentiment\n",
    "    sentiment_df['combined_compound'] = sentiment_df[['twitter_weighted', 'reddit_weighted', 'news_weighted']].sum(axis=1, skipna=True)\n",
    "\n",
    "    # Calculate Moving Averages (Exponential Weighted)\n",
    "    for ma in ma_list:\n",
    "        sentiment_df[f'combined_compound_ma_{ma}'] = sentiment_df['combined_compound'].ewm(span=ma, adjust=False).mean()\n",
    "\n",
    "    sentiment_df['term'] = term  # Add term column\n",
    "\n",
    "    return sentiment_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba221d4",
   "metadata": {},
   "source": [
    "### ğŸš€ Part 4: Weight Optimization Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "56d0adce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize Sentiment Weights\n",
    "def optimize_weights(price_df, ma_list, term):\n",
    "    best_results = {}\n",
    "    print(f\"\\nğŸ”¹ Optimizing weights for {term}\\n\")\n",
    "    \n",
    "    # Standardize price date column\n",
    "    price_df = standardize_datetime(price_df, 'date')\n",
    "    \n",
    "    for ma in ma_list:\n",
    "        best_corr = -1\n",
    "        best_weights = None\n",
    "        best_p_value = None\n",
    "        best_confidence_interval = None\n",
    "\n",
    "        for w in product(np.arange(0, 1.1, 0.1), repeat=3):\n",
    "            if sum(w) != 1:\n",
    "                continue  # Ensure sum = 1\n",
    "\n",
    "            test_weights = {'twitter': w[0], 'reddit': w[1], 'news': w[2]}\n",
    "            # Aggregate sentiment for test weights\n",
    "            temp_sentiment = aggregate_sentiment(start_date, end_date, term, test_weights, ma_list)\n",
    "            temp_sentiment = standardize_datetime(temp_sentiment, 'date')\n",
    "\n",
    "            # Merge sentiment & price data\n",
    "            merged_df = pd.merge(temp_sentiment, price_df, on='date', how='inner')\n",
    "\n",
    "            if merged_df.empty:\n",
    "                continue\n",
    "\n",
    "            clean_df = merged_df[['close', f'combined_compound_ma_{ma}']].dropna()\n",
    "            if len(clean_df) > 1:\n",
    "                corr, p_value = pearsonr(clean_df['close'], clean_df[f'combined_compound_ma_{ma}'])\n",
    "                confidence_interval = calculate_confidence_interval(corr, len(clean_df))\n",
    "\n",
    "                if corr > 0 and corr > best_corr:\n",
    "                    best_corr = corr\n",
    "                    best_weights = test_weights\n",
    "                    best_p_value = p_value\n",
    "                    best_confidence_interval = confidence_interval\n",
    "\n",
    "        if best_weights is None:\n",
    "            # Default fallback\n",
    "            best_weights = DEFAULT_WEIGHTS\n",
    "            best_corr = 0\n",
    "            best_p_value = np.nan\n",
    "            best_confidence_interval = (np.nan, np.nan)\n",
    "            print(f\"âš ï¸ No positive correlation found for MA {ma}. Using default weights.\")\n",
    "\n",
    "        # Store best result\n",
    "        best_results[ma] = {\n",
    "            'weights': best_weights,\n",
    "            'correlation': round(best_corr, 12),\n",
    "            'p_value': round(best_p_value, 15) if best_p_value is not None else np.nan,\n",
    "            'confidence_interval_lower': round(best_confidence_interval[0], 4) if best_confidence_interval else np.nan,\n",
    "            'confidence_interval_upper': round(best_confidence_interval[1], 4) if best_confidence_interval else np.nan\n",
    "        }\n",
    "\n",
    "        print(f\"   ğŸ“Š MA {ma}:\")\n",
    "        print(f\"      ğŸ‹ï¸ Weights â†’ Twitter: {best_weights['twitter']:.2f}, Reddit: {best_weights['reddit']:.2f}, News: {best_weights['news']:.2f}\")\n",
    "        print(f\"      ğŸ“ˆ Correlation: {best_corr:.4f}\")\n",
    "        print(f\"      ğŸ” P-value: {best_p_value:.15f}\")\n",
    "        print(f\"      ğŸ¯ 95% CI: ({best_confidence_interval[0]:.4f}, {best_confidence_interval[1]:.4f})\\n\")\n",
    "\n",
    "    return best_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b381cf",
   "metadata": {},
   "source": [
    "### ğŸš€ Part 5: Execution of Weight Optimization Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f3b73bce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Starting optimization for ETH\n",
      "\n",
      "ğŸ”¹ Optimizing weights for ETH\n",
      "\n",
      "   ğŸ“Š MA 7:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.80, Reddit: 0.00, News: 0.20\n",
      "      ğŸ“ˆ Correlation: 0.6006\n",
      "      ğŸ” P-value: 0.000010214344590\n",
      "      ğŸ¯ 95% CI: (0.3758, 0.7586)\n",
      "\n",
      "   ğŸ“Š MA 21:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.80, Reddit: 0.10, News: 0.10\n",
      "      ğŸ“ˆ Correlation: 0.8677\n",
      "      ğŸ” P-value: 0.000000000000006\n",
      "      ğŸ¯ 95% CI: (0.7718, 0.9250)\n",
      "\n",
      "   ğŸ“Š MA 50:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.50, Reddit: 0.50, News: 0.00\n",
      "      ğŸ“ˆ Correlation: 0.8750\n",
      "      ğŸ” P-value: 0.000000000000002\n",
      "      ğŸ¯ 95% CI: (0.7837, 0.9292)\n",
      "\n",
      "   ğŸ“Š MA 100:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.00, Reddit: 1.00, News: 0.00\n",
      "      ğŸ“ˆ Correlation: 0.8557\n",
      "      ğŸ” P-value: 0.000000000000036\n",
      "      ğŸ¯ 95% CI: (0.7522, 0.9179)\n",
      "\n",
      "   ğŸ“Š MA 200:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.00, Reddit: 1.00, News: 0.00\n",
      "      ğŸ“ˆ Correlation: 0.8386\n",
      "      ğŸ” P-value: 0.000000000000348\n",
      "      ğŸ¯ 95% CI: (0.7248, 0.9079)\n",
      "\n",
      "\n",
      "ğŸš€ Starting optimization for SOL\n",
      "\n",
      "ğŸ”¹ Optimizing weights for SOL\n",
      "\n",
      "   ğŸ“Š MA 7:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.50, Reddit: 0.00, News: 0.50\n",
      "      ğŸ“ˆ Correlation: 0.5810\n",
      "      ğŸ” P-value: 0.000023042890731\n",
      "      ğŸ¯ 95% CI: (0.3496, 0.7455)\n",
      "\n",
      "   ğŸ“Š MA 21:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.00, Reddit: 0.00, News: 1.00\n",
      "      ğŸ“ˆ Correlation: 0.7675\n",
      "      ğŸ” P-value: 0.000000000490389\n",
      "      ğŸ¯ 95% CI: (0.6140, 0.8650)\n",
      "\n",
      "   ğŸ“Š MA 50:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.00, Reddit: 0.00, News: 1.00\n",
      "      ğŸ“ˆ Correlation: 0.6565\n",
      "      ğŸ” P-value: 0.000000725837878\n",
      "      ğŸ¯ 95% CI: (0.4525, 0.7953)\n",
      "\n",
      "   ğŸ“Š MA 100:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.00, Reddit: 0.00, News: 1.00\n",
      "      ğŸ“ˆ Correlation: 0.5307\n",
      "      ğŸ” P-value: 0.000148197985523\n",
      "      ğŸ¯ 95% CI: (0.2842, 0.7114)\n",
      "\n",
      "   ğŸ“Š MA 200:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.00, Reddit: 0.00, News: 1.00\n",
      "      ğŸ“ˆ Correlation: 0.4572\n",
      "      ğŸ” P-value: 0.001402458073713\n",
      "      ğŸ¯ 95% CI: (0.1924, 0.6599)\n",
      "\n",
      "\n",
      "ğŸš€ Starting optimization for KAS\n",
      "\n",
      "ğŸ”¹ Optimizing weights for KAS\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ“Š MA 7:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.40, Reddit: 0.50, News: 0.10\n",
      "      ğŸ“ˆ Correlation: 0.3786\n",
      "      ğŸ” P-value: 0.009465950090434\n",
      "      ğŸ¯ 95% CI: (0.0992, 0.6027)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ“Š MA 21:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.40, Reddit: 0.10, News: 0.50\n",
      "      ğŸ“ˆ Correlation: 0.2641\n",
      "      ğŸ” P-value: 0.076085408387071\n",
      "      ğŸ¯ 95% CI: (-0.0283, 0.5150)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ“Š MA 50:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.70, Reddit: 0.00, News: 0.30\n",
      "      ğŸ“ˆ Correlation: 0.0603\n",
      "      ğŸ” P-value: 0.690413052251597\n",
      "      ğŸ¯ 95% CI: (-0.2341, 0.3446)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ No positive correlation found for MA 100. Using default weights.\n",
      "   ğŸ“Š MA 100:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.96, Reddit: 0.02, News: 0.02\n",
      "      ğŸ“ˆ Correlation: 0.0000\n",
      "      ğŸ” P-value: nan\n",
      "      ğŸ¯ 95% CI: (nan, nan)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ No positive correlation found for MA 200. Using default weights.\n",
      "   ğŸ“Š MA 200:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.96, Reddit: 0.02, News: 0.02\n",
      "      ğŸ“ˆ Correlation: 0.0000\n",
      "      ğŸ” P-value: nan\n",
      "      ğŸ¯ 95% CI: (nan, nan)\n",
      "\n",
      "\n",
      "ğŸš€ Starting optimization for LINK\n",
      "\n",
      "ğŸ”¹ Optimizing weights for LINK\n",
      "\n",
      "   ğŸ“Š MA 7:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.60, Reddit: 0.20, News: 0.20\n",
      "      ğŸ“ˆ Correlation: 0.3637\n",
      "      ğŸ” P-value: 0.012981035387732\n",
      "      ğŸ¯ 95% CI: (0.0820, 0.5915)\n",
      "\n",
      "   ğŸ“Š MA 21:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.60, Reddit: 0.20, News: 0.20\n",
      "      ğŸ“ˆ Correlation: 0.2938\n",
      "      ğŸ” P-value: 0.047505893516944\n",
      "      ğŸ¯ 95% CI: (0.0038, 0.5382)\n",
      "\n",
      "   ğŸ“Š MA 50:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.50, Reddit: 0.10, News: 0.40\n",
      "      ğŸ“ˆ Correlation: 0.2340\n",
      "      ğŸ” P-value: 0.117614031504355\n",
      "      ğŸ¯ 95% CI: (-0.0605, 0.4909)\n",
      "\n",
      "   ğŸ“Š MA 100:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.20, Reddit: 0.00, News: 0.80\n",
      "      ğŸ“ˆ Correlation: 0.2611\n",
      "      ğŸ” P-value: 0.079716913775702\n",
      "      ğŸ¯ 95% CI: (-0.0316, 0.5125)\n",
      "\n",
      "   ğŸ“Š MA 200:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.20, Reddit: 0.00, News: 0.80\n",
      "      ğŸ“ˆ Correlation: 0.2796\n",
      "      ğŸ” P-value: 0.059852478685477\n",
      "      ğŸ¯ 95% CI: (-0.0116, 0.5271)\n",
      "\n",
      "\n",
      "ğŸš€ Starting optimization for ADA\n",
      "\n",
      "ğŸ”¹ Optimizing weights for ADA\n",
      "\n",
      "   ğŸ“Š MA 7:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 1.00, Reddit: 0.00, News: 0.00\n",
      "      ğŸ“ˆ Correlation: 0.2307\n",
      "      ğŸ” P-value: 0.122914416351951\n",
      "      ğŸ¯ 95% CI: (-0.0639, 0.4883)\n",
      "\n",
      "   ğŸ“Š MA 21:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 1.00, Reddit: 0.00, News: 0.00\n",
      "      ğŸ“ˆ Correlation: 0.0911\n",
      "      ğŸ” P-value: 0.547125109533568\n",
      "      ğŸ¯ 95% CI: (-0.2046, 0.3716)\n",
      "\n",
      "âš ï¸ No positive correlation found for MA 50. Using default weights.\n",
      "   ğŸ“Š MA 50:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.96, Reddit: 0.02, News: 0.02\n",
      "      ğŸ“ˆ Correlation: 0.0000\n",
      "      ğŸ” P-value: nan\n",
      "      ğŸ¯ 95% CI: (nan, nan)\n",
      "\n",
      "   ğŸ“Š MA 100:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.00, Reddit: 1.00, News: 0.00\n",
      "      ğŸ“ˆ Correlation: 0.0480\n",
      "      ğŸ” P-value: 0.751407454279562\n",
      "      ğŸ¯ 95% CI: (-0.2457, 0.3337)\n",
      "\n",
      "   ğŸ“Š MA 200:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.00, Reddit: 1.00, News: 0.00\n",
      "      ğŸ“ˆ Correlation: 0.0952\n",
      "      ğŸ” P-value: 0.529134212431995\n",
      "      ğŸ¯ 95% CI: (-0.2006, 0.3751)\n",
      "\n",
      "\n",
      "ğŸš€ Starting optimization for MATIC\n",
      "\n",
      "ğŸ”¹ Optimizing weights for MATIC\n",
      "\n",
      "âš ï¸ No positive correlation found for MA 7. Using default weights.\n",
      "   ğŸ“Š MA 7:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.96, Reddit: 0.02, News: 0.02\n",
      "      ğŸ“ˆ Correlation: 0.0000\n",
      "      ğŸ” P-value: nan\n",
      "      ğŸ¯ 95% CI: (nan, nan)\n",
      "\n",
      "   ğŸ“Š MA 21:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.00, Reddit: 1.00, News: 0.00\n",
      "      ğŸ“ˆ Correlation: 0.2475\n",
      "      ğŸ” P-value: 0.097242475043843\n",
      "      ğŸ¯ 95% CI: (-0.0461, 0.5018)\n",
      "\n",
      "   ğŸ“Š MA 50:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.00, Reddit: 1.00, News: 0.00\n",
      "      ğŸ“ˆ Correlation: 0.4574\n",
      "      ğŸ” P-value: 0.001394573149419\n",
      "      ğŸ¯ 95% CI: (0.1927, 0.6600)\n",
      "\n",
      "   ğŸ“Š MA 100:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.00, Reddit: 1.00, News: 0.00\n",
      "      ğŸ“ˆ Correlation: 0.5309\n",
      "      ğŸ” P-value: 0.000147289384163\n",
      "      ğŸ¯ 95% CI: (0.2844, 0.7115)\n",
      "\n",
      "   ğŸ“Š MA 200:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.00, Reddit: 1.00, News: 0.00\n",
      "      ğŸ“ˆ Correlation: 0.5643\n",
      "      ğŸ” P-value: 0.000044120055660\n",
      "      ğŸ¯ 95% CI: (0.3277, 0.7343)\n",
      "\n",
      "\n",
      "ğŸš€ Starting optimization for AVAX\n",
      "\n",
      "ğŸ”¹ Optimizing weights for AVAX\n",
      "\n",
      "   ğŸ“Š MA 7:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.00, Reddit: 0.00, News: 1.00\n",
      "      ğŸ“ˆ Correlation: 0.3628\n",
      "      ğŸ” P-value: 0.014318879229131\n",
      "      ğŸ¯ 95% CI: (0.0775, 0.5931)\n",
      "\n",
      "   ğŸ“Š MA 21:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.10, Reddit: 0.00, News: 0.90\n",
      "      ğŸ“ˆ Correlation: 0.1846\n",
      "      ğŸ” P-value: 0.224786940512485\n",
      "      ğŸ¯ 95% CI: (-0.1152, 0.4536)\n",
      "\n",
      "   ğŸ“Š MA 50:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.50, Reddit: 0.00, News: 0.50\n",
      "      ğŸ“ˆ Correlation: 0.0708\n",
      "      ğŸ” P-value: 0.643859197891596\n",
      "      ğŸ¯ 95% CI: (-0.2274, 0.3569)\n",
      "\n",
      "   ğŸ“Š MA 100:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 1.00, Reddit: 0.00, News: 0.00\n",
      "      ğŸ“ˆ Correlation: 0.1030\n",
      "      ğŸ” P-value: 0.500924100151328\n",
      "      ğŸ¯ 95% CI: (-0.1965, 0.3849)\n",
      "\n",
      "   ğŸ“Š MA 200:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 1.00, Reddit: 0.00, News: 0.00\n",
      "      ğŸ“ˆ Correlation: 0.1302\n",
      "      ğŸ” P-value: 0.393890287742905\n",
      "      ğŸ¯ 95% CI: (-0.1698, 0.4082)\n",
      "\n",
      "\n",
      "ğŸš€ Starting optimization for DOGE\n",
      "\n",
      "ğŸ”¹ Optimizing weights for DOGE\n",
      "\n",
      "   ğŸ“Š MA 7:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.70, Reddit: 0.10, News: 0.20\n",
      "      ğŸ“ˆ Correlation: 0.5373\n",
      "      ğŸ” P-value: 0.000118044092922\n",
      "      ğŸ¯ 95% CI: (0.2926, 0.7159)\n",
      "\n",
      "   ğŸ“Š MA 21:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 1.00, Reddit: 0.00, News: 0.00\n",
      "      ğŸ“ˆ Correlation: 0.6978\n",
      "      ğŸ” P-value: 0.000000070570575\n",
      "      ğŸ¯ 95% CI: (0.5110, 0.8217)\n",
      "\n",
      "   ğŸ“Š MA 50:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 1.00, Reddit: 0.00, News: 0.00\n",
      "      ğŸ“ˆ Correlation: 0.5596\n",
      "      ğŸ” P-value: 0.000052686610668\n",
      "      ğŸ¯ 95% CI: (0.3216, 0.7311)\n",
      "\n",
      "   ğŸ“Š MA 100:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 1.00, Reddit: 0.00, News: 0.00\n",
      "      ğŸ“ˆ Correlation: 0.4584\n",
      "      ğŸ” P-value: 0.001357531063668\n",
      "      ğŸ¯ 95% CI: (0.1939, 0.6607)\n",
      "\n",
      "   ğŸ“Š MA 200:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 1.00, Reddit: 0.00, News: 0.00\n",
      "      ğŸ“ˆ Correlation: 0.4008\n",
      "      ğŸ” P-value: 0.005777532634928\n",
      "      ğŸ¯ 95% CI: (0.1250, 0.6191)\n",
      "\n",
      "\n",
      "ğŸš€ Starting optimization for BTC\n",
      "\n",
      "ğŸ”¹ Optimizing weights for BTC\n",
      "\n",
      "   ğŸ“Š MA 7:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.70, Reddit: 0.00, News: 0.30\n",
      "      ğŸ“ˆ Correlation: 0.3689\n",
      "      ğŸ” P-value: 0.011633439762091\n",
      "      ğŸ¯ 95% CI: (0.0881, 0.5955)\n",
      "\n",
      "   ğŸ“Š MA 21:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.40, Reddit: 0.00, News: 0.60\n",
      "      ğŸ“ˆ Correlation: 0.8071\n",
      "      ğŸ” P-value: 0.000000000012506\n",
      "      ğŸ¯ 95% CI: (0.6749, 0.8891)\n",
      "\n",
      "   ğŸ“Š MA 50:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.00, Reddit: 0.10, News: 0.90\n",
      "      ğŸ“ˆ Correlation: 0.8942\n",
      "      ğŸ” P-value: 0.000000000000000\n",
      "      ğŸ¯ 95% CI: (0.8157, 0.9404)\n",
      "\n",
      "   ğŸ“Š MA 100:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.00, Reddit: 0.30, News: 0.70\n",
      "      ğŸ“ˆ Correlation: 0.8938\n",
      "      ğŸ” P-value: 0.000000000000000\n",
      "      ğŸ¯ 95% CI: (0.8149, 0.9401)\n",
      "\n",
      "   ğŸ“Š MA 200:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.00, Reddit: 0.40, News: 0.60\n",
      "      ğŸ“ˆ Correlation: 0.8914\n",
      "      ğŸ” P-value: 0.000000000000000\n",
      "      ğŸ¯ 95% CI: (0.8109, 0.9388)\n",
      "\n",
      "\n",
      "ğŸš€ Starting optimization for POPCAT\n",
      "\n",
      "ğŸ”¹ Optimizing weights for POPCAT\n",
      "\n",
      "   ğŸ“Š MA 7:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 1.00, Reddit: 0.00, News: 0.00\n",
      "      ğŸ“ˆ Correlation: 0.6188\n",
      "      ğŸ” P-value: 0.000004568882681\n",
      "      ğŸ¯ 95% CI: (0.4004, 0.7707)\n",
      "\n",
      "   ğŸ“Š MA 21:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 1.00, Reddit: 0.00, News: 0.00\n",
      "      ğŸ“ˆ Correlation: 0.5888\n",
      "      ğŸ” P-value: 0.000016779790055\n",
      "      ğŸ¯ 95% CI: (0.3600, 0.7507)\n",
      "\n",
      "   ğŸ“Š MA 50:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 1.00, Reddit: 0.00, News: 0.00\n",
      "      ğŸ“ˆ Correlation: 0.3028\n",
      "      ğŸ” P-value: 0.040803500787418\n",
      "      ğŸ¯ 95% CI: (0.0137, 0.5452)\n",
      "\n",
      "   ğŸ“Š MA 100:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 1.00, Reddit: 0.00, News: 0.00\n",
      "      ğŸ“ˆ Correlation: 0.1515\n",
      "      ğŸ” P-value: 0.315004776415640\n",
      "      ğŸ¯ 95% CI: (-0.1452, 0.4232)\n",
      "\n",
      "   ğŸ“Š MA 200:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 1.00, Reddit: 0.00, News: 0.00\n",
      "      ğŸ“ˆ Correlation: 0.0734\n",
      "      ğŸ” P-value: 0.627811775071921\n",
      "      ğŸ¯ 95% CI: (-0.2216, 0.3561)\n",
      "\n",
      "\n",
      "ğŸš€ Starting optimization for SUI\n",
      "\n",
      "ğŸ”¹ Optimizing weights for SUI\n",
      "\n",
      "   ğŸ“Š MA 7:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.40, Reddit: 0.20, News: 0.40\n",
      "      ğŸ“ˆ Correlation: 0.2887\n",
      "      ğŸ” P-value: 0.051665625180191\n",
      "      ğŸ¯ 95% CI: (-0.0017, 0.5342)\n",
      "\n",
      "   ğŸ“Š MA 21:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.20, Reddit: 0.50, News: 0.30\n",
      "      ğŸ“ˆ Correlation: 0.5755\n",
      "      ğŸ” P-value: 0.000028626966016\n",
      "      ğŸ¯ 95% CI: (0.3424, 0.7419)\n",
      "\n",
      "   ğŸ“Š MA 50:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.10, Reddit: 0.60, News: 0.30\n",
      "      ğŸ“ˆ Correlation: 0.6698\n",
      "      ğŸ” P-value: 0.000000357166782\n",
      "      ğŸ¯ 95% CI: (0.4711, 0.8038)\n",
      "\n",
      "   ğŸ“Š MA 100:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.00, Reddit: 0.70, News: 0.30\n",
      "      ğŸ“ˆ Correlation: 0.6916\n",
      "      ğŸ” P-value: 0.000000102515904\n",
      "      ğŸ¯ 95% CI: (0.5021, 0.8177)\n",
      "\n",
      "   ğŸ“Š MA 200:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.00, Reddit: 0.70, News: 0.30\n",
      "      ğŸ“ˆ Correlation: 0.7049\n",
      "      ğŸ” P-value: 0.000000045542442\n",
      "      ğŸ¯ 95% CI: (0.5212, 0.8261)\n",
      "\n",
      "\n",
      "ğŸš€ Starting optimization for HNT\n",
      "\n",
      "ğŸ”¹ Optimizing weights for HNT\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ No positive correlation found for MA 7. Using default weights.\n",
      "   ğŸ“Š MA 7:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.96, Reddit: 0.02, News: 0.02\n",
      "      ğŸ“ˆ Correlation: 0.0000\n",
      "      ğŸ” P-value: nan\n",
      "      ğŸ¯ 95% CI: (nan, nan)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ No positive correlation found for MA 21. Using default weights.\n",
      "   ğŸ“Š MA 21:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.96, Reddit: 0.02, News: 0.02\n",
      "      ğŸ“ˆ Correlation: 0.0000\n",
      "      ğŸ” P-value: nan\n",
      "      ğŸ¯ 95% CI: (nan, nan)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ No positive correlation found for MA 50. Using default weights.\n",
      "   ğŸ“Š MA 50:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.96, Reddit: 0.02, News: 0.02\n",
      "      ğŸ“ˆ Correlation: 0.0000\n",
      "      ğŸ” P-value: nan\n",
      "      ğŸ¯ 95% CI: (nan, nan)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ No positive correlation found for MA 100. Using default weights.\n",
      "   ğŸ“Š MA 100:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.96, Reddit: 0.02, News: 0.02\n",
      "      ğŸ“ˆ Correlation: 0.0000\n",
      "      ğŸ” P-value: nan\n",
      "      ğŸ¯ 95% CI: (nan, nan)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ No positive correlation found for MA 200. Using default weights.\n",
      "   ğŸ“Š MA 200:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.96, Reddit: 0.02, News: 0.02\n",
      "      ğŸ“ˆ Correlation: 0.0000\n",
      "      ğŸ” P-value: nan\n",
      "      ğŸ¯ 95% CI: (nan, nan)\n",
      "\n",
      "\n",
      "ğŸš€ Starting optimization for WIF\n",
      "\n",
      "ğŸ”¹ Optimizing weights for WIF\n",
      "\n",
      "   ğŸ“Š MA 7:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.80, Reddit: 0.00, News: 0.20\n",
      "      ğŸ“ˆ Correlation: 0.4317\n",
      "      ğŸ” P-value: 0.002736641222132\n",
      "      ğŸ¯ 95% CI: (0.1617, 0.6416)\n",
      "\n",
      "   ğŸ“Š MA 21:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.50, Reddit: 0.00, News: 0.50\n",
      "      ğŸ“ˆ Correlation: 0.7542\n",
      "      ğŸ” P-value: 0.000000001436657\n",
      "      ğŸ¯ 95% CI: (0.5939, 0.8569)\n",
      "\n",
      "   ğŸ“Š MA 50:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.40, Reddit: 0.00, News: 0.60\n",
      "      ğŸ“ˆ Correlation: 0.7931\n",
      "      ğŸ” P-value: 0.000000000049914\n",
      "      ğŸ¯ 95% CI: (0.6532, 0.8807)\n",
      "\n",
      "   ğŸ“Š MA 100:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.40, Reddit: 0.00, News: 0.60\n",
      "      ğŸ“ˆ Correlation: 0.7482\n",
      "      ğŸ” P-value: 0.000000002270746\n",
      "      ğŸ¯ 95% CI: (0.5850, 0.8532)\n",
      "\n",
      "   ğŸ“Š MA 200:\n",
      "      ğŸ‹ï¸ Weights â†’ Twitter: 0.40, Reddit: 0.00, News: 0.60\n",
      "      ğŸ“ˆ Correlation: 0.7145\n",
      "      ğŸ” P-value: 0.000000024596837\n",
      "      ğŸ¯ 95% CI: (0.5351, 0.8322)\n",
      "\n",
      "\n",
      "âœ… Weight Optimization Completed for All Terms ğŸš€\n"
     ]
    }
   ],
   "source": [
    "# Execute Weight Optimization for All Terms\n",
    "optimized_results = {}\n",
    "\n",
    "for term in TERMS:\n",
    "    print(f\"\\nğŸš€ Starting optimization for {term}\")\n",
    "    price_data = fetch_data(f\"SELECT date, term, close FROM yahoo_price_tbl WHERE term = '{term}'\")\n",
    "    \n",
    "    optimized_results[term] = optimize_weights(price_data, MOVING_AVERAGES, term)\n",
    "\n",
    "print(\"\\nâœ… Weight Optimization Completed for All Terms ğŸš€\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b9a7f9",
   "metadata": {},
   "source": [
    "### ğŸš€ Part 6: Aggregating Sentiment Data for All Terms (Using Default Weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e74fdccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentiment aggregation for term: ETH\n",
      "Processing sentiment aggregation for term: SOL\n",
      "Processing sentiment aggregation for term: KAS\n",
      "Processing sentiment aggregation for term: LINK\n",
      "Processing sentiment aggregation for term: ADA\n",
      "Processing sentiment aggregation for term: MATIC\n",
      "Processing sentiment aggregation for term: AVAX\n",
      "Processing sentiment aggregation for term: DOGE\n",
      "Processing sentiment aggregation for term: BTC\n",
      "Processing sentiment aggregation for term: POPCAT\n",
      "Processing sentiment aggregation for term: SUI\n",
      "Processing sentiment aggregation for term: HNT\n",
      "Processing sentiment aggregation for term: WIF\n",
      "\n",
      "âœ… Combined sentiment data aggregation complete for all terms.\n"
     ]
    }
   ],
   "source": [
    "# Combine Sentiment Data for All Terms Using Default Weights\n",
    "sentiment_data_all_terms = []\n",
    "\n",
    "for term in TERMS:\n",
    "    print(f\"Processing sentiment aggregation for term: {term}\")\n",
    "    sentiment_df = aggregate_sentiment(start_date, end_date, term, DEFAULT_WEIGHTS, MOVING_AVERAGES)\n",
    "    \n",
    "    # Ensure all sentiment columns exist\n",
    "    for source in ['twitter_weighted', 'reddit_weighted', 'news_weighted']:\n",
    "        if source not in sentiment_df.columns:\n",
    "            sentiment_df[source] = 0  # Fill missing sources with 0\n",
    "\n",
    "    # Final combined sentiment already calculated\n",
    "    sentiment_data_all_terms.append(sentiment_df)\n",
    "\n",
    "# Combine all terms into one DataFrame\n",
    "combined_sentiment_df = pd.concat(sentiment_data_all_terms, ignore_index=True)\n",
    "print(\"\\nâœ… Combined sentiment data aggregation complete for all terms.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37aaf6cb",
   "metadata": {},
   "source": [
    "### ğŸš€ Part 7: Flatten Optimized Results for Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4ba765b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Optimized weights flattened and ready.\n"
     ]
    }
   ],
   "source": [
    "# Flatten Optimized Results Dictionary\n",
    "flattened_data = []\n",
    "\n",
    "for term, data in optimized_results.items():\n",
    "    for ma, result in data.items():\n",
    "        row = {\n",
    "            'term': term,\n",
    "            'ma': ma,\n",
    "            'twitter_weight': result['weights']['twitter'],\n",
    "            'reddit_weight': result['weights']['reddit'],\n",
    "            'news_weight': result['weights']['news'],\n",
    "            'correlation': result['correlation'],\n",
    "            'p_value': result['p_value'],\n",
    "            'confidence_interval_lower': result['confidence_interval_lower'],\n",
    "            'confidence_interval_upper': result['confidence_interval_upper'],\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date,\n",
    "            'run_date': run_date\n",
    "        }\n",
    "        flattened_data.append(row)\n",
    "\n",
    "# Convert flattened results to DataFrame\n",
    "optimized_results_df = pd.DataFrame(flattened_data)\n",
    "\n",
    "print(\"\\nâœ… Optimized weights flattened and ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee9b720",
   "metadata": {},
   "source": [
    "### ğŸš€ Part 8: Sentiment-Price Correlation & Lag Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "11e3a7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Analyzing ETH Sentiment-Price Relationship\n",
      "\n",
      "ğŸ”¹ Analyzing SOL Sentiment-Price Relationship\n",
      "\n",
      "ğŸ”¹ Analyzing KAS Sentiment-Price Relationship\n",
      "\n",
      "ğŸ”¹ Analyzing LINK Sentiment-Price Relationship\n",
      "\n",
      "ğŸ”¹ Analyzing ADA Sentiment-Price Relationship\n",
      "\n",
      "ğŸ”¹ Analyzing MATIC Sentiment-Price Relationship\n",
      "\n",
      "ğŸ”¹ Analyzing AVAX Sentiment-Price Relationship\n",
      "\n",
      "ğŸ”¹ Analyzing DOGE Sentiment-Price Relationship\n",
      "\n",
      "ğŸ”¹ Analyzing BTC Sentiment-Price Relationship\n",
      "\n",
      "ğŸ”¹ Analyzing POPCAT Sentiment-Price Relationship\n",
      "\n",
      "ğŸ”¹ Analyzing SUI Sentiment-Price Relationship\n",
      "\n",
      "ğŸ”¹ Analyzing HNT Sentiment-Price Relationship\n",
      "\n",
      "ğŸ”¹ Analyzing WIF Sentiment-Price Relationship\n",
      "\n",
      "âœ… Correlation and Lag Analysis Completed.\n"
     ]
    }
   ],
   "source": [
    "def analyze_sentiment_price_correlation(sentiment_df, price_df, terms, ma_list=[7, 21, 50, 100, 200], max_lag=7):\n",
    "    all_correlation_results = []\n",
    "\n",
    "    for term in terms:\n",
    "        print(f\"\\nğŸ”¹ Analyzing {term} Sentiment-Price Relationship\")\n",
    "\n",
    "        # Merge sentiment & price data\n",
    "        merged_df = pd.merge(\n",
    "            sentiment_df[sentiment_df['term'] == term][['date', 'combined_compound'] + [f'combined_compound_ma_{ma}' for ma in ma_list]],\n",
    "            price_df[price_df['term'] == term][['date', 'close']],\n",
    "            on='date',\n",
    "            how='inner'\n",
    "        )\n",
    "\n",
    "        if merged_df.empty:\n",
    "            print(f\"âš ï¸ No matching data for {term}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Compute daily price returns\n",
    "        merged_df['price_return'] = merged_df['close'].pct_change()\n",
    "        merged_df.dropna(subset=['price_return'], inplace=True)\n",
    "\n",
    "        if len(merged_df) < 2:\n",
    "            print(f\"âš ï¸ Insufficient data for {term}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Iterate through each date in merged_df\n",
    "        for current_date in merged_df['date'].unique():\n",
    "            current_data = merged_df[merged_df['date'] <= current_date].copy()\n",
    "\n",
    "            for ma in ma_list:\n",
    "                ma_col = f'combined_compound_ma_{ma}'\n",
    "\n",
    "                if len(current_data) < 2:\n",
    "                    continue\n",
    "\n",
    "                correlations = {}\n",
    "\n",
    "                # Compute cross-correlation at different lags\n",
    "                for lag in range(-max_lag, max_lag + 1):\n",
    "                    current_data.loc[:, f'sentiment_lag_{lag}'] = current_data[ma_col].shift(lag)\n",
    "                    corr = current_data[['sentiment_lag_' + str(lag), 'price_return']].corr().iloc[0, 1]\n",
    "                    if pd.isna(corr):\n",
    "                        corr = 0\n",
    "                    correlations[lag] = corr\n",
    "\n",
    "                # Convert correlation results\n",
    "                correlation_df = pd.DataFrame.from_dict(correlations, orient='index', columns=['Correlation'])\n",
    "\n",
    "                # Best correlation & lag\n",
    "                best_lag = correlation_df['Correlation'].abs().idxmax()\n",
    "                best_correlation = correlation_df.loc[best_lag, 'Correlation']\n",
    "\n",
    "                # Scale only positive correlations to 20-100\n",
    "                leading_indicator_score = 20 if best_correlation <= 0 else 20 + ((best_correlation / 1) * (100 - 20))\n",
    "\n",
    "                all_correlation_results.append({\n",
    "                    'date': current_date,\n",
    "                    'term': term,\n",
    "                    'ma': ma,\n",
    "                    'best_lag': best_lag,\n",
    "                    'max_correlation': round(best_correlation, 4),\n",
    "                    'leading_indicator_score': round(leading_indicator_score, 2)\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(all_correlation_results)\n",
    "\n",
    "# Execute Correlation Analysis\n",
    "price_df_all_terms = fetch_data(\"SELECT date, term, close FROM yahoo_price_tbl\")\n",
    "correlation_results_df = analyze_sentiment_price_correlation(combined_sentiment_df, price_df_all_terms, TERMS)\n",
    "\n",
    "print(\"\\nâœ… Correlation and Lag Analysis Completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a18957",
   "metadata": {},
   "source": [
    "### ğŸš€ Part 9: Pivot, Merge & Save Final Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b620f345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Final enriched dataset successfully saved to final_merged_sentiment_price_correlation_FULL.csv ğŸš€\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['date', 'news_raw', 'reddit_raw', 'twitter_raw', 'news_weighted',\n",
       "       'reddit_weighted', 'twitter_weighted', 'combined_compound',\n",
       "       'combined_compound_ma_7', 'combined_compound_ma_21',\n",
       "       'combined_compound_ma_50', 'combined_compound_ma_100',\n",
       "       'combined_compound_ma_200', 'term', 'best_lag_ma_7', 'best_lag_ma_21',\n",
       "       'best_lag_ma_50', 'best_lag_ma_100', 'best_lag_ma_200',\n",
       "       'leading_indicator_score_ma_7', 'leading_indicator_score_ma_21',\n",
       "       'leading_indicator_score_ma_50', 'leading_indicator_score_ma_100',\n",
       "       'leading_indicator_score_ma_200', 'max_correlation_ma_7',\n",
       "       'max_correlation_ma_21', 'max_correlation_ma_50',\n",
       "       'max_correlation_ma_100', 'max_correlation_ma_200',\n",
       "       'confidence_interval_lower_ma_7', 'confidence_interval_lower_ma_21',\n",
       "       'confidence_interval_lower_ma_50', 'confidence_interval_lower_ma_100',\n",
       "       'confidence_interval_lower_ma_200', 'confidence_interval_upper_ma_7',\n",
       "       'confidence_interval_upper_ma_21', 'confidence_interval_upper_ma_50',\n",
       "       'confidence_interval_upper_ma_100', 'confidence_interval_upper_ma_200',\n",
       "       'correlation_ma_7', 'correlation_ma_21', 'correlation_ma_50',\n",
       "       'correlation_ma_100', 'correlation_ma_200', 'news_weight_ma_7',\n",
       "       'news_weight_ma_21', 'news_weight_ma_50', 'news_weight_ma_100',\n",
       "       'news_weight_ma_200', 'p_value_ma_7', 'p_value_ma_21', 'p_value_ma_50',\n",
       "       'p_value_ma_100', 'p_value_ma_200', 'reddit_weight_ma_7',\n",
       "       'reddit_weight_ma_21', 'reddit_weight_ma_50', 'reddit_weight_ma_100',\n",
       "       'reddit_weight_ma_200', 'twitter_weight_ma_7', 'twitter_weight_ma_21',\n",
       "       'twitter_weight_ma_50', 'twitter_weight_ma_100',\n",
       "       'twitter_weight_ma_200'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean correlation results\n",
    "final_results_df_cleaned = correlation_results_df.dropna(subset=['ma'])\n",
    "\n",
    "# Keep necessary columns\n",
    "final_results_df_cleaned = final_results_df_cleaned[['date', 'term', 'ma', 'best_lag', 'max_correlation', 'leading_indicator_score']]\n",
    "\n",
    "# Pivot the lag/correlation data\n",
    "pivoted_results_df = final_results_df_cleaned.pivot_table(\n",
    "    index=['date', 'term'],\n",
    "    columns='ma',\n",
    "    values=['best_lag', 'max_correlation', 'leading_indicator_score'],\n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "pivoted_results_df.columns = [f\"{col[0]}_ma_{col[1]}\" if col[1] else col[0] for col in pivoted_results_df.columns]\n",
    "\n",
    "# Merge with sentiment data\n",
    "final_merged_df = pd.merge(\n",
    "    combined_sentiment_df,\n",
    "    pivoted_results_df,\n",
    "    on=['date', 'term'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Flatten optimized results for merging\n",
    "optimized_results_flat = optimized_results_df.pivot_table(\n",
    "    index='term',\n",
    "    columns='ma',\n",
    "    values=['twitter_weight', 'reddit_weight', 'news_weight', 'correlation', 'p_value', 'confidence_interval_lower', 'confidence_interval_upper'],\n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "optimized_results_flat.columns = [f\"{col[0]}_ma_{col[1]}\" if col[1] else col[0] for col in optimized_results_flat.columns]\n",
    "\n",
    "# Merge optimized results\n",
    "final_merged_df = pd.merge(\n",
    "    final_merged_df,\n",
    "    optimized_results_flat,\n",
    "    on='term',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Save enriched DataFrame to CSV\n",
    "output_file = \"final_merged_sentiment_price_correlation_FULL.csv\"\n",
    "final_merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Final enriched dataset successfully saved to {output_file} ğŸš€\")\n",
    "final_merged_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682a8cc4",
   "metadata": {},
   "source": [
    "### ğŸš€ Part 10: PostgreSQL Insertion Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "84b2de4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data successfully inserted into snt_ma_blend_tbl\n",
      "âœ… Data successfully inserted into snt_ma_blend_detail_tbl\n"
     ]
    }
   ],
   "source": [
    "# PostgreSQL Insert Function\n",
    "def insert_to_db(df, table_name, conflict_columns):\n",
    "    if df.empty:\n",
    "        print(f\"âš ï¸ No data to insert into {table_name}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            for index, row in df.iterrows():\n",
    "                insert_query = f\"\"\"\n",
    "                    INSERT INTO {table_name} ({', '.join(df.columns)}) \n",
    "                    VALUES ({', '.join([f'%({col})s' for col in df.columns])})\n",
    "                    ON CONFLICT ({', '.join(conflict_columns)}) DO UPDATE SET\n",
    "                    {', '.join([f\"{col} = EXCLUDED.{col}\" for col in df.columns if col not in conflict_columns])};\n",
    "                \"\"\"\n",
    "                conn.execute(insert_query, row.to_dict())\n",
    "        print(f\"âœ… Data successfully inserted into {table_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error inserting into {table_name}: {e}\")\n",
    "\n",
    "# Insert combined sentiment data\n",
    "insert_to_db(combined_sentiment_df, \"snt_ma_blend_tbl\", [\"date\", \"term\"])\n",
    "\n",
    "# Insert optimized weights\n",
    "insert_to_db(optimized_results_df, \"snt_ma_blend_detail_tbl\", [\"term\", \"ma\", \"start_date\", \"end_date\", \"run_date\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90e8e91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d99a7b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "956c5b82",
   "metadata": {},
   "source": [
    "### ğŸš€ Part 10a: Flatten Detail tbl for Tableau Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1ac47b9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Flattened heatmap data saved for Tableau in sentiment_price_correlation_heatmap.csv\n",
      "\n",
      "âœ… Pivoted heatmap data saved for Tableau in sentiment_price_correlation_heatmap_pivoted.csv\n",
      "âœ… Data successfully inserted into snt_ma_correlation_summary_tbl\n",
      "\n",
      "âœ… snt_ma_correlation_summary table updated ğŸš€\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize Storage for Heatmap Data\n",
    "heatmap_data = []\n",
    "\n",
    "for term in TERMS:\n",
    "    for ma in [7, 21, 50, 100, 200]:  # Moving averages\n",
    "        ma_col = f'combined_compound_ma_{ma}'\n",
    "\n",
    "        # Fetch Sentiment Data\n",
    "        sentiment_query = f\"\"\"\n",
    "        SELECT date, term, {ma_col}\n",
    "        FROM snt_ma_blend_tbl\n",
    "        WHERE term = '{term}'\n",
    "        \"\"\"\n",
    "        sentiment_df = fetch_data(sentiment_query)\n",
    "\n",
    "        # Fetch Price Data\n",
    "        price_query = f\"\"\"\n",
    "        SELECT date, term, close\n",
    "        FROM yahoo_price_tbl\n",
    "        WHERE term = '{term}'\n",
    "        \"\"\"\n",
    "        price_df = fetch_data(price_query)\n",
    "\n",
    "        # Merge DataFrames on 'date'\n",
    "        merged_df = pd.merge(\n",
    "            sentiment_df[['date', ma_col]],\n",
    "            price_df[['date', 'close']],\n",
    "            on='date',\n",
    "            how='inner'\n",
    "        )\n",
    "\n",
    "        if merged_df.empty:\n",
    "            continue\n",
    "\n",
    "        # Compute Daily Returns\n",
    "        merged_df['price_return'] = merged_df['close'].pct_change()\n",
    "        merged_df.dropna(subset=['price_return', ma_col], inplace=True)\n",
    "\n",
    "        # Compute Cross-Correlation at Different Lags\n",
    "        correlations = {}\n",
    "        for lag in range(-7, 8):  # Lag from -7 to +7 days\n",
    "            merged_df[f'sentiment_lag_{lag}'] = merged_df[ma_col].shift(lag)\n",
    "            correlations[lag] = merged_df[[f'sentiment_lag_{lag}', 'price_return']].corr().iloc[0, 1]\n",
    "\n",
    "        # Store heatmap data\n",
    "        most_recent_date = sentiment_df['date'].max()\n",
    "        for lag, corr_value in correlations.items():\n",
    "            heatmap_data.append({\n",
    "                'date': most_recent_date,\n",
    "                'term': term,\n",
    "                'ma': ma,\n",
    "                'lag': lag,\n",
    "                'correlation': round(corr_value, 4)\n",
    "            })\n",
    "\n",
    "# Convert to DataFrame\n",
    "heatmap_df = pd.DataFrame(heatmap_data)\n",
    "\n",
    "# âœ… Pivot the data to have one row per date, term, and ma with lag correlations as columns\n",
    "flattened_heatmap_df = heatmap_df.pivot_table(\n",
    "    index=['date', 'term', 'ma'],\n",
    "    columns='lag',\n",
    "    values='correlation',\n",
    "    aggfunc='first'\n",
    ")\n",
    "\n",
    "# Flatten the MultiIndex columns and rename them appropriately\n",
    "flattened_heatmap_df.columns = [f\"lag_{int(lag)}\" for lag in flattened_heatmap_df.columns]\n",
    "flattened_heatmap_df.reset_index(inplace=True)\n",
    "\n",
    "# âœ… Format 'ma' column as 'MA 7', 'MA 21', etc.\n",
    "flattened_heatmap_df[\"ma\"] = flattened_heatmap_df[\"ma\"].apply(lambda x: f\"MA {x}\")\n",
    "\n",
    "# âœ… Save flattened heatmap to CSV for Tableau\n",
    "heatmap_output_file = \"sentiment_price_correlation_heatmap.csv\"\n",
    "flattened_heatmap_df.to_csv(heatmap_output_file, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Flattened heatmap data saved for Tableau in {heatmap_output_file}\")\n",
    "\n",
    "# âœ… Pivot the lag columns back to original format for Tableau\n",
    "pivoted_heatmap_df = pd.melt(\n",
    "    flattened_heatmap_df,\n",
    "    id_vars=[\"date\", \"term\", \"ma\"],\n",
    "    var_name=\"Lag\",\n",
    "    value_name=\"Correlation\"\n",
    ")\n",
    "\n",
    "# Clean up the 'Lag' column to only retain the numerical value\n",
    "pivoted_heatmap_df[\"Lag\"] = pivoted_heatmap_df[\"Lag\"].str.replace(\"lag_\", \"\").astype(int)\n",
    "\n",
    "# âœ… Save the pivoted version for Tableau\n",
    "pivoted_heatmap_output_file = \"sentiment_price_correlation_heatmap_pivoted.csv\"\n",
    "pivoted_heatmap_df.to_csv(pivoted_heatmap_output_file, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Pivoted heatmap data saved for Tableau in {pivoted_heatmap_output_file}\")\n",
    "\n",
    "\n",
    "# Insert summary table as usual\n",
    "insert_to_db(final_results_df_cleaned, \"snt_ma_correlation_summary_tbl\", [\"date\", \"term\", \"ma\"])\n",
    "print(f\"\\nâœ… snt_ma_correlation_summary table updated ğŸš€\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a1f26c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>term</th>\n",
       "      <th>ma</th>\n",
       "      <th>best_lag</th>\n",
       "      <th>max_correlation</th>\n",
       "      <th>leading_indicator_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-04-26</td>\n",
       "      <td>ETH</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-04-26</td>\n",
       "      <td>ETH</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-04-26</td>\n",
       "      <td>ETH</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-04-26</td>\n",
       "      <td>ETH</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-04-26</td>\n",
       "      <td>ETH</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2845</th>\n",
       "      <td>2025-06-08</td>\n",
       "      <td>WIF</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.4961</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2846</th>\n",
       "      <td>2025-06-08</td>\n",
       "      <td>WIF</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.4876</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2847</th>\n",
       "      <td>2025-06-08</td>\n",
       "      <td>WIF</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.4418</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2848</th>\n",
       "      <td>2025-06-08</td>\n",
       "      <td>WIF</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.4155</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2849</th>\n",
       "      <td>2025-06-08</td>\n",
       "      <td>WIF</td>\n",
       "      <td>200</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.4000</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2850 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date term   ma  best_lag  max_correlation  leading_indicator_score\n",
       "0     2025-04-26  ETH    7         0           1.0000                    100.0\n",
       "1     2025-04-26  ETH   21         0           1.0000                    100.0\n",
       "2     2025-04-26  ETH   50         0           1.0000                    100.0\n",
       "3     2025-04-26  ETH  100         0           1.0000                    100.0\n",
       "4     2025-04-26  ETH  200         0           1.0000                    100.0\n",
       "...          ...  ...  ...       ...              ...                      ...\n",
       "2845  2025-06-08  WIF    7         4          -0.4961                     20.0\n",
       "2846  2025-06-08  WIF   21         4          -0.4876                     20.0\n",
       "2847  2025-06-08  WIF   50         4          -0.4418                     20.0\n",
       "2848  2025-06-08  WIF  100         4          -0.4155                     20.0\n",
       "2849  2025-06-08  WIF  200         4          -0.4000                     20.0\n",
       "\n",
       "[2850 rows x 6 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results_df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a4380738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>term</th>\n",
       "      <th>ma</th>\n",
       "      <th>lag</th>\n",
       "      <th>correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-06-08</td>\n",
       "      <td>ETH</td>\n",
       "      <td>7</td>\n",
       "      <td>-7</td>\n",
       "      <td>-0.0155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-06-08</td>\n",
       "      <td>ETH</td>\n",
       "      <td>7</td>\n",
       "      <td>-6</td>\n",
       "      <td>0.0215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-06-08</td>\n",
       "      <td>ETH</td>\n",
       "      <td>7</td>\n",
       "      <td>-5</td>\n",
       "      <td>-0.0841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-06-08</td>\n",
       "      <td>ETH</td>\n",
       "      <td>7</td>\n",
       "      <td>-4</td>\n",
       "      <td>-0.0167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-06-08</td>\n",
       "      <td>ETH</td>\n",
       "      <td>7</td>\n",
       "      <td>-3</td>\n",
       "      <td>-0.0028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>2025-06-08</td>\n",
       "      <td>WIF</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>2025-06-08</td>\n",
       "      <td>WIF</td>\n",
       "      <td>200</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>2025-06-08</td>\n",
       "      <td>WIF</td>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>2025-06-08</td>\n",
       "      <td>WIF</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "      <td>0.1012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>2025-06-08</td>\n",
       "      <td>WIF</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "      <td>0.1336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>975 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date term   ma  lag  correlation\n",
       "0    2025-06-08  ETH    7   -7      -0.0155\n",
       "1    2025-06-08  ETH    7   -6       0.0215\n",
       "2    2025-06-08  ETH    7   -5      -0.0841\n",
       "3    2025-06-08  ETH    7   -4      -0.0167\n",
       "4    2025-06-08  ETH    7   -3      -0.0028\n",
       "..          ...  ...  ...  ...          ...\n",
       "970  2025-06-08  WIF  200    3       0.0580\n",
       "971  2025-06-08  WIF  200    4       0.1013\n",
       "972  2025-06-08  WIF  200    5       0.0685\n",
       "973  2025-06-08  WIF  200    6       0.1012\n",
       "974  2025-06-08  WIF  200    7       0.1336\n",
       "\n",
       "[975 rows x 5 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heatmap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2ccd41e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>term</th>\n",
       "      <th>ma</th>\n",
       "      <th>lag_-7</th>\n",
       "      <th>lag_-6</th>\n",
       "      <th>lag_-5</th>\n",
       "      <th>lag_-4</th>\n",
       "      <th>lag_-3</th>\n",
       "      <th>lag_-2</th>\n",
       "      <th>lag_-1</th>\n",
       "      <th>lag_0</th>\n",
       "      <th>lag_1</th>\n",
       "      <th>lag_2</th>\n",
       "      <th>lag_3</th>\n",
       "      <th>lag_4</th>\n",
       "      <th>lag_5</th>\n",
       "      <th>lag_6</th>\n",
       "      <th>lag_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-06-07</td>\n",
       "      <td>HNT</td>\n",
       "      <td>MA 7</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>-0.0478</td>\n",
       "      <td>-0.0657</td>\n",
       "      <td>-0.0243</td>\n",
       "      <td>-0.0090</td>\n",
       "      <td>-0.0299</td>\n",
       "      <td>-0.0908</td>\n",
       "      <td>-0.1604</td>\n",
       "      <td>0.1388</td>\n",
       "      <td>-0.0420</td>\n",
       "      <td>-0.0554</td>\n",
       "      <td>0.0448</td>\n",
       "      <td>0.0831</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.0727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-06-07</td>\n",
       "      <td>HNT</td>\n",
       "      <td>MA 21</td>\n",
       "      <td>-0.0142</td>\n",
       "      <td>-0.0559</td>\n",
       "      <td>-0.0918</td>\n",
       "      <td>-0.0432</td>\n",
       "      <td>-0.0179</td>\n",
       "      <td>-0.0179</td>\n",
       "      <td>-0.0774</td>\n",
       "      <td>-0.1528</td>\n",
       "      <td>0.1542</td>\n",
       "      <td>-0.0260</td>\n",
       "      <td>-0.0462</td>\n",
       "      <td>0.0634</td>\n",
       "      <td>0.0952</td>\n",
       "      <td>0.0470</td>\n",
       "      <td>0.0879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-06-07</td>\n",
       "      <td>HNT</td>\n",
       "      <td>MA 50</td>\n",
       "      <td>-0.0120</td>\n",
       "      <td>-0.0520</td>\n",
       "      <td>-0.0878</td>\n",
       "      <td>-0.0455</td>\n",
       "      <td>-0.0034</td>\n",
       "      <td>-0.0064</td>\n",
       "      <td>-0.0664</td>\n",
       "      <td>-0.1379</td>\n",
       "      <td>0.1701</td>\n",
       "      <td>-0.0023</td>\n",
       "      <td>-0.0235</td>\n",
       "      <td>0.0786</td>\n",
       "      <td>0.1090</td>\n",
       "      <td>0.0668</td>\n",
       "      <td>0.0982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-06-07</td>\n",
       "      <td>HNT</td>\n",
       "      <td>MA 100</td>\n",
       "      <td>-0.0055</td>\n",
       "      <td>-0.0471</td>\n",
       "      <td>-0.0762</td>\n",
       "      <td>-0.0402</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0595</td>\n",
       "      <td>-0.1224</td>\n",
       "      <td>0.1758</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>-0.0058</td>\n",
       "      <td>0.0864</td>\n",
       "      <td>0.1225</td>\n",
       "      <td>0.0796</td>\n",
       "      <td>0.1028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-06-07</td>\n",
       "      <td>HNT</td>\n",
       "      <td>MA 200</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>-0.0428</td>\n",
       "      <td>-0.0651</td>\n",
       "      <td>-0.0337</td>\n",
       "      <td>0.0291</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>-0.0542</td>\n",
       "      <td>-0.1088</td>\n",
       "      <td>0.1756</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0900</td>\n",
       "      <td>0.1322</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.1043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2025-06-08</td>\n",
       "      <td>WIF</td>\n",
       "      <td>MA 7</td>\n",
       "      <td>-0.0110</td>\n",
       "      <td>-0.1319</td>\n",
       "      <td>-0.0486</td>\n",
       "      <td>0.0265</td>\n",
       "      <td>-0.1083</td>\n",
       "      <td>-0.0505</td>\n",
       "      <td>-0.0270</td>\n",
       "      <td>-0.1787</td>\n",
       "      <td>0.0315</td>\n",
       "      <td>0.0305</td>\n",
       "      <td>-0.0205</td>\n",
       "      <td>0.0473</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>0.0828</td>\n",
       "      <td>0.1028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2025-06-08</td>\n",
       "      <td>WIF</td>\n",
       "      <td>MA 21</td>\n",
       "      <td>-0.0358</td>\n",
       "      <td>-0.1364</td>\n",
       "      <td>-0.0770</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>-0.1132</td>\n",
       "      <td>-0.0641</td>\n",
       "      <td>-0.0171</td>\n",
       "      <td>-0.1394</td>\n",
       "      <td>0.0546</td>\n",
       "      <td>0.0457</td>\n",
       "      <td>-0.0024</td>\n",
       "      <td>0.0496</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0886</td>\n",
       "      <td>0.1168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2025-06-08</td>\n",
       "      <td>WIF</td>\n",
       "      <td>MA 50</td>\n",
       "      <td>-0.0257</td>\n",
       "      <td>-0.1122</td>\n",
       "      <td>-0.0638</td>\n",
       "      <td>0.0224</td>\n",
       "      <td>-0.0845</td>\n",
       "      <td>-0.0433</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>-0.0976</td>\n",
       "      <td>0.0806</td>\n",
       "      <td>0.0687</td>\n",
       "      <td>0.0242</td>\n",
       "      <td>0.0710</td>\n",
       "      <td>0.0415</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.1321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2025-06-08</td>\n",
       "      <td>WIF</td>\n",
       "      <td>MA 100</td>\n",
       "      <td>-0.0077</td>\n",
       "      <td>-0.0862</td>\n",
       "      <td>-0.0425</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>-0.0560</td>\n",
       "      <td>-0.0227</td>\n",
       "      <td>0.0291</td>\n",
       "      <td>-0.0679</td>\n",
       "      <td>0.0992</td>\n",
       "      <td>0.0894</td>\n",
       "      <td>0.0451</td>\n",
       "      <td>0.0899</td>\n",
       "      <td>0.0584</td>\n",
       "      <td>0.1040</td>\n",
       "      <td>0.1362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2025-06-08</td>\n",
       "      <td>WIF</td>\n",
       "      <td>MA 200</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>-0.0667</td>\n",
       "      <td>-0.0258</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>-0.0354</td>\n",
       "      <td>-0.0086</td>\n",
       "      <td>0.0410</td>\n",
       "      <td>-0.0480</td>\n",
       "      <td>0.1096</td>\n",
       "      <td>0.1027</td>\n",
       "      <td>0.0580</td>\n",
       "      <td>0.1013</td>\n",
       "      <td>0.0685</td>\n",
       "      <td>0.1012</td>\n",
       "      <td>0.1336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date term      ma  lag_-7  lag_-6  lag_-5  lag_-4  lag_-3  lag_-2  \\\n",
       "0   2025-06-07  HNT    MA 7  0.0071 -0.0478 -0.0657 -0.0243 -0.0090 -0.0299   \n",
       "1   2025-06-07  HNT   MA 21 -0.0142 -0.0559 -0.0918 -0.0432 -0.0179 -0.0179   \n",
       "2   2025-06-07  HNT   MA 50 -0.0120 -0.0520 -0.0878 -0.0455 -0.0034 -0.0064   \n",
       "3   2025-06-07  HNT  MA 100 -0.0055 -0.0471 -0.0762 -0.0402  0.0146  0.0019   \n",
       "4   2025-06-07  HNT  MA 200  0.0004 -0.0428 -0.0651 -0.0337  0.0291  0.0076   \n",
       "..         ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "60  2025-06-08  WIF    MA 7 -0.0110 -0.1319 -0.0486  0.0265 -0.1083 -0.0505   \n",
       "61  2025-06-08  WIF   MA 21 -0.0358 -0.1364 -0.0770  0.0103 -0.1132 -0.0641   \n",
       "62  2025-06-08  WIF   MA 50 -0.0257 -0.1122 -0.0638  0.0224 -0.0845 -0.0433   \n",
       "63  2025-06-08  WIF  MA 100 -0.0077 -0.0862 -0.0425  0.0363 -0.0560 -0.0227   \n",
       "64  2025-06-08  WIF  MA 200  0.0063 -0.0667 -0.0258  0.0453 -0.0354 -0.0086   \n",
       "\n",
       "    lag_-1   lag_0   lag_1   lag_2   lag_3   lag_4   lag_5   lag_6   lag_7  \n",
       "0  -0.0908 -0.1604  0.1388 -0.0420 -0.0554  0.0448  0.0831  0.0256  0.0727  \n",
       "1  -0.0774 -0.1528  0.1542 -0.0260 -0.0462  0.0634  0.0952  0.0470  0.0879  \n",
       "2  -0.0664 -0.1379  0.1701 -0.0023 -0.0235  0.0786  0.1090  0.0668  0.0982  \n",
       "3  -0.0595 -0.1224  0.1758  0.0152 -0.0058  0.0864  0.1225  0.0796  0.1028  \n",
       "4  -0.0542 -0.1088  0.1756  0.0269  0.0064  0.0900  0.1322  0.0869  0.1043  \n",
       "..     ...     ...     ...     ...     ...     ...     ...     ...     ...  \n",
       "60 -0.0270 -0.1787  0.0315  0.0305 -0.0205  0.0473  0.0156  0.0828  0.1028  \n",
       "61 -0.0171 -0.1394  0.0546  0.0457 -0.0024  0.0496  0.0208  0.0886  0.1168  \n",
       "62  0.0093 -0.0976  0.0806  0.0687  0.0242  0.0710  0.0415  0.1010  0.1321  \n",
       "63  0.0291 -0.0679  0.0992  0.0894  0.0451  0.0899  0.0584  0.1040  0.1362  \n",
       "64  0.0410 -0.0480  0.1096  0.1027  0.0580  0.1013  0.0685  0.1012  0.1336  \n",
       "\n",
       "[65 rows x 18 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_heatmap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ab896e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c74de4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba72d009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a17276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ed815e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
