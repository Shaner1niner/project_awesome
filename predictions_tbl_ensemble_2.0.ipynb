{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dda825ed",
   "metadata": {},
   "source": [
    "1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17676c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c2a009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# Define your date range and terms\n",
    "# Get today's date (end_date)\n",
    "end_date = (datetime.today() + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "# Get start_date (90 days before today)\n",
    "start_date = time.strftime('%Y-%m-%d', time.localtime(time.time() - 200 * 86400))\n",
    "# Calculate the extended date range for sentiment analysis\n",
    "extended_start_date = pd.to_datetime(start_date) - pd.Timedelta(days=21)\n",
    "\n",
    "# Convert start and end dates to Timestamp for consistency and compatibility\n",
    "start_date = pd.to_datetime(start_date)\n",
    "end_date = pd.to_datetime(end_date)\n",
    "\n",
    "\n",
    "grp_terms = ['SOL', 'KAS', 'LINK', 'ADA', 'MATIC', 'AVAX', 'POPCAT', 'SUI', 'HNT', 'WIF', 'BTC', 'DOGE','ETH' \n",
    "             ,'GME', 'NVDA','JPM', 'GOOGL','DXY', 'TSMC', 'CVX', 'COIN', 'AMZN', 'MSFT', 'NFLX', 'DIS', 'AAPL', 'TSLA'\n",
    "            ]\n",
    "\n",
    "# Initialize a dictionary to collect DataFrames for each term\n",
    "combined_data_dict = {}\n",
    "# Create dictionaries to store different types of data\n",
    "price_data_dict = {}  # Stores the raw price data\n",
    "technical_indicators_dict = {}  # Stores the extended data with technical indicators\n",
    "filtered_data_dict = {}  # Stores the filtered data after applying the technical indicators\n",
    "\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    'dbname': 'twt_snt',\n",
    "    'user': 'postgres',\n",
    "    'password': 'Ilpmnl!69gg',\n",
    "    'host': 'localhost',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "# Function to fetch tweets from the database\n",
    "def fetch_tweets(start_date, end_date, term):\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_params)\n",
    "        cursor = conn.cursor()\n",
    "        query = \"\"\"\n",
    "            SELECT * FROM twt_tbl\n",
    "            WHERE term = %s AND date BETWEEN %s AND %s\n",
    "        \"\"\"\n",
    "        cursor.execute(query, (term, extended_start_date, end_date))\n",
    "        rows = cursor.fetchall()\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        df = pd.DataFrame(rows, columns=columns)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching tweets: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "# Function to fetch moving averages from the database\n",
    "def fetch_moving_averages(extended_start_date, end_date, term):\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_params)\n",
    "        cursor = conn.cursor()\n",
    "        query = \"\"\"\n",
    "            SELECT date, term, combined_compound_ma_7, combined_compound_ma_21, \n",
    "                   combined_compound_ma_50, combined_compound_ma_100, combined_compound_ma_200, combined_compound\n",
    "            FROM snt_ma_blend_tbl\n",
    "            WHERE term = %s AND date BETWEEN %s AND %s\n",
    "        \"\"\"\n",
    "        cursor.execute(query, (term, extended_start_date, end_date))\n",
    "        rows = cursor.fetchall()\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        df = pd.DataFrame(rows, columns=columns)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching moving averages: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "# Function to fetch data from the database\n",
    "def fetch_price_data(start_date, end_date, term):\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_params)\n",
    "        cursor = conn.cursor()\n",
    "        query = \"\"\"\n",
    "            SELECT date, term, open, high, low, close, adj_close, volume,\n",
    "            close_ma_7, close_ma_21, close_ma_50, close_ma_100, close_ma_200\n",
    "            FROM yahoo_price_tbl\n",
    "            WHERE term = %s AND date BETWEEN %s AND %s\n",
    "        \"\"\"\n",
    "        cursor.execute(query, (term, start_date, end_date))\n",
    "        rows = cursor.fetchall()\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        df = pd.DataFrame(rows, columns=columns)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching price data: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "def fetch_signals(start_date, end_date, term):\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_params)\n",
    "        cursor = conn.cursor()\n",
    "        query = \"\"\"\n",
    "            SELECT date, term, buy, sell, neutral\n",
    "            FROM signal_cnt_tbl\n",
    "            WHERE term = %s AND date BETWEEN %s AND %s\n",
    "        \"\"\"\n",
    "        cursor.execute(query, (term, start_date, end_date))\n",
    "        rows = cursor.fetchall()\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "        # Ensure datetime format and indexing\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df.set_index('date', inplace=True)\n",
    "\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching signals for term {term}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()        \n",
    "\n",
    "# Functions to calculate technical indicators (as given)\n",
    "def calculate_rsi(series, period=14):\n",
    "    delta = series.diff(1)\n",
    "    gain = delta.where(delta > 0, 0.0)\n",
    "    loss = -delta.where(delta < 0, 0.0)\n",
    "    avg_gain = gain.rolling(window=period, min_periods=1).mean()\n",
    "    avg_loss = loss.rolling(window=period, min_periods=1).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def calculate_stochastic_rsi(df, rsi_column, window=14):\n",
    "    rsi_min = df[rsi_column].rolling(window=window, min_periods=1).min()\n",
    "    rsi_max = df[rsi_column].rolling(window=window, min_periods=1).max()\n",
    "    stoch_rsi = (df[rsi_column] - rsi_min) / (rsi_max - rsi_min)\n",
    "    return stoch_rsi * 100\n",
    "\n",
    "def calculate_mfi(df, window=14):\n",
    "    typical_price = (df['high'] + df['low'] + df['close']) / 3\n",
    "    money_flow = typical_price * df['volume']\n",
    "    positive_flow = (money_flow.where(typical_price > typical_price.shift(1), 0)).rolling(window=window).sum()\n",
    "    negative_flow = (money_flow.where(typical_price < typical_price.shift(1), 0)).rolling(window=window).sum()\n",
    "    mfi = 100 - (100 / (1 + positive_flow / negative_flow))\n",
    "    return mfi\n",
    "\n",
    "def calculate_macd(series, short_window=12, long_window=26, signal_window=9):\n",
    "    short_ema = series.ewm(span=short_window, adjust=False).mean()\n",
    "    long_ema = series.ewm(span=long_window, adjust=False).mean()\n",
    "    macd = short_ema - long_ema\n",
    "    signal = macd.ewm(span=signal_window, adjust=False).mean()\n",
    "    return macd, signal\n",
    "\n",
    "\n",
    "def calculate_bollinger_bands(df, ma_column, window=20, num_std_dev=2, band_type='price'):\n",
    "    df[f'{band_type}_MA'] = df[ma_column]\n",
    "    df[f'{band_type}_STD'] = df[ma_column].rolling(window=window).std()\n",
    "    df[f'{band_type}_Upper_Band'] = df[f'{band_type}_MA'] + (df[f'{band_type}_STD'] * num_std_dev)\n",
    "    df[f'{band_type}_Lower_Band'] = df[f'{band_type}_MA'] - (df[f'{band_type}_STD'] * num_std_dev)\n",
    "    return df\n",
    "\n",
    "def scale_features_to_price(df, columns_to_scale, reference_column):\n",
    "    scaled_columns = {}\n",
    "    for col in columns_to_scale:\n",
    "        scaler = MinMaxScaler(feature_range=(df[reference_column].min(), df[reference_column].max()))\n",
    "        scaled_columns[f'scaled_{col}'] = scaler.fit_transform(df[[col]]).flatten()\n",
    "    \n",
    "    # Convert scaled_columns dictionary to a DataFrame\n",
    "    scaled_df = pd.DataFrame(scaled_columns, index=df.index)\n",
    "    \n",
    "    # Concatenate the original DataFrame with the scaled DataFrame\n",
    "    return pd.concat([df, scaled_df], axis=1)\n",
    "\n",
    "def calculate_boll_upper_advanced(boll_upper_price, boll_lower_sent, boll_upper_sent, boll_lower_price):\n",
    "    if boll_upper_price >= boll_lower_sent and boll_upper_sent >= boll_lower_price:\n",
    "        # When the bands overlap, take the minimum of the upper bounds, but ensure it's above the lower bound\n",
    "        return max(min(boll_upper_price, boll_upper_sent), boll_lower_price)\n",
    "    else:\n",
    "        # When there's no overlap, choose the upper band that is closer to the other band's lower boundary\n",
    "        if abs(boll_upper_price - boll_lower_sent) < abs(boll_upper_sent - boll_lower_price):\n",
    "            return max(boll_upper_price, boll_lower_price)\n",
    "        else:\n",
    "            return max(boll_upper_sent, boll_lower_price)\n",
    "\n",
    "def calculate_boll_lower_advanced(boll_lower_price, boll_upper_sent, boll_lower_sent, boll_upper_price):\n",
    "    if boll_lower_price <= boll_upper_sent and boll_lower_sent <= boll_upper_price:\n",
    "        # When the bands overlap, take the maximum of the lower bounds, but ensure it's below the upper bound\n",
    "        return min(max(boll_lower_price, boll_lower_sent), boll_upper_price)\n",
    "    else:\n",
    "        # When there's no overlap, choose the lower band that is closer to the other band's upper boundary\n",
    "        if abs(boll_lower_price - boll_upper_sent) < abs(boll_lower_sent - boll_upper_price):\n",
    "            return min(boll_lower_price, boll_upper_price)\n",
    "        else:\n",
    "            return min(boll_lower_sent, boll_upper_price)\n",
    "        \n",
    "def normalize_column(df, column):\n",
    "    min_val = df[column].min()\n",
    "    max_val = df[column].max()\n",
    "    return ((df[column] - min_val) / (max_val - min_val)) * 100\n",
    "        \n",
    "# Function to find divergence\n",
    "def find_MACD_price_divergence(df):\n",
    "    divergence = ['None']  # Start with 'None' for the first row since no comparison can be made\n",
    "    for i in range(1, len(df)):\n",
    "        if df['close'].iloc[i] < df['close'].iloc[i-1] and df['MACD'].iloc[i] > df['MACD'].iloc[i-1]:\n",
    "            divergence.append('Bullish MACD Price Divergence')\n",
    "        elif df['close'].iloc[i] > df['close'].iloc[i-1] and df['MACD'].iloc[i] < df['MACD'].iloc[i-1]:\n",
    "            divergence.append('Bearish MACD Price Divergence')\n",
    "        else:\n",
    "            divergence.append('None')\n",
    "    return divergence\n",
    "\n",
    "# Function to find divergence\n",
    "def find_MACD_sentiment_divergence(df):\n",
    "    divergence = ['None']  # Start with 'None' for the first row since no comparison can be made\n",
    "    for i in range(1, len(df)):\n",
    "        if df['close'].iloc[i] < df['close'].iloc[i-1] and df['Sentiment_MACD'].iloc[i] > df['Sentiment_MACD'].iloc[i-1]:\n",
    "            divergence.append('Bullish MACD Sentiment Divergence')\n",
    "        elif df['close'].iloc[i] > df['close'].iloc[i-1] and df['Sentiment_MACD'].iloc[i] < df['Sentiment_MACD'].iloc[i-1]:\n",
    "            divergence.append('Bearish MACD Sentiment Divergence')\n",
    "        else:\n",
    "            divergence.append('None')\n",
    "    return divergence\n",
    "\n",
    "\n",
    "# Function to find divergence\n",
    "def find_RSI_price_divergence(df):\n",
    "    divergence = ['None']  # Start with 'None' for the first row since no comparison can be made\n",
    "    for i in range(1, len(df)):\n",
    "        if df['close'].iloc[i] < df['close'].iloc[i-1] and df['RSI'].iloc[i] > df['RSI'].iloc[i-1]:\n",
    "            divergence.append('Bullish RSI Price Divergence')\n",
    "        elif df['close'].iloc[i] > df['close'].iloc[i-1] and df['RSI'].iloc[i] < df['RSI'].iloc[i-1]:\n",
    "            divergence.append('Bearish RSI Price Divergence')\n",
    "        else:\n",
    "            divergence.append('None')\n",
    "    return divergence\n",
    "\n",
    "# Function to find divergence\n",
    "def find_RSI_sentiment_divergence(df):\n",
    "    divergence = ['None']  # Start with 'None' for the first row since no comparison can be made\n",
    "    for i in range(1, len(df)):\n",
    "        if df['close'].iloc[i] < df['close'].iloc[i-1] and df['Sentiment_RSI'].iloc[i] > df['Sentiment_RSI'].iloc[i-1]:\n",
    "            divergence.append('Bullish RSI Sentiment Divergence')\n",
    "        elif df['close'].iloc[i] > df['close'].iloc[i-1] and df['Sentiment_RSI'].iloc[i] < df['Sentiment_RSI'].iloc[i-1]:\n",
    "            divergence.append('Bearish RSI Sentiment Divergence')\n",
    "        else:\n",
    "            divergence.append('None')\n",
    "    return divergence\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8e18b0",
   "metadata": {},
   "source": [
    "2. Data Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b509c3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing term: SOL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:323: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:328: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing term: KAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:328: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing term: LINK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:323: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:328: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing term: ADA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:323: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:328: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing term: MATIC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:323: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:328: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing term: AVAX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:323: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:328: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing term: POPCAT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:323: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:328: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing term: SUI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:323: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:328: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing term: HNT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:323: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:328: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing term: WIF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:323: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:328: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing term: BTC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:323: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:328: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing term: DOGE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:323: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:328: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing term: ETH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:323: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:328: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing term: GME\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:328: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing term: NVDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:328: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing term: JPM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:328: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing term: GOOGL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:328: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing term: DXY\n",
      "Processing term: TSMC\n",
      "Processing term: CVX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:328: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing term: COIN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:328: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing term: AMZN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:328: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing term: MSFT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:328: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing term: NFLX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:328: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing term: DIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:328: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing term: AAPL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:328: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing term: TSLA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:328: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:329: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Combined DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:438: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  final_combined_data_df.reset_index(inplace=True)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:440: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  final_combined_data_df['day_of_week'] = final_combined_data_df['date'].dt.dayofweek\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:441: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  final_combined_data_df['month'] = final_combined_data_df['date'].dt.month\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:442: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  final_combined_data_df['day_of_month'] = final_combined_data_df['date'].dt.day\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3510504269.py:443: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  final_combined_data_df['is_weekend'] = final_combined_data_df['date'].dt.dayofweek >= 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>term</th>\n",
       "      <th>combined_compound_ma_7</th>\n",
       "      <th>combined_compound_ma_21</th>\n",
       "      <th>combined_compound_ma_50</th>\n",
       "      <th>combined_compound_ma_100</th>\n",
       "      <th>combined_compound_ma_200</th>\n",
       "      <th>combined_compound</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>...</th>\n",
       "      <th>crossover_type_50</th>\n",
       "      <th>crossover_100</th>\n",
       "      <th>crossover_type_100</th>\n",
       "      <th>crossover_200</th>\n",
       "      <th>crossover_type_200</th>\n",
       "      <th>prev_close_up_down</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>is_weekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5301</th>\n",
       "      <td>2025-06-03</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.161703</td>\n",
       "      <td>0.131179</td>\n",
       "      <td>0.103489</td>\n",
       "      <td>0.076439</td>\n",
       "      <td>0.054156</td>\n",
       "      <td>0.170356</td>\n",
       "      <td>346.595</td>\n",
       "      <td>355.4000</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5302</th>\n",
       "      <td>2025-06-04</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.143425</td>\n",
       "      <td>0.127307</td>\n",
       "      <td>0.102905</td>\n",
       "      <td>0.076680</td>\n",
       "      <td>0.054499</td>\n",
       "      <td>0.088593</td>\n",
       "      <td>345.095</td>\n",
       "      <td>345.6000</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5303</th>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.108588</td>\n",
       "      <td>0.116105</td>\n",
       "      <td>0.099030</td>\n",
       "      <td>0.075242</td>\n",
       "      <td>0.053997</td>\n",
       "      <td>0.004078</td>\n",
       "      <td>322.490</td>\n",
       "      <td>324.5499</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5304</th>\n",
       "      <td>2025-06-06</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.078489</td>\n",
       "      <td>0.104476</td>\n",
       "      <td>0.094683</td>\n",
       "      <td>0.073518</td>\n",
       "      <td>0.053342</td>\n",
       "      <td>-0.011810</td>\n",
       "      <td>298.830</td>\n",
       "      <td>305.5000</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5305</th>\n",
       "      <td>2025-06-07</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.061670</td>\n",
       "      <td>0.095998</td>\n",
       "      <td>0.091410</td>\n",
       "      <td>0.072285</td>\n",
       "      <td>0.052923</td>\n",
       "      <td>0.011215</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 189 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  term  combined_compound_ma_7  combined_compound_ma_21  \\\n",
       "5301 2025-06-03  TSLA                0.161703                 0.131179   \n",
       "5302 2025-06-04  TSLA                0.143425                 0.127307   \n",
       "5303 2025-06-05  TSLA                0.108588                 0.116105   \n",
       "5304 2025-06-06  TSLA                0.078489                 0.104476   \n",
       "5305 2025-06-07  TSLA                0.061670                 0.095998   \n",
       "\n",
       "      combined_compound_ma_50  combined_compound_ma_100  \\\n",
       "5301                 0.103489                  0.076439   \n",
       "5302                 0.102905                  0.076680   \n",
       "5303                 0.099030                  0.075242   \n",
       "5304                 0.094683                  0.073518   \n",
       "5305                 0.091410                  0.072285   \n",
       "\n",
       "      combined_compound_ma_200  combined_compound     open      high  ...  \\\n",
       "5301                  0.054156           0.170356  346.595  355.4000  ...   \n",
       "5302                  0.054499           0.088593  345.095  345.6000  ...   \n",
       "5303                  0.053997           0.004078  322.490  324.5499  ...   \n",
       "5304                  0.053342          -0.011810  298.830  305.5000  ...   \n",
       "5305                  0.052923           0.011215      NaN       NaN  ...   \n",
       "\n",
       "      crossover_type_50  crossover_100 crossover_type_100  crossover_200  \\\n",
       "5301                nan            NaN                nan            NaN   \n",
       "5302                nan            NaN                nan            NaN   \n",
       "5303                nan            NaN                nan            NaN   \n",
       "5304                nan            NaN                nan            NaN   \n",
       "5305                nan            NaN                nan            NaN   \n",
       "\n",
       "      crossover_type_200  prev_close_up_down  day_of_week  month  \\\n",
       "5301                 nan                   1            1      6   \n",
       "5302                 nan                   0            2      6   \n",
       "5303                 nan                   0            3      6   \n",
       "5304                 nan                   1            4      6   \n",
       "5305                 nan                   0            5      6   \n",
       "\n",
       "      day_of_month  is_weekend  \n",
       "5301             3       False  \n",
       "5302             4       False  \n",
       "5303             5       False  \n",
       "5304             6       False  \n",
       "5305             7        True  \n",
       "\n",
       "[5 rows x 189 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define dictionaries to store different results\n",
    "moving_averages_dict = {}\n",
    "scaled_features_dict = {}\n",
    "combined_data_dict = {}\n",
    "\n",
    "# Iterate over each term in the group\n",
    "for grp_term in grp_terms:\n",
    "    print(f\"Processing term: {grp_term}\")\n",
    "\n",
    "    # Step 1: Fetch price data for the extended range\n",
    "    extended_price_df = fetch_price_data(extended_start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'), grp_term)\n",
    "    \n",
    "    if extended_price_df is None or extended_price_df.empty:\n",
    "        print(f\"No price data found for term: {grp_term}\")\n",
    "        continue\n",
    "    # Also Step 1: Fetch signals data for the extended range    \n",
    "    signals_df = fetch_signals(start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'), grp_term)\n",
    "\n",
    "    # Step 2: Set 'date' as the index and ensure it's unique\n",
    "    extended_price_df['date'] = pd.to_datetime(extended_price_df['date'])\n",
    "    extended_price_df.set_index('date', inplace=True)\n",
    "    extended_price_df = extended_price_df.loc[~extended_price_df.index.duplicated(keep='first')]\n",
    "\n",
    "    # Step 3: Fetch moving averages data\n",
    "    ma_db_df = fetch_moving_averages(start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'), grp_term)\n",
    "\n",
    "    if ma_db_df is not None and not ma_db_df.empty:\n",
    "        ma_db_df['date'] = pd.to_datetime(ma_db_df['date'])\n",
    "        ma_db_df.set_index('date', inplace=True)\n",
    "\n",
    "        # Drop the 'term' column from extended_price_df to avoid duplication during the join\n",
    "        extended_price_df.drop(columns=['term'], errors='ignore', inplace=True)\n",
    "\n",
    "        # Join moving average data to extended price data\n",
    "        combined_data_df = ma_db_df.join(extended_price_df, how='left')\n",
    "\n",
    "        # DATE RELATED ERROR CORRECTING\n",
    "        # Filter out rows with dates in the future\n",
    "        current_date = pd.Timestamp.now().normalize()\n",
    "        # combined_data_df = combined_data_df[combined_data_df.index <= current_date]\n",
    "        # Ensure the index is a proper `DatetimeIndex` and sorted\n",
    "        combined_data_df.index = pd.to_datetime(combined_data_df.index)\n",
    "        combined_data_df = combined_data_df.sort_index()    \n",
    "\n",
    "    \n",
    "        # Step 4: Calculate Technical Indicators\n",
    "        combined_data_df['RSI'] = calculate_rsi(combined_data_df['close'])\n",
    "        combined_data_df['Stochastic_RSI'] = calculate_stochastic_rsi(combined_data_df, 'RSI')\n",
    "        combined_data_df['MFI'] = calculate_mfi(combined_data_df)\n",
    "        combined_data_df['MACD'], combined_data_df['MACD_Signal'] = calculate_macd(combined_data_df['close'])\n",
    "        \n",
    "        # Step 4.1: Calculate Sentiment Technical Indicators\n",
    "        # Calculate RSI for sentiment data using 'daily_avg_combined_compound'\n",
    "        combined_data_df['Sentiment_RSI'] = calculate_rsi(combined_data_df['combined_compound'])\n",
    "        combined_data_df['Sentiment_Stochastic_RSI'] = calculate_stochastic_rsi(combined_data_df, 'Sentiment_RSI')\n",
    "        combined_data_df['Sentiment_MACD'], combined_data_df['Sentiment_MACD_Signal'] = calculate_macd(combined_data_df['combined_compound'])\n",
    "        \n",
    "        # Step 4.2-4.6: Calculate Sentiment Technical Indicators supporting features\n",
    "        # Adding stdev for RSI and Sentiment_RSI\n",
    "        #Step 4.21: Calculate the difference between RSI and Sentiment_RSI\n",
    "        combined_data_df['RSI_Difference'] = combined_data_df['RSI'] - combined_data_df['Sentiment_RSI']\n",
    "\n",
    "        # Step 4.22: Calculate the rolling standard deviation of this difference\n",
    "        # You can specify the window size (e.g., 14 days) for the rolling standard deviation\n",
    "        combined_data_df['RSI_Sentiment_STD'] = combined_data_df['RSI_Difference'].rolling(window=14).std().abs()\n",
    "        # Now, combined_data_df['RSI_Sentiment_STD'] contains the standard deviation between RSI and Sentiment_RSI\n",
    "        \n",
    "        # Step 4.23: Calculate the rolling mean of the RSI_Difference\n",
    "        combined_data_df['RSI_Difference_Mean'] = combined_data_df['RSI_Difference'].rolling(window=14).mean()\n",
    "\n",
    "        # Step 4.24: Calculate the number of standard deviations from the mean\n",
    "        combined_data_df['RSI_Difference_STD_Deviation'] = (\n",
    "            (combined_data_df['RSI_Difference'] - combined_data_df['RSI_Difference_Mean']) /\n",
    "            combined_data_df['RSI_Sentiment_STD']\n",
    "        )\n",
    "        # Now, combined_data_df['RSI_Difference_STD_Deviation'] contains the number of standard deviations from the mean for each day\n",
    "        \n",
    "        # Step 4.24: Identify if RSI_Difference_STD_Deviation is greater than 2\n",
    "        combined_data_df['RSI_STD_above_2'] = abs(combined_data_df['RSI_Difference_STD_Deviation']) > 2\n",
    "\n",
    "        # Step 4.25: execute the divergence function\n",
    "        combined_data_df['RSI_Price_Divergence'] = find_RSI_price_divergence(combined_data_df)\n",
    "        combined_data_df['RSI_Sentiment_Divergence'] = find_RSI_sentiment_divergence(combined_data_df)\n",
    "        \n",
    "        # Step 4.26: count the divergence recorded in the divergence function\n",
    "        # Calculate consecutive counts for RSI Price divergence, similar to what was done with MACD\n",
    "        combined_data_df['Consecutive_Count_RSI_Price_Divergence'] = (\n",
    "            combined_data_df['RSI_Price_Divergence']\n",
    "            .apply(lambda x: x if x != 'None' else None)\n",
    "            .groupby((combined_data_df['RSI_Price_Divergence'] != combined_data_df['RSI_Price_Divergence'].shift()).cumsum())\n",
    "            .cumcount()\n",
    "            .where(combined_data_df['RSI_Price_Divergence'] != 'None', 0)\n",
    "        )\n",
    "        \n",
    "        # Step 4.26A: Calculate consecutive counts for RSI Sentiment divergence, similar to what is done with MACD\n",
    "        combined_data_df['Consecutive_Count_RSI_Sentiment_Divergence'] = (\n",
    "            combined_data_df['RSI_Sentiment_Divergence']\n",
    "            .apply(lambda x: x if x != 'None' else None)\n",
    "            .groupby((combined_data_df['RSI_Sentiment_Divergence'] != combined_data_df['RSI_Sentiment_Divergence'].shift()).cumsum())\n",
    "            .cumcount()\n",
    "            .where(combined_data_df['RSI_Sentiment_Divergence'] != 'None', 0)\n",
    "        )\n",
    "        \n",
    "        # Step 4.27 calculate RSI_Trend_Reversal variable\n",
    "        combined_data_df['RSI_Overbought'] = (combined_data_df['RSI'] > 70) & (combined_data_df['Sentiment_RSI'] > 70)\n",
    "        combined_data_df['RSI_Oversold'] = (combined_data_df['RSI'] < 30) & (combined_data_df['Sentiment_RSI'] < 30)\n",
    "\n",
    "        # Create conditions for divergence\n",
    "        combined_data_df['Bearish_Divergence'] = (combined_data_df['RSI_Price_Divergence'] == 'Bearish RSI Price Divergence') & (combined_data_df['RSI_Sentiment_Divergence'] == 'Bearish RSI Sentiment Divergence')\n",
    "        combined_data_df['Bullish_Divergence'] = (combined_data_df['RSI_Price_Divergence'] == 'Bullish RSI Price Divergence') & (combined_data_df['RSI_Sentiment_Divergence'] == 'Bullish RSI Sentiment Divergence')\n",
    "\n",
    "        # Create a new column for RSI Trend Reversal based on overbought/oversold levels and divergence\n",
    "        combined_data_df['RSI_Trend_Reversal'] = np.where(\n",
    "            (combined_data_df['RSI_Overbought'] & combined_data_df['Bearish_Divergence']),\n",
    "            'Likely Downward Reversal',  # Bearish reversal when both RSI and Sentiment_RSI are overbought and bearish divergence occurs\n",
    "            np.where(\n",
    "                (combined_data_df['RSI_Oversold'] & combined_data_df['Bullish_Divergence']),\n",
    "                'Likely Upward Reversal',  # Bullish reversal when both RSI and Sentiment_RSI are oversold and bullish divergence occurs\n",
    "                'No Reversal'  # Default value when no reversal condition is met\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Step 4.3: Calculate the rolling mean of the stoch RSI_Difference\n",
    "        # Apply smoothing to the Sentiment_Stochastic_RSI using a moving average or EMA; Here we use a 2-period EMA for smoothing\n",
    "        smoothing_window = 2\n",
    "        combined_data_df['Smoothed_Sentiment_Stochastic_RSI'] = combined_data_df['Sentiment_Stochastic_RSI'].ewm(span=smoothing_window, adjust=False).mean()\n",
    "\n",
    "        # Adding stdev for RSI and Sentiment_RSI\n",
    "        # Step 4.31: Calculate the difference between Stochastic_RSI and Sentiment_Stochastic_RSI\n",
    "        combined_data_df['Stoch_RSI_Difference'] = combined_data_df['Stochastic_RSI'] - combined_data_df['Sentiment_Stochastic_RSI']\n",
    "\n",
    "        \n",
    "        # Step 4.32: Calculate the rolling standard deviation of this difference\n",
    "        # You can specify the window size (e.g., 14 days) for the rolling standard deviation\n",
    "        combined_data_df['Stoch_RSI_Sentiment_STD'] = combined_data_df['Stoch_RSI_Difference'].rolling(window=14).std().abs()\n",
    "\n",
    "        # Step 4.33: Calculate the rolling mean of the RSI_Difference\n",
    "        combined_data_df['Stoch_RSI_Difference_Mean'] = combined_data_df['Stoch_RSI_Difference'].rolling(window=14).mean()\n",
    "\n",
    "        # Step 4.34: Calculate the number of standard deviations from the mean\n",
    "        combined_data_df['Stoch_RSI_Difference_STD_Deviation'] = (\n",
    "            (combined_data_df['Stoch_RSI_Difference'] - combined_data_df['Stoch_RSI_Difference_Mean']) /\n",
    "            combined_data_df['Stoch_RSI_Sentiment_STD']\n",
    "        )\n",
    "        \n",
    "        # Step 4.35: Identify if Stoch_RSI_Difference_STD_Deviation is greater than 2\n",
    "        combined_data_df['Stoch_RSI_STD_above_2'] = abs(combined_data_df['Stoch_RSI_Difference_STD_Deviation']) > 2\n",
    "        \n",
    "        #  # Step 4.365: count how many periods stoch RSI is in extreme position\n",
    "        # Initialize the counter columns\n",
    "        combined_data_df['Stoch_RSI_Both_Extreme_Counter'] = 0\n",
    "\n",
    "        # Iterate through the DataFrame to update the counter\n",
    "        for i in range(1, len(combined_data_df)):\n",
    "            if combined_data_df.iloc[i]['Stochastic_RSI'] > 80 and combined_data_df.iloc[i]['Sentiment_Stochastic_RSI'] > 80:\n",
    "                if combined_data_df.iloc[i-1]['Stoch_RSI_Both_Extreme_Counter'] > 0:  # Continuation of a positive streak\n",
    "                    combined_data_df.iloc[i, combined_data_df.columns.get_loc('Stoch_RSI_Both_Extreme_Counter')] = combined_data_df.iloc[i-1]['Stoch_RSI_Both_Extreme_Counter'] + 1\n",
    "                else:  # Start of a new positive streak\n",
    "                    combined_data_df.iloc[i, combined_data_df.columns.get_loc('Stoch_RSI_Both_Extreme_Counter')] = 1\n",
    "            elif combined_data_df.iloc[i]['Stochastic_RSI'] < 20 and combined_data_df.iloc[i]['Sentiment_Stochastic_RSI'] < 20:\n",
    "                if combined_data_df.iloc[i-1]['Stoch_RSI_Both_Extreme_Counter'] < 0:  # Continuation of a negative streak\n",
    "                    combined_data_df.iloc[i, combined_data_df.columns.get_loc('Stoch_RSI_Both_Extreme_Counter')] = combined_data_df.iloc[i-1]['Stoch_RSI_Both_Extreme_Counter'] - 1\n",
    "                else:  # Start of a new negative streak\n",
    "                    combined_data_df.iloc[i, combined_data_df.columns.get_loc('Stoch_RSI_Both_Extreme_Counter')] = -1\n",
    "            else:\n",
    "                combined_data_df.iloc[i, combined_data_df.columns.get_loc('Stoch_RSI_Both_Extreme_Counter')] = 0  # Reset the counter if the condition is not met or both are zero\n",
    "\n",
    "            # Additional condition to reset counter if both values are exactly zero\n",
    "            if combined_data_df.iloc[i]['Stochastic_RSI'] == 0 and combined_data_df.iloc[i]['Sentiment_Stochastic_RSI'] == 0:\n",
    "                combined_data_df.iloc[i, combined_data_df.columns.get_loc('Stoch_RSI_Both_Extreme_Counter')] = 0\n",
    "        \n",
    "        \n",
    "        # Step 4.40 Calculate extra goodies for the MACD \n",
    "        # Scale Sentiment_MACD to the scale of MACD\n",
    "        macd_scaler = MinMaxScaler(feature_range=(combined_data_df['MACD'].min(), combined_data_df['MACD'].max()))\n",
    "        combined_data_df['scaled_Sentiment_MACD'] = macd_scaler.fit_transform(combined_data_df[['Sentiment_MACD']]).flatten()\n",
    "\n",
    "        # Step 4.41 Scale Sentiment_MACD_Signal to the scale of MACD_Signal\n",
    "        macd_signal_scaler = MinMaxScaler(feature_range=(combined_data_df['MACD_Signal'].min(), combined_data_df['MACD_Signal'].max()))\n",
    "        combined_data_df['scaled_Sentiment_MACD_Signal'] = macd_signal_scaler.fit_transform(combined_data_df[['Sentiment_MACD_Signal']]).flatten()\n",
    "\n",
    "        #Step 4.42 Calculate the MACD histogram for price data\n",
    "        combined_data_df['MACD_Histogram'] = combined_data_df['MACD'] - combined_data_df['MACD_Signal']\n",
    "\n",
    "        #Step 4.43 Calculate the MACD histogram for sentiment data\n",
    "        combined_data_df['Sentiment_MACD_Histogram'] = combined_data_df['scaled_Sentiment_MACD'] - combined_data_df['scaled_Sentiment_MACD_Signal']\n",
    "\n",
    "        #Step 4.44: Calculate the difference between MACD_Signal and Sentiment_MACD_Signal\n",
    "        combined_data_df['MACD_Signal_Difference'] = combined_data_df['MACD_Signal'] - combined_data_df['scaled_Sentiment_MACD_Signal']\n",
    "\n",
    "        #Step 4.45: Calculate the rolling standard deviation of this difference\n",
    "        combined_data_df['MACD_Signal_Sentiment_STD'] = combined_data_df['MACD_Signal_Difference'].rolling(window=14).std().abs()\n",
    "\n",
    "        #Step 4.46: Calculate the rolling mean of the MACD_Signal_Difference\n",
    "        combined_data_df['MACD_Signal_Difference_Mean'] = combined_data_df['MACD_Signal_Difference'].rolling(window=14).mean()\n",
    "\n",
    "        #Step 4.46: Calculate the number of standard deviations from the mean\n",
    "        combined_data_df['MACD_Signal_Difference_STD_Deviation'] = (\n",
    "            (combined_data_df['MACD_Signal_Difference'] - combined_data_df['MACD_Signal_Difference_Mean']) /\n",
    "            combined_data_df['MACD_Signal_Sentiment_STD']\n",
    "        )\n",
    "\n",
    "        # Step 4.47: Identify if MACD_Signal_Difference_STD_Deviation is greater than 2\n",
    "        macd_signal_condition_above_2 = abs(combined_data_df['MACD_Signal_Difference_STD_Deviation']) > 2\n",
    "\n",
    "        # Step 4.48: Record the MACD_Signal_Difference_STD_Deviation directly to the DataFrame\n",
    "        combined_data_df['MACD_Signal_trend_reversal'] = np.where(\n",
    "            macd_signal_condition_above_2,\n",
    "            combined_data_df['MACD_Signal_Difference_STD_Deviation'],\n",
    "            0\n",
    "        )\n",
    "\n",
    "        # Step 4.49: Identify if there is a cross between Sentiment_MACD_Signal and MACD_Signal and record it\n",
    "        # Capture positive and negative crosses for future analysis in Tableau\n",
    "        combined_data_df['MACD_Signal_Cross'] = np.where(\n",
    "            (combined_data_df['scaled_Sentiment_MACD_Signal'] > combined_data_df['MACD_Signal']) &\n",
    "            (combined_data_df['scaled_Sentiment_MACD_Signal'].shift(1) <= combined_data_df['MACD_Signal'].shift(1)),\n",
    "            1,  # Bullish cross\n",
    "            np.where(\n",
    "                (combined_data_df['scaled_Sentiment_MACD_Signal'] < combined_data_df['MACD_Signal']) &\n",
    "                (combined_data_df['scaled_Sentiment_MACD_Signal'].shift(1) >= combined_data_df['MACD_Signal'].shift(1)),\n",
    "                -1,  # Bearish cross\n",
    "                0  # No cross\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Step 4.491: Create a new column to capture the significance of the cross using both deviation and the MACD difference\n",
    "        combined_data_df['MACD_Cross_Significance'] = np.where(\n",
    "            combined_data_df['MACD_Signal_Cross'] != 0,\n",
    "            combined_data_df['MACD_Signal_trend_reversal'] * combined_data_df['MACD_Signal_Cross'],\n",
    "            0\n",
    "        )\n",
    "\n",
    "        # Step 4.492: Determine the direction of the Sentiment MACD Signal\n",
    "        combined_data_df['Sentiment_MACD_Signal_Direction'] = np.where(\n",
    "            combined_data_df['scaled_Sentiment_MACD_Signal'] > combined_data_df['scaled_Sentiment_MACD_Signal'].shift(1),\n",
    "            1,  # Upward direction\n",
    "            np.where(\n",
    "                combined_data_df['scaled_Sentiment_MACD_Signal'] < combined_data_df['scaled_Sentiment_MACD_Signal'].shift(1),\n",
    "                -1,  # Downward direction\n",
    "                0  # No change\n",
    "            )\n",
    "        )\n",
    "        \n",
    "\n",
    "        #Step 4.493 Find MACD Price divergence and store it in the DataFrame\n",
    "        combined_data_df['MACD_Price_Divergence'] = find_MACD_price_divergence(combined_data_df)\n",
    "        combined_data_df['Consecutive_Count_MACD_Price_Divergence'] = combined_data_df['MACD_Price_Divergence'].apply(lambda x: x if x != 'None' else None).groupby((combined_data_df['MACD_Price_Divergence'] != combined_data_df['MACD_Price_Divergence'].shift()).cumsum()).cumcount().where(combined_data_df['MACD_Price_Divergence'] != 'None', 0)\n",
    " \n",
    "        #Step 4.494 Find MACD sentiment divergence and store it in the DataFrame\n",
    "        combined_data_df['MACD_Sentiment_Divergence'] = find_MACD_sentiment_divergence(combined_data_df)\n",
    "        combined_data_df['Consecutive_Count_MACD_Sentiment_Divergence'] = combined_data_df['MACD_Sentiment_Divergence'].apply(lambda x: x if x != 'None' else None).groupby((combined_data_df['MACD_Sentiment_Divergence'] != combined_data_df['MACD_Sentiment_Divergence'].shift()).cumsum()).cumcount().where(combined_data_df['MACD_Sentiment_Divergence'] != 'None', 0)\n",
    " \n",
    "        # Step 7: Scale selected sentiment features to match the scale of the 'close' price\n",
    "        columns_to_scale = [\n",
    "            'combined_compound_ma_7', 'combined_compound_ma_21', 'combined_compound_ma_50',\n",
    "            'combined_compound_ma_100', 'combined_compound_ma_200','combined_compound'\n",
    "        ]\n",
    "        combined_data_df = scale_features_to_price(combined_data_df, columns_to_scale, 'close')\n",
    "        \n",
    "        # Step 5: Generate previous day and trend indicators\n",
    "        ma_columns = [col for col in combined_data_df.columns if 'combined_compound' in col or 'close_ma' in col or 'scaled_combined_compound' in col]\n",
    "\n",
    "        for ma_column in ma_columns:\n",
    "            combined_data_df[f'prev_{ma_column}'] = combined_data_df[ma_column].shift(1)\n",
    "            combined_data_df[f'{ma_column}_trend'] = (combined_data_df[ma_column] > combined_data_df[f'prev_{ma_column}']).astype(int)\n",
    "            combined_data_df[f'{ma_column}_pct_change'] = combined_data_df[ma_column].pct_change() * 100\n",
    "            combined_data_df[f'{ma_column}_direction_change_flag'] = combined_data_df[f'{ma_column}_trend'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
    "\n",
    "        # Step 6: Volume Moving Averages\n",
    "        ma_vol_df = combined_data_df[['volume']].copy()\n",
    "        for ma in [7, 20]:\n",
    "            ma_vol_df[f'{ma}_day_MA_Volume'] = ma_vol_df['volume'].rolling(window=ma, min_periods=1).mean()\n",
    "\n",
    "        # Join Volume moving averages\n",
    "        combined_data_df = combined_data_df.join(ma_vol_df, how='left', rsuffix='_vol')\n",
    "\n",
    "        # Fill NaN values for moving average columns\n",
    "        for column in combined_data_df.columns:\n",
    "            if '_MA' in column:\n",
    "                combined_data_df[column].fillna(method='ffill', inplace=True)\n",
    "                combined_data_df[column].fillna(method='bfill', inplace=True)\n",
    "           \n",
    "        # fill Close for Bollinger band display only\n",
    "        combined_data_df['close_fill'] = combined_data_df['close']        \n",
    "        fill_columns = ['close_fill']\n",
    "\n",
    "        for column in fill_columns:\n",
    "                combined_data_df[column].fillna(method='ffill', inplace=True)\n",
    "                combined_data_df[column].fillna(method='bfill', inplace=True)\n",
    "        \n",
    "\n",
    "        # Create High Volume Flags\n",
    "        combined_data_df['High_Volume_7'] = (combined_data_df['volume'] > combined_data_df['7_day_MA_Volume']).astype(int)\n",
    "        combined_data_df['High_Volume_20'] = (combined_data_df['volume'] > combined_data_df['20_day_MA_Volume']).astype(int)\n",
    "\n",
    "        # Apply forward fill and backfill for numerical values\n",
    "        # combined_data_df.fillna(method='ffill', inplace=True)\n",
    "        #combined_data_df.fillna(method='bfill', inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "        # Step 8: Add trend columns for scaled values\n",
    "        combined_data_df['3_day_avg_combined_compound_ma_7'] = combined_data_df['combined_compound_ma_7'].rolling(window=3).mean()\n",
    "        combined_data_df['3_day_avg_combined_compound_ma_7_prev'] = combined_data_df['3_day_avg_combined_compound_ma_7'].shift(1)\n",
    "        combined_data_df['3_day_avg_combined_compound_ma_7_trend'] = (combined_data_df['3_day_avg_combined_compound_ma_7'] > combined_data_df['3_day_avg_combined_compound_ma_7_prev']).astype(int)\n",
    "\n",
    "        # Step 9: Calculate differences between scaled values and close moving averages\n",
    "        if 'close_ma_7' in combined_data_df.columns and 'scaled_combined_compound_ma_7' in combined_data_df.columns:\n",
    "            combined_data_df['ma_7_diff'] = combined_data_df['close_ma_7'] - combined_data_df['scaled_combined_compound_ma_7']\n",
    "            mean_difference = combined_data_df['ma_7_diff'].mean()\n",
    "            std_difference = combined_data_df['ma_7_diff'].std()\n",
    "            combined_data_df['ma_7_diff_std'] = (combined_data_df['ma_7_diff'] - mean_difference) / std_difference\n",
    "        else:\n",
    "            print(f\"Required columns not found for {grp_term}. Skipping difference calculations.\")\n",
    "            \n",
    "            \n",
    "        # Use the extended data for Bollinger Band calculation\n",
    "        # Step 2: Calculate the standard deviation for sentiment using 'combined_compound_ma_7' over the extended data range\n",
    "        combined_data_df['sentiment_STD'] = combined_data_df['combined_compound_ma_7'].rolling(window=20).std()\n",
    "\n",
    "        # Step 3: Calculate the sentiment Upper and Lower Bollinger Bands using the extended data\n",
    "        sentiment_ma_column = 'combined_compound_ma_7'\n",
    "        num_std_dev = 2\n",
    "        combined_data_df['sentiment_Upper_Band'] = combined_data_df[sentiment_ma_column] + (combined_data_df['sentiment_STD'] * num_std_dev)\n",
    "        combined_data_df['sentiment_Lower_Band'] = combined_data_df[sentiment_ma_column] - (combined_data_df['sentiment_STD'] * num_std_dev)\n",
    "\n",
    "        # Step 4: Ensure sentiment_Upper_Band is always above sentiment_Lower_Band\n",
    "        mask = combined_data_df['sentiment_Upper_Band'] < combined_data_df['sentiment_Lower_Band']\n",
    "        combined_data_df.loc[mask, ['sentiment_Upper_Band', 'sentiment_Lower_Band']] = combined_data_df.loc[mask, ['sentiment_Lower_Band', 'sentiment_Upper_Band']].values\n",
    "\n",
    "        # Step 5: Calculate the price Bollinger Bands using the close price moving average ('close_ma_21') over the extended data range\n",
    "        price_ma_column = 'close_ma_21'\n",
    "        combined_data_df['price_STD'] = combined_data_df[price_ma_column].rolling(window=20).std()\n",
    "\n",
    "        # Calculate Upper and Lower Bollinger Bands for the price\n",
    "        combined_data_df['price_Upper_Band'] = combined_data_df[price_ma_column] + (combined_data_df['price_STD'] * num_std_dev)\n",
    "        combined_data_df['price_Lower_Band'] = combined_data_df[price_ma_column] - (combined_data_df['price_STD'] * num_std_dev)\n",
    "\n",
    "        # Step 6: Calculate sentiment divergence based on aligned data\n",
    "        # This is done on the extended data, and later we'll trim the result\n",
    "        sentiment_divergence = combined_data_df[['sentiment_Upper_Band', 'sentiment_Lower_Band']].sub(\n",
    "            combined_data_df[['price_Upper_Band', 'price_Lower_Band']].values\n",
    "        )\n",
    "        sentiment_divergence['divergence'] = sentiment_divergence.abs().sum(axis=1)\n",
    "\n",
    "        # Add sentiment divergence to the DataFrame\n",
    "        combined_data_df['sentiment_divergence'] = sentiment_divergence['divergence']\n",
    "\n",
    "        # Step 7: Calculate the adjusted overlap upper and lower bands\n",
    "        combined_data_df['boll_upper_overlap_band'] = combined_data_df.apply(\n",
    "            lambda row: calculate_boll_upper_advanced(\n",
    "                row['price_Upper_Band'], \n",
    "                row['sentiment_Lower_Band'], \n",
    "                row['sentiment_Upper_Band'], \n",
    "                row['price_Lower_Band']\n",
    "            ), axis=1\n",
    "        )\n",
    "\n",
    "        combined_data_df['boll_lower_overlap_band'] = combined_data_df.apply(\n",
    "            lambda row: calculate_boll_lower_advanced(\n",
    "                row['price_Lower_Band'], \n",
    "                row['sentiment_Upper_Band'], \n",
    "                row['sentiment_Lower_Band'], \n",
    "                row['price_Upper_Band']\n",
    "            ), axis=1\n",
    "        )\n",
    "        \n",
    "        # Crossovers\n",
    "        # Calculate the combined normalized scores for all moving averages\n",
    "        for ma in [7, 21, 50, 100, 200]:\n",
    "            sentiment_ma_col = f'combined_compound_ma_{ma}'\n",
    "            price_ma_col = f'close_ma_{ma}'\n",
    "\n",
    "            combined_data_df[sentiment_ma_col].fillna(method='ffill', inplace=True)\n",
    "            combined_data_df[price_ma_col].fillna(method='ffill', inplace=True)\n",
    "\n",
    "            normalized_sentiment_col = f'normalized_sentiment_{ma}'\n",
    "            normalized_price_col = f'normalized_price_{ma}'\n",
    "\n",
    "            combined_data_df[normalized_sentiment_col] = normalize_column(combined_data_df, sentiment_ma_col)\n",
    "            combined_data_df[normalized_price_col] = normalize_column(combined_data_df, price_ma_col)\n",
    "\n",
    "            # Calculate combined normalized score using weights\n",
    "            combined_data_df[f'combined_normalized_score_{ma}'] = (\n",
    "                combined_data_df[normalized_sentiment_col] * 0.8 +\n",
    "                combined_data_df[normalized_price_col] * 0.2\n",
    "            )\n",
    "        # Iterate over each moving average period and calculate crossovers dynamically\n",
    "        for ma in [7, 21, 50, 100, 200]:\n",
    "            # Dynamic column names\n",
    "            normalized_sentiment_col = f'normalized_sentiment_{ma}'\n",
    "            normalized_price_col = f'normalized_price_{ma}'\n",
    "            crossover_column = f'crossover_{ma}'\n",
    "            crossover_type_column = f'crossover_type_{ma}'\n",
    "\n",
    "            # Calculate the crossover points for each moving average using normalized columns and store the 'Close' value\n",
    "            combined_data_df[crossover_column] = np.where(\n",
    "                (combined_data_df[normalized_sentiment_col] > combined_data_df[normalized_price_col]) & \n",
    "                (combined_data_df[normalized_sentiment_col].shift(1) <= combined_data_df[normalized_price_col].shift(1)) |\n",
    "                (combined_data_df[normalized_sentiment_col] < combined_data_df[normalized_price_col]) & \n",
    "                (combined_data_df[normalized_sentiment_col].shift(1) >= combined_data_df[normalized_price_col].shift(1)), \n",
    "                combined_data_df['close'], \n",
    "                np.nan\n",
    "            )\n",
    "\n",
    "            # Define the crossover type (up or down)\n",
    "            combined_data_df[crossover_type_column] = np.where(\n",
    "                (combined_data_df[normalized_sentiment_col] > combined_data_df[normalized_price_col]) & \n",
    "                (combined_data_df[normalized_sentiment_col].shift(1) <= combined_data_df[normalized_price_col].shift(1)),\n",
    "                'cross_up',\n",
    "                np.where(\n",
    "                    (combined_data_df[normalized_sentiment_col] < combined_data_df[normalized_price_col]) & \n",
    "                    (combined_data_df[normalized_sentiment_col].shift(1) >= combined_data_df[normalized_price_col].shift(1)),\n",
    "                    'cross_down',\n",
    "                    np.nan\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        combined_data_df['prev_close_up_down'] = combined_data_df['close'].diff().apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "        # Step 8: Trim the DataFrame to fit the original date window (start_date to end_date)\n",
    "        combined_data_df = combined_data_df.loc[start_date:end_date]\n",
    "\n",
    "        # Step 11: Remove duplicate columns if they exist\n",
    "        combined_data_df = combined_data_df.loc[:, ~combined_data_df.columns.duplicated()]\n",
    "\n",
    "        # Step 12: Store the final DataFrame in the dictionary\n",
    "        moving_averages_dict[grp_term] = combined_data_df.copy()\n",
    "\n",
    "# Step 13: Concatenate all DataFrames from the dictionary after processing each term\n",
    "final_combined_data_df = pd.concat(moving_averages_dict.values(), axis=0)\n",
    "\n",
    "# Optionally, reset index if you need to work with the 'date' column directly\n",
    "final_combined_data_df.reset_index(inplace=True)\n",
    "\n",
    "final_combined_data_df['day_of_week'] = final_combined_data_df['date'].dt.dayofweek\n",
    "final_combined_data_df['month'] = final_combined_data_df['date'].dt.month\n",
    "final_combined_data_df['day_of_month'] = final_combined_data_df['date'].dt.day\n",
    "final_combined_data_df['is_weekend'] = final_combined_data_df['date'].dt.dayofweek >= 5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Display the final DataFrame or inspect it as needed\n",
    "print(\"Final Combined DataFrame:\")\n",
    "display(final_combined_data_df.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9176f931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\2729771975.py:60: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_blend_data = pd.read_sql_query(query, conn)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\2729771975.py:89: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_signal_data = pd.read_sql_query(query_signals, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fetched and aggregated data successfully!\n",
      "✅ Successfully merged aggregated blend data!\n",
      "✅ Fetched signal count data successfully!\n",
      "✅ Successfully merged signal count data with final_combined_data_df!\n"
     ]
    }
   ],
   "source": [
    "# ✅ PostgreSQL connection parameters\n",
    "db_params = {\n",
    "    'dbname': 'twt_snt',\n",
    "    'user': 'postgres',\n",
    "    'password': 'Ilpmnl!69gg',\n",
    "    'host': 'localhost',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "#_______snt_ma_blend_detail_tbl___________________\n",
    "\n",
    "# ✅ Enhanced SQL query with multiple aggregations\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    term,\n",
    "    AVG(twitter_weight) AS avg_twitter_weight,\n",
    "    MIN(twitter_weight) AS min_twitter_weight,\n",
    "    MAX(twitter_weight) AS max_twitter_weight,\n",
    "    STDDEV(twitter_weight) AS std_twitter_weight,\n",
    "\n",
    "    AVG(reddit_weight) AS avg_reddit_weight,\n",
    "    MIN(reddit_weight) AS min_reddit_weight,\n",
    "    MAX(reddit_weight) AS max_reddit_weight,\n",
    "    STDDEV(reddit_weight) AS std_reddit_weight,\n",
    "\n",
    "    AVG(news_weight) AS avg_news_weight,\n",
    "    MIN(news_weight) AS min_news_weight,\n",
    "    MAX(news_weight) AS max_news_weight,\n",
    "    STDDEV(news_weight) AS std_news_weight,\n",
    "\n",
    "    SUM(correlation * (twitter_weight + reddit_weight + news_weight)) / NULLIF(SUM(twitter_weight + reddit_weight + news_weight), 0) AS weighted_correlation,\n",
    "    MIN(correlation) AS min_correlation,\n",
    "    MAX(correlation) AS max_correlation,\n",
    "    STDDEV(correlation) AS std_correlation,\n",
    "\n",
    "    AVG(p_value) AS avg_p_value,\n",
    "    MIN(p_value) AS min_p_value,\n",
    "    MAX(p_value) AS max_p_value,\n",
    "    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY p_value) AS median_p_value,\n",
    "\n",
    "    AVG(confidence_interval_lower) AS avg_ci_lower,\n",
    "    MIN(confidence_interval_lower) AS min_ci_lower,\n",
    "    MAX(confidence_interval_lower) AS max_ci_lower,\n",
    "    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY confidence_interval_lower) AS median_ci_lower,\n",
    "\n",
    "    AVG(confidence_interval_upper) AS avg_ci_upper,\n",
    "    MIN(confidence_interval_upper) AS min_ci_upper,\n",
    "    MAX(confidence_interval_upper) AS max_ci_upper,\n",
    "    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY confidence_interval_upper) AS median_ci_upper,\n",
    "\n",
    "    MAX(run_date) AS latest_run_date\n",
    "FROM \n",
    "    snt_ma_blend_detail_tbl\n",
    "GROUP BY \n",
    "    term\n",
    "\"\"\"\n",
    "\n",
    "# ✅ Connect to the database and fetch data\n",
    "conn = psycopg2.connect(**db_params)\n",
    "df_blend_data = pd.read_sql_query(query, conn)\n",
    "conn.close()\n",
    "\n",
    "print(\"✅ Fetched and aggregated data successfully!\")\n",
    "\n",
    "\n",
    "\n",
    "# ✅ Merge aggregated blend data into final_combined_data_df\n",
    "final_combined_data_df = pd.merge(final_combined_data_df, df_blend_data, on=\"term\", how=\"left\")\n",
    "\n",
    "print(\"✅ Successfully merged aggregated blend data!\")\n",
    "\n",
    "\n",
    "#_______signal_cnt_tbl___________________\n",
    "\n",
    "# ✅ SQL query to fetch signal count data\n",
    "query_signals = \"\"\"\n",
    "SELECT \n",
    "    term,\n",
    "    date,\n",
    "    buy,\n",
    "    sell,\n",
    "    neutral\n",
    "FROM \n",
    "    signal_cnt_tbl\n",
    "\"\"\"\n",
    "\n",
    "# ✅ Connect to the database and fetch signal data\n",
    "conn = psycopg2.connect(**db_params)\n",
    "df_signal_data = pd.read_sql_query(query_signals, conn)\n",
    "conn.close()\n",
    "\n",
    "print(\"✅ Fetched signal count data successfully!\")\n",
    "\n",
    "# ✅ Convert 'date' in final_combined_data_df to datetime\n",
    "final_combined_data_df[\"date\"] = pd.to_datetime(final_combined_data_df[\"date\"])\n",
    "\n",
    "# ✅ Convert 'date' in df_signal_data to datetime\n",
    "df_signal_data[\"date\"] = pd.to_datetime(df_signal_data[\"date\"])\n",
    "\n",
    "# ✅ Merge signal count data into final_combined_data_df on 'date' and 'term'\n",
    "final_combined_data_df = pd.merge(\n",
    "    final_combined_data_df, \n",
    "    df_signal_data, \n",
    "    on=[\"date\", \"term\"], \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"✅ Successfully merged signal count data with final_combined_data_df!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da9abaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully connected to the database using SQLAlchemy!\n",
      "✅ Fetched detail and summary correlation data successfully!\n",
      "✅ Successfully merged correlation detail and summary data!\n",
      "✅ Meta-features created successfully!\n",
      "✅ Successfully melted final_combined_data_df to prepare for merging!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3811878358.py:111: FutureWarning: This dataframe has a column name that matches the 'value_name' column name of the resulting Dataframe. In the future this will raise an error, please set the 'value_name' parameter of DataFrame.melt to a unique name.\n",
      "  final_combined_data_df_melted = pd.melt(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully merged meta-features with final_combined_data_df!\n",
      "✅ Successfully pivoted the data back to original format!\n",
      "both          44700\n",
      "left_only     23550\n",
      "right_only        0\n",
      "Name: _merge, dtype: int64\n",
      "✅ Final Combined DataFrame is ready with enriched meta-features!\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# ✅ Required Imports\n",
    "# ==============================\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "\n",
    "# ==============================\n",
    "# ✅ Database Connection Setup\n",
    "# ==============================\n",
    "# Use SQLAlchemy for cleaner connection management\n",
    "DB_URI = \"postgresql+psycopg2://postgres:Ilpmnl!69gg@localhost:5432/twt_snt\"\n",
    "engine = create_engine(DB_URI)\n",
    "\n",
    "print(\"✅ Successfully connected to the database using SQLAlchemy!\")\n",
    "\n",
    "# ==============================\n",
    "# ✅ Fetch Data from Both Tables\n",
    "# ==============================\n",
    "\n",
    "# ✅ SQL query to fetch detail correlation data\n",
    "query_detail = \"\"\"\n",
    "SELECT \n",
    "    date,\n",
    "    term,\n",
    "    ma,\n",
    "    lag,\n",
    "    correlation,\n",
    "    window_length,\n",
    "    run_date\n",
    "FROM \n",
    "    snt_ma_correlation_detail_tbl\n",
    "\"\"\"\n",
    "\n",
    "# ✅ SQL query to fetch summary correlation data\n",
    "query_summary = \"\"\"\n",
    "SELECT \n",
    "    date,\n",
    "    term,\n",
    "    ma,\n",
    "    best_lag,\n",
    "    max_correlation,\n",
    "    leading_indicator_score,\n",
    "    window_length,\n",
    "    run_date\n",
    "FROM \n",
    "    snt_ma_correlation_summary_tbl\n",
    "\"\"\"\n",
    "\n",
    "# ✅ Fetch data using SQLAlchemy\n",
    "df_detail_data = pd.read_sql_query(query_detail, engine)\n",
    "df_summary_data = pd.read_sql_query(query_summary, engine)\n",
    "\n",
    "print(\"✅ Fetched detail and summary correlation data successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# ✅ Data Preprocessing\n",
    "# ==============================\n",
    "\n",
    "# ✅ Convert 'date' columns to datetime\n",
    "df_detail_data[\"date\"] = pd.to_datetime(df_detail_data[\"date\"])\n",
    "df_summary_data[\"date\"] = pd.to_datetime(df_summary_data[\"date\"])\n",
    "\n",
    "# ✅ Check for NaN and drop missing 'ma' if necessary\n",
    "df_detail_data.dropna(subset=[\"ma\"], inplace=True)\n",
    "df_summary_data.dropna(subset=[\"ma\"], inplace=True)\n",
    "\n",
    "# ==============================\n",
    "# ✅ Merge Detail and Summary Data\n",
    "# ==============================\n",
    "\n",
    "# ✅ Merge detail and summary data on 'date', 'term', and 'ma'\n",
    "merged_df = pd.merge(\n",
    "    df_detail_data,\n",
    "    df_summary_data,\n",
    "    on=[\"date\", \"term\", \"ma\"],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"_detail\", \"_summary\")\n",
    ")\n",
    "\n",
    "print(\"✅ Successfully merged correlation detail and summary data!\")\n",
    "\n",
    "# ==============================\n",
    "# ✅ Create Meta-Features\n",
    "# ==============================\n",
    "\n",
    "# ✅ Lag Difference: Compare the best lag vs. lags in the detail data.\n",
    "merged_df[\"lag_diff\"] = merged_df[\"lag\"] - merged_df[\"best_lag\"]\n",
    "\n",
    "# ✅ Correlation Ratio: Ratio between current lag correlation and best correlation.   \n",
    "merged_df[\"correlation_ratio\"] = merged_df[\"correlation\"] / (merged_df[\"max_correlation\"] + 1e-6)\n",
    "\n",
    "# ✅ Correlation Difference: Difference between the current lag correlation and best correlation.    \n",
    "merged_df[\"correlation_diff\"] = merged_df[\"correlation\"] - merged_df[\"max_correlation\"]\n",
    "\n",
    "# ✅ Relative Correlation to Window Length: How correlation changes as window length increases.\n",
    "merged_df[\"correlation_per_window\"] = merged_df[\"correlation\"] / merged_df[\"window_length_detail\"]\n",
    "\n",
    "# ✅ Lag Abs Diff as Indicator: Magnitude of lag difference as a proxy for how far sentiment leads or lags.\n",
    "merged_df[\"abs_lag_diff\"] = merged_df[\"lag_diff\"].abs()\n",
    "\n",
    "print(\"✅ Meta-features created successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# ✅ Prepare Final Combined Data for Merge\n",
    "# ==============================\n",
    "\n",
    "# ✅ Melt/Unpivot final_combined_data_df to create 'ma' column\n",
    "final_combined_data_df_melted = pd.melt(\n",
    "    final_combined_data_df,\n",
    "    id_vars=[\"date\", \"term\"],\n",
    "    value_vars=[\n",
    "        \"combined_compound_ma_7\",\n",
    "        \"combined_compound_ma_21\",\n",
    "        \"combined_compound_ma_50\",\n",
    "        \"combined_compound_ma_100\",\n",
    "        \"combined_compound_ma_200\"\n",
    "    ],\n",
    "    var_name=\"ma\",\n",
    "    value_name=\"combined_compound\"\n",
    ")\n",
    "\n",
    "# ✅ Extract only the MA number from 'ma' column\n",
    "final_combined_data_df_melted[\"ma\"] = final_combined_data_df_melted[\"ma\"].str.replace(\n",
    "    \"combined_compound_ma_\", \"\").astype(int)\n",
    "\n",
    "print(\"✅ Successfully melted final_combined_data_df to prepare for merging!\")\n",
    "\n",
    "# ==============================\n",
    "# ✅ Merge Meta-Features into Final Combined Data\n",
    "# ==============================\n",
    "\n",
    "# ✅ Merge melted data with meta-features\n",
    "final_combined_data_df = pd.merge(\n",
    "    final_combined_data_df_melted,\n",
    "    merged_df,\n",
    "    on=[\"date\", \"term\", \"ma\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"✅ Successfully merged meta-features with final_combined_data_df!\")\n",
    "\n",
    "# ==============================\n",
    "# ✅ Pivot Back to Original Format (Optional if Needed)\n",
    "# ==============================\n",
    "\n",
    "# ✅ Pivot the data back if you want the original format after merging\n",
    "final_combined_data_df_pivoted = final_combined_data_df.pivot_table(\n",
    "    index=[\"date\", \"term\"],\n",
    "    columns=\"ma\",\n",
    "    values=[\n",
    "        \"combined_compound\",\n",
    "        \"lag_diff\",\n",
    "        \"correlation_ratio\",\n",
    "        \"correlation_diff\",\n",
    "        \"correlation_per_window\",\n",
    "        \"abs_lag_diff\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ✅ Flatten MultiIndex columns\n",
    "final_combined_data_df_pivoted.columns = [f\"{col[0]}_ma_{int(col[1])}\" for col in final_combined_data_df_pivoted.columns]\n",
    "final_combined_data_df_pivoted.reset_index(inplace=True)\n",
    "\n",
    "print(\"✅ Successfully pivoted the data back to original format!\")\n",
    "\n",
    "# ==============================\n",
    "# 🔍 Optional: Debug Merge Status\n",
    "# ==============================\n",
    "\n",
    "# ✅ Check for merge issues if necessary\n",
    "debug_df = pd.merge(\n",
    "    final_combined_data_df_melted,\n",
    "    merged_df,\n",
    "    on=[\"date\", \"term\", \"ma\"],\n",
    "    how=\"left\",\n",
    "    indicator=True\n",
    ")\n",
    "print(debug_df[\"_merge\"].value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 🎉 Final Outcome: Enriched DataFrame Ready for Model Training\n",
    "# ==============================\n",
    "print(\"✅ Final Combined DataFrame is ready with enriched meta-features!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d07c1414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from xgboost import XGBClassifier \n",
    "from lightgbm import LGBMClassifier \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier, GradientBoostingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b315be66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date term  ma  combined_compound  lag  correlation  \\\n",
      "0 2024-11-19  SOL   7           0.191733  NaN          NaN   \n",
      "1 2024-11-20  SOL   7           0.226319  NaN          NaN   \n",
      "2 2024-11-21  SOL   7           0.240264  NaN          NaN   \n",
      "3 2024-11-22  SOL   7           0.243445  NaN          NaN   \n",
      "4 2024-11-23  SOL   7           0.267342  NaN          NaN   \n",
      "\n",
      "   window_length_detail run_date_detail  best_lag  max_correlation  \\\n",
      "0                   NaN             NaT       NaN              NaN   \n",
      "1                   NaN             NaT       NaN              NaN   \n",
      "2                   NaN             NaT       NaN              NaN   \n",
      "3                   NaN             NaT       NaN              NaN   \n",
      "4                   NaN             NaT       NaN              NaN   \n",
      "\n",
      "   leading_indicator_score  window_length_summary run_date_summary  lag_diff  \\\n",
      "0                      NaN                    NaN              NaT       NaN   \n",
      "1                      NaN                    NaN              NaT       NaN   \n",
      "2                      NaN                    NaN              NaT       NaN   \n",
      "3                      NaN                    NaN              NaT       NaN   \n",
      "4                      NaN                    NaN              NaT       NaN   \n",
      "\n",
      "   correlation_ratio  correlation_diff  correlation_per_window  abs_lag_diff  \n",
      "0                NaN               NaN                     NaN           NaN  \n",
      "1                NaN               NaN                     NaN           NaN  \n",
      "2                NaN               NaN                     NaN           NaN  \n",
      "3                NaN               NaN                     NaN           NaN  \n",
      "4                NaN               NaN                     NaN           NaN  \n"
     ]
    }
   ],
   "source": [
    "print(final_combined_data_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afc05ea",
   "metadata": {},
   "source": [
    "3. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5960149",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: SOL\n",
      "Processing: KAS\n",
      "Processing: LINK\n",
      "Processing: ADA\n",
      "Processing: MATIC\n",
      "Processing: AVAX\n",
      "Processing: POPCAT\n",
      "Processing: SUI\n",
      "Processing: HNT\n",
      "Processing: WIF\n",
      "Processing: BTC\n",
      "Processing: DOGE\n",
      "Processing: ETH\n",
      "Processing: GME\n",
      "Processing: NVDA\n",
      "Processing: JPM\n",
      "Processing: GOOGL\n",
      "Processing: DXY\n",
      "Processing: TSMC\n",
      "Processing: CVX\n",
      "Processing: COIN\n",
      "Processing: AMZN\n",
      "Processing: MSFT\n",
      "Processing: NFLX\n",
      "Processing: DIS\n",
      "Processing: AAPL\n",
      "Processing: TSLA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3650519994.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.loc[:, col] = pd.to_numeric(X[col], errors='coerce')\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3650519994.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[numeric_columns] = X[numeric_columns].replace([np.inf, -np.inf], np.nan)\n",
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3650519994.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[numeric_columns] = X[numeric_columns].clip(lower=-1e10, upper=1e10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training First-Level Ensembles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Local\\Temp\\ipykernel_6752\\3650519994.py:64: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.loc[:, col] = X[col].astype(str).fillna(\"UNKNOWN\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c26455639ab419fa9295dd1f24c8bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 2663\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020528 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 30208\n",
      "[LightGBM] [Info] Number of data points in the train set: 4223, number of used features: 207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.369406 -> initscore=-0.534767\n",
      "[LightGBM] [Info] Start training from score -0.534767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py:158: UserWarning: [20:03:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Meta-Ensemble...\n",
      "[LightGBM] [Info] Number of positive: 1560, number of negative: 2663\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012688 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 30208\n",
      "[LightGBM] [Info] Number of data points in the train set: 4223, number of used features: 207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.369406 -> initscore=-0.534767\n",
      "[LightGBM] [Info] Start training from score -0.534767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py:158: UserWarning: [20:09:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Ensemble Accuracy: 0.71\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.83      0.78       663\n",
      "         1.0       0.64      0.49      0.55       393\n",
      "\n",
      "    accuracy                           0.71      1056\n",
      "   macro avg       0.68      0.66      0.67      1056\n",
      "weighted avg       0.70      0.71      0.70      1056\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[553 110]\n",
      " [201 192]]\n",
      "\n",
      "Predictions for next day per term:\n",
      "Prediction for 2025-06-08 for term 'SOL' `next_close`: DOWN (0)\n",
      "Prediction for 2025-06-08 for term 'KAS' `next_close`: DOWN (0)\n",
      "Prediction for 2025-06-08 for term 'LINK' `next_close`: DOWN (0)\n",
      "Prediction for 2025-06-08 for term 'ADA' `next_close`: DOWN (0)\n",
      "Prediction for 2025-06-08 for term 'MATIC' `next_close`: DOWN (0)\n",
      "Prediction for 2025-06-08 for term 'AVAX' `next_close`: DOWN (0)\n",
      "Prediction for 2025-06-08 for term 'POPCAT' `next_close`: DOWN (0)\n",
      "Prediction for 2025-06-08 for term 'SUI' `next_close`: DOWN (0)\n",
      "Prediction for 2025-06-07 for term 'HNT' `next_close`: UP (1)\n",
      "Prediction for 2025-06-08 for term 'WIF' `next_close`: UP (1)\n",
      "Prediction for 2025-06-08 for term 'BTC' `next_close`: DOWN (0)\n",
      "Prediction for 2025-06-08 for term 'DOGE' `next_close`: DOWN (0)\n",
      "Prediction for 2025-06-08 for term 'ETH' `next_close`: DOWN (0)\n",
      "Prediction for 2025-06-07 for term 'GME' `next_close`: DOWN (0)\n",
      "Prediction for 2025-06-07 for term 'NVDA' `next_close`: DOWN (0)\n",
      "Prediction for 2025-06-07 for term 'JPM' `next_close`: DOWN (0)\n",
      "Prediction for 2025-06-07 for term 'GOOGL' `next_close`: DOWN (0)\n",
      "Prediction for 2025-04-13 for term 'DXY' `next_close`: DOWN (0)\n",
      "Prediction for 2025-06-07 for term 'TSMC' `next_close`: DOWN (0)\n",
      "Prediction for 2025-06-07 for term 'CVX' `next_close`: DOWN (0)\n",
      "Prediction for 2025-06-07 for term 'COIN' `next_close`: DOWN (0)\n",
      "Prediction for 2025-06-07 for term 'AMZN' `next_close`: DOWN (0)\n",
      "Prediction for 2025-06-07 for term 'MSFT' `next_close`: DOWN (0)\n",
      "Prediction for 2025-06-06 for term 'NFLX' `next_close`: DOWN (0)\n",
      "Prediction for 2025-06-07 for term 'DIS' `next_close`: DOWN (0)\n",
      "Prediction for 2025-06-07 for term 'AAPL' `next_close`: DOWN (0)\n",
      "Prediction for 2025-06-07 for term 'TSLA' `next_close`: DOWN (0)\n",
      "✅ Predictions updated with overwrite protection and saved to log file.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ====================== PARAMETERS ======================\n",
    "\n",
    "# Date range\n",
    "end_date = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "start_date = (datetime.datetime.today() - timedelta(days=200)).strftime('%Y-%m-%d')\n",
    "extended_start_date = (pd.to_datetime(start_date) - pd.Timedelta(days=21)).strftime('%Y-%m-%d')\n",
    "\n",
    "# Terms\n",
    "grp_terms = ['SOL', 'KAS', 'LINK', 'ADA', 'MATIC', 'AVAX', 'POPCAT', 'SUI', 'HNT', 'WIF', 'BTC', 'DOGE', 'ETH', \n",
    "             'GME', 'NVDA', 'JPM', 'GOOGL', 'DXY', 'TSMC', 'CVX', 'COIN', 'AMZN', 'MSFT', 'NFLX', 'DIS', 'AAPL', 'TSLA']\n",
    "\n",
    "# ============== PREPROCESS MOVING AVERAGE DICTIONARY ==============\n",
    "\n",
    "if 'moving_averages_dict' not in globals() or not moving_averages_dict:\n",
    "    raise ValueError(\"`moving_averages_dict` is not defined or empty.\")\n",
    "\n",
    "cleaned_dict = {}\n",
    "for grp_term, df in moving_averages_dict.items():\n",
    "    print(f\"Processing: {grp_term}\")\n",
    "\n",
    "    df = df.reset_index()  # Make 'date' a column\n",
    "    if 'prev_close_up_down' not in df.columns or 'date' not in df.columns:\n",
    "        print(f\"Skipping {grp_term} due to missing columns.\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    df = df.copy()\n",
    "    df['next_close'] = df['prev_close_up_down'].shift(-1)\n",
    "    df = df[:-1]\n",
    "    df['term'] = grp_term\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day_of_month'] = df['date'].dt.day\n",
    "    df['is_weekend'] = df['day_of_week'] >= 5\n",
    "    cleaned_dict[grp_term] = df\n",
    "\n",
    "# Combine all terms\n",
    "final_combined_data_df = pd.concat(cleaned_dict.values(), ignore_index=True)\n",
    "\n",
    "# Drop rows with NaN targets\n",
    "final_combined_data_df.dropna(subset=['next_close'], inplace=True)\n",
    "\n",
    "# ======================== FEATURES ========================\n",
    "\n",
    "target = 'next_close'\n",
    "exclude_cols = ['date', 'prev_close_up_down', target, 'term']\n",
    "feature_columns = [col for col in final_combined_data_df.columns if col not in exclude_cols]\n",
    "\n",
    "X = final_combined_data_df[feature_columns]\n",
    "y = final_combined_data_df[target]\n",
    "\n",
    "# Separate numeric & categorical\n",
    "numeric_columns = X.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_columns = X.select_dtypes(exclude=['number']).columns.tolist()\n",
    "\n",
    "# Clean numeric & categorical\n",
    "for col in numeric_columns:\n",
    "    X.loc[:, col] = pd.to_numeric(X[col], errors='coerce')\n",
    "X[numeric_columns] = X[numeric_columns].replace([np.inf, -np.inf], np.nan)\n",
    "X[numeric_columns] = X[numeric_columns].clip(lower=-1e10, upper=1e10)\n",
    "\n",
    "for col in categorical_columns:\n",
    "    X.loc[:, col] = X[col].astype(str).fillna(\"UNKNOWN\")\n",
    "\n",
    "# ===================== PREPROCESSOR =====================\n",
    "\n",
    "transformers = [('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='mean')),\n",
    "                                        ('scaler', StandardScaler())]), numeric_columns)]\n",
    "\n",
    "if categorical_columns:\n",
    "    transformers.append(('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns))\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers)\n",
    "\n",
    "# ===================== CLASSIFIERS =====================\n",
    "\n",
    "gb_classifier = GradientBoostingClassifier(n_estimators=150, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "\n",
    "catboost_classifier = CatBoostClassifier(iterations=140, learning_rate=0.15, depth=4, random_state=42, verbose=0)\n",
    "\n",
    "xgb_classifier = XGBClassifier(\n",
    "    n_estimators=150,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=7,\n",
    "    subsample=1.0,\n",
    "    colsample_bytree=0.7,\n",
    "    gamma=5,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=5,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "pipeline_gb = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', gb_classifier)])\n",
    "pipeline_catboost = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', catboost_classifier)])\n",
    "pipeline_xgb = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', xgb_classifier)])\n",
    "\n",
    "\n",
    "# Logistic Regression with best hyperparameters\n",
    "lr_classifier = LogisticRegression(\n",
    "    C=5.623413251903491,\n",
    "    class_weight=None,\n",
    "    max_iter=130,\n",
    "    penalty='l1',\n",
    "    solver='liblinear',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "pipeline_lr = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', lr_classifier)\n",
    "])\n",
    "\n",
    "# Random Forest Pipeline\n",
    "rf_classifier = RandomForestClassifier(n_estimators=30, max_depth=34, min_samples_leaf=2, min_samples_split=25, random_state=42)\n",
    "pipeline_rf = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', rf_classifier)])\n",
    "\n",
    "# Gradient Boosting (Optional with Tweaked Parameters)\n",
    "gb_classifier_v2 = GradientBoostingClassifier(n_estimators=200, learning_rate=0.05, max_depth=4, random_state=42)\n",
    "pipeline_gb_v2 = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', gb_classifier_v2)])\n",
    "\n",
    "pipeline_svm = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(\n",
    "        C=1.0,\n",
    "        kernel='rbf',\n",
    "        gamma='auto',\n",
    "        probability=True,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipeline_lgbm = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LGBMClassifier(\n",
    "        colsample_bytree=0.6,\n",
    "        learning_rate=0.09,\n",
    "        max_depth=-1,\n",
    "        n_estimators=220,\n",
    "        num_leaves=24,\n",
    "        reg_alpha=0.2,\n",
    "        reg_lambda=0.1,\n",
    "        subsample=0.60,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipeline_et = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ExtraTreesClassifier(\n",
    "        n_estimators=120,\n",
    "        max_depth=None,\n",
    "        max_features=0.5,\n",
    "        min_samples_split=4,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# ===================== TRAIN-TEST SPLIT =====================\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# First ensemble\n",
    "ensemble_1 = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('randomforest', pipeline_rf),\n",
    "        ('svm', pipeline_svm),\n",
    "        ('lightgbm', pipeline_lgbm),\n",
    "        ('extratrees', pipeline_et)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Second ensemble\n",
    "ensemble_2 = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('gradient_boosting', pipeline_gb),\n",
    "        ('logistic_regression', pipeline_lr),\n",
    "        ('xgboost', pipeline_xgb)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "print(\"Training First-Level Ensembles...\")\n",
    "with tqdm(total=2) as pbar:\n",
    "    ensemble_1.fit(X_train, y_train)\n",
    "    pbar.update(1)\n",
    "    ensemble_2.fit(X_train, y_train)\n",
    "    pbar.update(1)\n",
    "    \n",
    "\n",
    "\n",
    "# Meta-Ensemble\n",
    "meta_ensemble = VotingClassifier(estimators=[('ensemble_1', ensemble_1), ('ensemble_2', ensemble_2)], voting='soft')\n",
    "print(\"Training Meta-Ensemble...\")\n",
    "meta_ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "y_pred_meta = meta_ensemble.predict(X_test)\n",
    "print(f'Meta-Ensemble Accuracy: {accuracy_score(y_test, y_pred_meta):.2f}')\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_meta))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_meta))\n",
    "\n",
    "# ===================== NEXT DAY PREDICTIONS =====================\n",
    "\n",
    "print(\"\\nPredictions for next day per term:\")\n",
    "\n",
    "prediction_csv = \"predictions_log.csv\"\n",
    "prediction_log_df = pd.DataFrame(columns=['date', 'term', 'prediction', 'actual'])\n",
    "\n",
    "terms = final_combined_data_df['term'].unique()\n",
    "\n",
    "for term in terms:\n",
    "    term_data = final_combined_data_df[final_combined_data_df['term'] == term]\n",
    "    if term_data.empty:\n",
    "        print(f\"No data found for term '{term}', skipping.\")\n",
    "        continue\n",
    "\n",
    "    latest_data = term_data.iloc[-1].copy()\n",
    "    tomorrow_date = latest_data['date'] + datetime.timedelta(days=1)\n",
    "    tomorrow_str = tomorrow_date.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Update date-based features\n",
    "    latest_data['date'] = tomorrow_date\n",
    "    latest_data['day_of_week'] = tomorrow_date.dayofweek\n",
    "    latest_data['month'] = tomorrow_date.month\n",
    "    latest_data['day_of_month'] = tomorrow_date.day\n",
    "    latest_data['is_weekend'] = tomorrow_date.dayofweek >= 5\n",
    "\n",
    "    # Prepare feature DataFrame\n",
    "    tomorrow_df = pd.DataFrame([latest_data[feature_columns]])\n",
    "\n",
    "    # Force numeric columns back to numeric safely\n",
    "    for col in numeric_columns:\n",
    "        tomorrow_df[col] = pd.to_numeric(tomorrow_df[col], errors='coerce')\n",
    "    tomorrow_df[numeric_columns] = tomorrow_df[numeric_columns].replace([np.inf, -np.inf], np.nan)\n",
    "    tomorrow_df[numeric_columns] = tomorrow_df[numeric_columns].fillna(tomorrow_df[numeric_columns].mean())\n",
    "\n",
    "    # Fill categoricals\n",
    "    for col in categorical_columns:\n",
    "        tomorrow_df[col] = tomorrow_df[col].fillna(\"UNKNOWN\").astype(str)\n",
    "\n",
    "    # Predict & log\n",
    "    try:\n",
    "        probs = meta_ensemble.predict_proba(tomorrow_df)[0]\n",
    "        prediction_value = int(probs[1] > 0.5)  # still binarizing to 0/1\n",
    "        confidence = round(probs[1], 4)  # confidence for class 1 (UP)\n",
    "\n",
    "        prediction_record = {\n",
    "            'date': tomorrow_str,\n",
    "            'term': term,\n",
    "            'prediction': prediction_value,\n",
    "            'confidence': confidence,\n",
    "            'actual': latest_data['prev_close_up_down']\n",
    "        }\n",
    "        prediction_log_df = pd.concat([prediction_log_df, pd.DataFrame([prediction_record])], ignore_index=True)\n",
    "\n",
    "        direction = \"UP (1)\" if prediction_value == 1 else \"DOWN (0)\"\n",
    "        print(f\"Prediction for {tomorrow_str} for term '{term}' `next_close`: {direction}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting for term '{term}' on {tomorrow_str}: {e}\")\n",
    "        \n",
    "# Append previous logs if file exists\n",
    "# Load existing log if available\n",
    "if os.path.exists(prediction_csv):\n",
    "    existing_predictions = pd.read_csv(prediction_csv)\n",
    "    \n",
    "    # Ensure consistent data types\n",
    "    existing_predictions['date'] = pd.to_datetime(existing_predictions['date'])\n",
    "    prediction_log_df['date'] = pd.to_datetime(prediction_log_df['date'])\n",
    "    \n",
    "    # Merge logic: keep historical data but update today's predictions\n",
    "    today = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Remove any existing predictions for today's date\n",
    "    existing_predictions = existing_predictions[\n",
    "        existing_predictions['date'].dt.strftime('%Y-%m-%d') != today\n",
    "    ]\n",
    "    \n",
    "    # Combine with new predictions\n",
    "    prediction_log_df = pd.concat([existing_predictions, prediction_log_df], ignore_index=True)\n",
    "    \n",
    "    # Remove duplicates (keeping last entry for each date+term)\n",
    "    prediction_log_df = prediction_log_df.drop_duplicates(\n",
    "        subset=['date', 'term'], \n",
    "        keep='last'\n",
    "    )\n",
    "\n",
    "# Final sort and write\n",
    "prediction_log_df.sort_values(by=['date', 'term'], inplace=True)\n",
    "prediction_log_df.to_csv(prediction_csv, index=False)\n",
    "print(\"✅ Predictions updated with overwrite protection and saved to log file.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd44678",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d99139dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions completed and appended to the file.\n"
     ]
    }
   ],
   "source": [
    "prediction_log_df.to_csv(prediction_csv, index=False)\n",
    "print(\"Predictions completed and appended to the file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d86770ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>term</th>\n",
       "      <th>combined_compound_ma_7</th>\n",
       "      <th>combined_compound_ma_21</th>\n",
       "      <th>combined_compound_ma_50</th>\n",
       "      <th>combined_compound_ma_100</th>\n",
       "      <th>combined_compound_ma_200</th>\n",
       "      <th>combined_compound</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>...</th>\n",
       "      <th>crossover_100</th>\n",
       "      <th>crossover_type_100</th>\n",
       "      <th>crossover_200</th>\n",
       "      <th>crossover_type_200</th>\n",
       "      <th>prev_close_up_down</th>\n",
       "      <th>next_close</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>is_weekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-11-19</td>\n",
       "      <td>SOL</td>\n",
       "      <td>0.191733</td>\n",
       "      <td>0.227897</td>\n",
       "      <td>0.238858</td>\n",
       "      <td>0.242456</td>\n",
       "      <td>0.244130</td>\n",
       "      <td>0.184109</td>\n",
       "      <td>239.925942</td>\n",
       "      <td>247.702181</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-11-20</td>\n",
       "      <td>SOL</td>\n",
       "      <td>0.226319</td>\n",
       "      <td>0.237186</td>\n",
       "      <td>0.242435</td>\n",
       "      <td>0.244191</td>\n",
       "      <td>0.244985</td>\n",
       "      <td>0.330077</td>\n",
       "      <td>237.924820</td>\n",
       "      <td>242.665362</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-11-21</td>\n",
       "      <td>SOL</td>\n",
       "      <td>0.240264</td>\n",
       "      <td>0.241269</td>\n",
       "      <td>0.243990</td>\n",
       "      <td>0.244941</td>\n",
       "      <td>0.245354</td>\n",
       "      <td>0.282099</td>\n",
       "      <td>235.659449</td>\n",
       "      <td>259.854713</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-11-22</td>\n",
       "      <td>SOL</td>\n",
       "      <td>0.243445</td>\n",
       "      <td>0.242334</td>\n",
       "      <td>0.244343</td>\n",
       "      <td>0.245101</td>\n",
       "      <td>0.245430</td>\n",
       "      <td>0.252988</td>\n",
       "      <td>256.639387</td>\n",
       "      <td>264.572046</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-11-23</td>\n",
       "      <td>SOL</td>\n",
       "      <td>0.267342</td>\n",
       "      <td>0.251125</td>\n",
       "      <td>0.248056</td>\n",
       "      <td>0.246961</td>\n",
       "      <td>0.246361</td>\n",
       "      <td>0.339033</td>\n",
       "      <td>257.182699</td>\n",
       "      <td>264.206361</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5274</th>\n",
       "      <td>2025-06-02</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.158819</td>\n",
       "      <td>0.127261</td>\n",
       "      <td>0.100760</td>\n",
       "      <td>0.074542</td>\n",
       "      <td>0.052988</td>\n",
       "      <td>0.212105</td>\n",
       "      <td>343.500000</td>\n",
       "      <td>348.020000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5275</th>\n",
       "      <td>2025-06-03</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.161703</td>\n",
       "      <td>0.131179</td>\n",
       "      <td>0.103489</td>\n",
       "      <td>0.076439</td>\n",
       "      <td>0.054156</td>\n",
       "      <td>0.170356</td>\n",
       "      <td>346.595000</td>\n",
       "      <td>355.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5276</th>\n",
       "      <td>2025-06-04</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.143425</td>\n",
       "      <td>0.127307</td>\n",
       "      <td>0.102905</td>\n",
       "      <td>0.076680</td>\n",
       "      <td>0.054499</td>\n",
       "      <td>0.088593</td>\n",
       "      <td>345.095000</td>\n",
       "      <td>345.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5277</th>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.108588</td>\n",
       "      <td>0.116105</td>\n",
       "      <td>0.099030</td>\n",
       "      <td>0.075242</td>\n",
       "      <td>0.053997</td>\n",
       "      <td>0.004078</td>\n",
       "      <td>322.490000</td>\n",
       "      <td>324.549900</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5278</th>\n",
       "      <td>2025-06-06</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.078489</td>\n",
       "      <td>0.104476</td>\n",
       "      <td>0.094683</td>\n",
       "      <td>0.073518</td>\n",
       "      <td>0.053342</td>\n",
       "      <td>-0.011810</td>\n",
       "      <td>298.830000</td>\n",
       "      <td>305.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5279 rows × 190 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  term  combined_compound_ma_7  combined_compound_ma_21  \\\n",
       "0    2024-11-19   SOL                0.191733                 0.227897   \n",
       "1    2024-11-20   SOL                0.226319                 0.237186   \n",
       "2    2024-11-21   SOL                0.240264                 0.241269   \n",
       "3    2024-11-22   SOL                0.243445                 0.242334   \n",
       "4    2024-11-23   SOL                0.267342                 0.251125   \n",
       "...         ...   ...                     ...                      ...   \n",
       "5274 2025-06-02  TSLA                0.158819                 0.127261   \n",
       "5275 2025-06-03  TSLA                0.161703                 0.131179   \n",
       "5276 2025-06-04  TSLA                0.143425                 0.127307   \n",
       "5277 2025-06-05  TSLA                0.108588                 0.116105   \n",
       "5278 2025-06-06  TSLA                0.078489                 0.104476   \n",
       "\n",
       "      combined_compound_ma_50  combined_compound_ma_100  \\\n",
       "0                    0.238858                  0.242456   \n",
       "1                    0.242435                  0.244191   \n",
       "2                    0.243990                  0.244941   \n",
       "3                    0.244343                  0.245101   \n",
       "4                    0.248056                  0.246961   \n",
       "...                       ...                       ...   \n",
       "5274                 0.100760                  0.074542   \n",
       "5275                 0.103489                  0.076439   \n",
       "5276                 0.102905                  0.076680   \n",
       "5277                 0.099030                  0.075242   \n",
       "5278                 0.094683                  0.073518   \n",
       "\n",
       "      combined_compound_ma_200  combined_compound        open        high  \\\n",
       "0                     0.244130           0.184109  239.925942  247.702181   \n",
       "1                     0.244985           0.330077  237.924820  242.665362   \n",
       "2                     0.245354           0.282099  235.659449  259.854713   \n",
       "3                     0.245430           0.252988  256.639387  264.572046   \n",
       "4                     0.246361           0.339033  257.182699  264.206361   \n",
       "...                        ...                ...         ...         ...   \n",
       "5274                  0.052988           0.212105  343.500000  348.020000   \n",
       "5275                  0.054156           0.170356  346.595000  355.400000   \n",
       "5276                  0.054499           0.088593  345.095000  345.600000   \n",
       "5277                  0.053997           0.004078  322.490000  324.549900   \n",
       "5278                  0.053342          -0.011810  298.830000  305.500000   \n",
       "\n",
       "      ...  crossover_100  crossover_type_100 crossover_200  \\\n",
       "0     ...            NaN                 nan           NaN   \n",
       "1     ...            NaN                 nan           NaN   \n",
       "2     ...            NaN                 nan           NaN   \n",
       "3     ...            NaN                 nan           NaN   \n",
       "4     ...            NaN                 nan           NaN   \n",
       "...   ...            ...                 ...           ...   \n",
       "5274  ...            NaN                 nan           NaN   \n",
       "5275  ...            NaN                 nan           NaN   \n",
       "5276  ...            NaN                 nan           NaN   \n",
       "5277  ...            NaN                 nan           NaN   \n",
       "5278  ...            NaN                 nan           NaN   \n",
       "\n",
       "      crossover_type_200  prev_close_up_down  next_close  day_of_week  month  \\\n",
       "0                    nan                   0         0.0            1     11   \n",
       "1                    nan                   0         1.0            2     11   \n",
       "2                    nan                   1         1.0            3     11   \n",
       "3                    nan                   1         0.0            4     11   \n",
       "4                    nan                   0         0.0            5     11   \n",
       "...                  ...                 ...         ...          ...    ...   \n",
       "5274                 nan                   0         1.0            0      6   \n",
       "5275                 nan                   1         0.0            1      6   \n",
       "5276                 nan                   0         0.0            2      6   \n",
       "5277                 nan                   0         1.0            3      6   \n",
       "5278                 nan                   1         0.0            4      6   \n",
       "\n",
       "      day_of_month  is_weekend  \n",
       "0               19       False  \n",
       "1               20       False  \n",
       "2               21       False  \n",
       "3               22       False  \n",
       "4               23        True  \n",
       "...            ...         ...  \n",
       "5274             2       False  \n",
       "5275             3       False  \n",
       "5276             4       False  \n",
       "5277             5       False  \n",
       "5278             6       False  \n",
       "\n",
       "[5279 rows x 190 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_combined_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f83ca0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daca1882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most recent tweet in dataset: 2025-06-07 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(\"Most recent tweet in dataset:\", final_combined_data_df[\"date\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "879f3679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 40 rows at 2025-06-07 20:10:16.779841\n",
      "Inserted 40 rows at 2025-06-07 20:10:16.795815\n",
      "Inserted 40 rows at 2025-06-07 20:10:16.814276\n",
      "Inserted 40 rows at 2025-06-07 20:10:16.833521\n",
      "Inserted 40 rows at 2025-06-07 20:10:16.851701\n",
      "Inserted 40 rows at 2025-06-07 20:10:16.869270\n",
      "Inserted 40 rows at 2025-06-07 20:10:16.887738\n",
      "Inserted 40 rows at 2025-06-07 20:10:16.905885\n",
      "Inserted 40 rows at 2025-06-07 20:10:16.923812\n",
      "Inserted 40 rows at 2025-06-07 20:10:16.940854\n",
      "Inserted 40 rows at 2025-06-07 20:10:16.957155\n",
      "Inserted 40 rows at 2025-06-07 20:10:16.973758\n",
      "Inserted 40 rows at 2025-06-07 20:10:16.990079\n",
      "Inserted 40 rows at 2025-06-07 20:10:17.007587\n",
      "Inserted 40 rows at 2025-06-07 20:10:17.023973\n",
      "Inserted 40 rows at 2025-06-07 20:10:17.041912\n",
      "Inserted 40 rows at 2025-06-07 20:10:17.060622\n",
      "Inserted 40 rows at 2025-06-07 20:10:17.077446\n",
      "Inserted 40 rows at 2025-06-07 20:10:17.093786\n",
      "Inserted 40 rows at 2025-06-07 20:10:17.111009\n",
      "Inserted 40 rows at 2025-06-07 20:10:17.127677\n",
      "Inserted 40 rows at 2025-06-07 20:10:17.143369\n",
      "Inserted 40 rows at 2025-06-07 20:10:17.159900\n",
      "Inserted 40 rows at 2025-06-07 20:10:17.176836\n",
      "Inserted 40 rows at 2025-06-07 20:10:17.192076\n",
      "Inserted 40 rows at 2025-06-07 20:10:17.205412\n",
      "Inserted 40 rows at 2025-06-07 20:10:17.222620\n",
      "Inserted 40 rows at 2025-06-07 20:10:17.240052\n",
      "Inserted 40 rows at 2025-06-07 20:10:17.255999\n",
      "Inserted 40 rows at 2025-06-07 20:10:17.274409\n",
      "Inserted 40 rows at 2025-06-07 20:10:17.290668\n",
      "Inserted 40 rows at 2025-06-07 20:10:17.308226\n",
      "Inserted 40 rows at 2025-06-07 20:10:17.325276\n",
      "Inserted 40 rows at 2025-06-07 20:10:17.341823\n",
      "Inserted 40 rows at 2025-06-07 20:10:17.359980\n",
      "Inserted 40 rows at 2025-06-07 20:10:17.380307\n",
      "Inserted 40 rows at 2025-06-07 20:10:17.397531\n",
      "Inserted 40 rows at 2025-06-07 20:10:17.416559\n",
      "Inserted 40 rows at 2025-06-07 20:10:17.433646\n",
      "Inserted 40 rows at 2025-06-07 20:10:17.452344\n",
      "Inserted 40 rows at 2025-06-07 20:10:17.470723\n",
      "Inserted 40 rows at 2025-06-07 20:10:17.487497\n",
      "Inserted 40 rows at 2025-06-07 20:10:17.505311\n",
      "Inserted 40 rows at 2025-06-07 20:10:17.522543\n",
      "Inserted 40 rows at 2025-06-07 20:10:17.539920\n",
      "Inserted final batch of 30 rows at 2025-06-07 20:10:17.563230\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    'dbname': 'twt_snt',\n",
    "    'user': 'postgres',\n",
    "    'password': 'Ilpmnl!69gg',\n",
    "    'host': 'localhost',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "# Example DataFrame\n",
    "# predictions_df = pd.DataFrame({\n",
    "#     'date': [pd.to_datetime('2024-12-20'), pd.to_datetime('2024-12-21')],\n",
    "#     'term': ['term1', 'term2'],\n",
    "#     'prediction': [1, 0],\n",
    "#     'actual': [1, None]\n",
    "# })\n",
    "\n",
    "def safe_value(value):\n",
    "    # Handle None, NaN, and Timestamp conversion\n",
    "    if pd.isnull(value):\n",
    "        return None\n",
    "    if isinstance(value, pd.Timestamp):\n",
    "        return value.date()  # Convert to a date object\n",
    "    return value\n",
    "\n",
    "# Create a connection to the database\n",
    "conn = psycopg2.connect(**db_params)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Define the insert query with ON CONFLICT DO NOTHING\n",
    "insert_query = \"\"\"\n",
    "    INSERT INTO predictions_tbl (term, prediction_date, prediction, actual)\n",
    "    VALUES (%s, %s, %s, %s)\n",
    "    ON CONFLICT (term, prediction_date) DO NOTHING\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 40\n",
    "batch_values = []\n",
    "\n",
    "try:\n",
    "    # If necessary, ensure 'date' is in datetime.date format\n",
    "    # predictions_df['date'] = pd.to_datetime(predictions_df['date']).dt.date\n",
    "\n",
    "    for index, row in prediction_log_df.iterrows():\n",
    "        # Prepare values for insertion\n",
    "        values = (\n",
    "            safe_value(row['term']),\n",
    "            safe_value(row['date']),\n",
    "            safe_value(row['prediction']),\n",
    "            safe_value(row['actual'])\n",
    "        )\n",
    "\n",
    "        batch_values.append(values)\n",
    "\n",
    "        # Commit in batches\n",
    "        if len(batch_values) >= batch_size:\n",
    "            try:\n",
    "                cursor.executemany(insert_query, batch_values)\n",
    "                conn.commit()\n",
    "                print(f\"Inserted {len(batch_values)} rows at {datetime.now()}\")\n",
    "                batch_values = []  # Clear the batch list\n",
    "            except Exception as e:\n",
    "                print(f\"Error inserting batch: {e}\")\n",
    "                conn.rollback()\n",
    "\n",
    "    # Insert remaining rows\n",
    "    if batch_values:\n",
    "        try:\n",
    "            cursor.executemany(insert_query, batch_values)\n",
    "            conn.commit()\n",
    "            print(f\"Inserted final batch of {len(batch_values)} rows at {datetime.now()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error inserting final batch: {e}\")\n",
    "            conn.rollback()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the cursor and connection\n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d5b332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9816102c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9401d0cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "435f5e74",
   "metadata": {},
   "source": [
    "### Backfill Missing - not currenty working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "403748bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔁 Found 1503 rows to backfill...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m     X_missing[col] \u001b[38;5;241m=\u001b[39m X_missing[col]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[43mmeta_ensemble\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_missing\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     46\u001b[0m preds \u001b[38;5;241m=\u001b[39m (probs \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Backfill\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_voting.py:481\u001b[0m, in \u001b[0;36mVotingClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute probabilities of possible outcomes for samples in X.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \n\u001b[0;32m    469\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;124;03m    Weighted average probability for each class per sample.\u001b[39;00m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    479\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    480\u001b[0m avg \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage(\n\u001b[1;32m--> 481\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect_probas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_weights_not_none\n\u001b[0;32m    482\u001b[0m )\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m avg\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_voting.py:456\u001b[0m, in \u001b[0;36mVotingClassifier._collect_probas\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_collect_probas\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    455\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Collect results from clf.predict calls.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray([clf\u001b[38;5;241m.\u001b[39mpredict_proba(X) \u001b[38;5;28;01mfor\u001b[39;00m clf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_voting.py:456\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_collect_probas\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    455\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Collect results from clf.predict calls.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray([\u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m clf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_voting.py:481\u001b[0m, in \u001b[0;36mVotingClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute probabilities of possible outcomes for samples in X.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \n\u001b[0;32m    469\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;124;03m    Weighted average probability for each class per sample.\u001b[39;00m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    479\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    480\u001b[0m avg \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage(\n\u001b[1;32m--> 481\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect_probas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_weights_not_none\n\u001b[0;32m    482\u001b[0m )\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m avg\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_voting.py:456\u001b[0m, in \u001b[0;36mVotingClassifier._collect_probas\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_collect_probas\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    455\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Collect results from clf.predict calls.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray([clf\u001b[38;5;241m.\u001b[39mpredict_proba(X) \u001b[38;5;28;01mfor\u001b[39;00m clf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_voting.py:456\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_collect_probas\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    455\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Collect results from clf.predict calls.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray([\u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m clf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\pipeline.py:720\u001b[0m, in \u001b[0;36mPipeline.predict_proba\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 720\u001b[0m         Xt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict_proba(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    723\u001b[0m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\compose\\_column_transformer.py:1076\u001b[0m, in \u001b[0;36mColumnTransformer.transform\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1074\u001b[0m     routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_empty_routing()\n\u001b[1;32m-> 1076\u001b[0m Xs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_func_on_transformers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_transform_one\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_as_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_dataframe_and_transform_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_output(Xs)\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Xs:\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;66;03m# All transformers are None\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\compose\\_column_transformer.py:885\u001b[0m, in \u001b[0;36mColumnTransformer._call_func_on_transformers\u001b[1;34m(self, X, y, func, column_as_labels, routed_params)\u001b[0m\n\u001b[0;32m    873\u001b[0m             extra_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    874\u001b[0m         jobs\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    875\u001b[0m             delayed(func)(\n\u001b[0;32m    876\u001b[0m                 transformer\u001b[38;5;241m=\u001b[39mclone(trans) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted \u001b[38;5;28;01melse\u001b[39;00m trans,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    882\u001b[0m             )\n\u001b[0;32m    883\u001b[0m         )\n\u001b[1;32m--> 885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    888\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\pipeline.py:1290\u001b[0m, in \u001b[0;36m_transform_one\u001b[1;34m(transformer, X, y, weight, params)\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform_one\u001b[39m(transformer, X, y, weight, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1269\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call transform and apply weight to output.\u001b[39;00m\n\u001b[0;32m   1270\u001b[0m \n\u001b[0;32m   1271\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1288\u001b[0m \u001b[38;5;124;03m        This should be of the form ``process_routing()[\"step_name\"]``.\u001b[39;00m\n\u001b[0;32m   1289\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1290\u001b[0m     res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mtransform(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mtransform)\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;66;03m# if we have a weight for this transformer, multiply output\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\preprocessing\\_encoders.py:1024\u001b[0m, in \u001b[0;36mOneHotEncoder.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;66;03m# validation of X happens in _check_X called by _transform\u001b[39;00m\n\u001b[0;32m   1020\u001b[0m warn_on_unknown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_unknown \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m   1021\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfrequent_if_exist\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1023\u001b[0m }\n\u001b[1;32m-> 1024\u001b[0m X_int, X_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhandle_unknown\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_unknown\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarn_on_unknown\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarn_on_unknown\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1031\u001b[0m n_samples, n_features \u001b[38;5;241m=\u001b[39m X_int\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drop_idx_after_grouping \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\preprocessing\\_encoders.py:206\u001b[0m, in \u001b[0;36m_BaseEncoder._transform\u001b[1;34m(self, X, handle_unknown, force_all_finite, warn_on_unknown, ignore_category_indices)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_features):\n\u001b[0;32m    205\u001b[0m     Xi \u001b[38;5;241m=\u001b[39m X_list[i]\n\u001b[1;32m--> 206\u001b[0m     diff, valid_mask \u001b[38;5;241m=\u001b[39m \u001b[43m_check_unknown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategories_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(valid_mask):\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m handle_unknown \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\_encode.py:304\u001b[0m, in \u001b[0;36m_check_unknown\u001b[1;34m(values, known_values, return_mask)\u001b[0m\n\u001b[0;32m    301\u001b[0m         valid_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mlen\u001b[39m(values), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m    303\u001b[0m \u001b[38;5;66;03m# check for nans in the known_values\u001b[39;00m\n\u001b[1;32m--> 304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mknown_values\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    305\u001b[0m     diff_is_nan \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39misnan(diff)\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m diff_is_nan\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;66;03m# removes nan from valid_mask\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load predictions log and enriched features\n",
    "df_log = pd.read_csv(\"predictions_log.csv\", parse_dates=[\"date\"])\n",
    "features_df = pd.read_csv(\"final_combined_data.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "# Identify missing predictions or confidence\n",
    "missing_mask = df_log[\"prediction\"].isna() | df_log[\"confidence\"].isna()\n",
    "df_missing = df_log[missing_mask].copy()\n",
    "\n",
    "if df_missing.empty:\n",
    "    print(\"✅ No missing predictions or confidence values found.\")\n",
    "else:\n",
    "    print(f\"🔁 Found {len(df_missing)} rows to backfill...\")\n",
    "\n",
    "    # Merge to get full features\n",
    "    df_backfill = pd.merge(df_missing[[\"date\", \"term\"]], features_df, on=[\"date\", \"term\"], how=\"left\")\n",
    "\n",
    "    # Add date-based features (defragmented)\n",
    "    time_features = pd.DataFrame({\n",
    "        \"day_of_week\": df_backfill[\"date\"].dt.dayofweek,\n",
    "        \"month\": df_backfill[\"date\"].dt.month,\n",
    "        \"day_of_month\": df_backfill[\"date\"].dt.day,\n",
    "        \"is_weekend\": df_backfill[\"date\"].dt.dayofweek >= 5\n",
    "    })\n",
    "    df_backfill = pd.concat([df_backfill.reset_index(drop=True), time_features], axis=1)\n",
    "\n",
    "    # Define model input columns\n",
    "    model_features = features_df.columns.difference([\"date\", \"term\"]).tolist()\n",
    "    model_features += [\"day_of_week\", \"month\", \"day_of_month\", \"is_weekend\"]\n",
    "\n",
    "    # Rebuild feature matrix\n",
    "    X_missing = df_backfill[model_features].copy()\n",
    "\n",
    "    # === CLEANING STEP THAT FIXES YOUR ISSUE ===\n",
    "    # Fill missing object (categorical) columns with string 'missing'\n",
    "    for col in X_missing.select_dtypes(include=\"object\").columns:\n",
    "        X_missing[col] = X_missing[col].fillna(\"missing\").astype(str)\n",
    "\n",
    "    # Fill numeric columns with 0 (or another imputation logic)\n",
    "    for col in X_missing.select_dtypes(include=[\"number\"]).columns:\n",
    "        X_missing[col] = X_missing[col].fillna(0)\n",
    "\n",
    "    # Predict\n",
    "    probs = meta_ensemble.predict_proba(X_missing)[:, 1]\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "    # Backfill\n",
    "    df_log.loc[missing_mask, \"confidence\"] = probs\n",
    "    df_log.loc[missing_mask, \"prediction\"] = preds\n",
    "\n",
    "    # Save\n",
    "    df_log.to_csv(\"predictions_log_backfilled.csv\", index=False)\n",
    "    print(\"✅ Backfill complete and saved to predictions_log_backfilled.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae25420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1029467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Create a pipeline (if you're doing preprocessing)\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Optional preprocessing\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# 2. Define your parameter grid\n",
    "rf_param_grid = {\n",
    "    'classifier__n_estimators': [30, 150],\n",
    "    'classifier__max_depth': [34],\n",
    "    'classifier__min_samples_split': [25, 26],\n",
    "    'classifier__min_samples_leaf': [2, 3]\n",
    "}\n",
    "\n",
    "# 3. Initialize GridSearchCV with the pipeline\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline_rf,  # Your pipeline or RandomForestClassifier directly\n",
    "    param_grid=rf_param_grid,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1  # Use all available CPU cores\n",
    ")\n",
    "\n",
    "# 4. Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 5. Print results\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation Score: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "# 6. Get the best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "#importances = best_rf.named_steps['classifier'].feature_importances_\n",
    "#importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038081ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(pipeline_rf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-Validation Accuracy: {np.mean(scores):.2f} ± {np.std(scores):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021cadba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661725ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Expanded and optimized parameter grid\n",
    "lr_param_grid = {\n",
    "    'classifier__C': np.logspace(-3, .75, 4),  # Wider range with logarithmic spacing [-3, 1, 20]\n",
    "    'classifier__solver': ['liblinear'],  # Both support L1 regularization ['liblinear', 'saga']\n",
    "    'classifier__penalty': ['l1','l2'],  # Test both regularization types ['l1','l2']\n",
    "    'classifier__max_iter': [130,200],  # Ensure convergence\n",
    "    'classifier__class_weight': [None]  # Handle class imbalance [None, 'balanced']\n",
    "}\n",
    "\n",
    "# Enhanced GridSearchCV setup\n",
    "grid_search_lr = GridSearchCV(\n",
    "    estimator=pipeline_lr,\n",
    "    param_grid=lr_param_grid,\n",
    "    cv=5,  # More folds for better validation\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,  # Parallelize computation\n",
    "    verbose=1,  # Show progress\n",
    "    return_train_score=True  # For learning curve analysis\n",
    ")\n",
    "\n",
    "# Fit with timing\n",
    "import time\n",
    "start_time = time.time()\n",
    "grid_search_lr.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "# Enhanced results reporting\n",
    "print(f\"\\nBest Parameters: {grid_search_lr.best_params_}\")\n",
    "print(f\"Best CV Accuracy: {grid_search_lr.best_score_:.4f}\")\n",
    "print(f\"Training Time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Additional useful outputs\n",
    "print(\"\\nTop 5 Parameter Combinations:\")\n",
    "results = pd.DataFrame(grid_search_lr.cv_results_)\n",
    "top_results = results.sort_values('mean_test_score', ascending=False).head(5)\n",
    "print(top_results[['params', 'mean_test_score', 'std_test_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40024fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(pipeline_lr, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-Validation Accuracy: {np.mean(scores):.2f} ± {np.std(scores):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790bf0ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06d9240",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_param_grid = {\n",
    "    'classifier__n_estimators': [100, 200],\n",
    "    'classifier__learning_rate': [0.01, 0.1],\n",
    "    'classifier__max_depth': [3, 7]\n",
    "}\n",
    "\n",
    "grid_search_gb = GridSearchCV(estimator=pipeline_gb, param_grid=gb_param_grid, cv=3, scoring='accuracy')\n",
    "grid_search_gb.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Gradient Boosting Parameters: {grid_search_gb.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac02817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ec1398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# 1. First create a preprocessing pipeline that handles:\n",
    "#    - Missing values (None/NaN)\n",
    "#    - String values that should be numeric\n",
    "#    - Feature scaling\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', make_pipeline(\n",
    "            SimpleImputer(strategy='median'),  # Handles None/NaN\n",
    "            StandardScaler()\n",
    "        ), X_train.select_dtypes(include=['number']).columns),\n",
    "    ],\n",
    "    remainder='drop'  # Drops non-numeric columns\n",
    ")\n",
    "\n",
    "# 2. Now create the full pipeline with preprocessing and SVM\n",
    "pipeline_svm = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(random_state=42))\n",
    "])\n",
    "\n",
    "# 3. Use a simpler parameter grid for initial testing\n",
    "svm_param_grid = {\n",
    "    'classifier__C': [.75, 1, 3],\n",
    "    'classifier__kernel': ['linear', 'rbf'],\n",
    "    'classifier__gamma': ['scale']\n",
    "}\n",
    "\n",
    "# 4. Run grid search with error_score='raise' to see exact errors\n",
    "grid_search_svm = GridSearchCV(\n",
    "    estimator=pipeline_svm,\n",
    "    param_grid=svm_param_grid,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    error_score='raise'  # Will show exact error if any\n",
    ")\n",
    "\n",
    "# 5. Fit the model\n",
    "print(\"Starting SVM hyperparameter tuning...\")\n",
    "try:\n",
    "    grid_search_svm.fit(X_train, y_train)\n",
    "    print(f\"\\nBest SVM Parameters: {grid_search_svm.best_params_}\")\n",
    "    print(f\"Best Cross-Validation Accuracy: {grid_search_svm.best_score_:.3f}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during fitting: {str(e)}\")\n",
    "    print(\"\\nCheck your input data for:\")\n",
    "    print(\"- Non-numeric values (especially strings like 'None')\")\n",
    "    print(\"- Missing values (NaN or None)\")\n",
    "    print(\"- Infinite values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79daf208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(pipeline_svm, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-Validation Accuracy: {np.mean(scores):.2f} ± {np.std(scores):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011a8b97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5006b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_param_grid = {\n",
    "    'classifier__iterations': [140, 145],\n",
    "    'classifier__learning_rate': [0.15, 0.2],\n",
    "    'classifier__depth': [3, 4]\n",
    "}\n",
    "\n",
    "grid_search_cb = GridSearchCV(estimator=pipeline_catboost, param_grid=cb_param_grid, cv=3, scoring='accuracy')\n",
    "grid_search_cb.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best CatBoost Parameters: {grid_search_cb.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960031da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(pipeline_catboost, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-Validation Accuracy: {np.mean(scores):.2f} ± {np.std(scores):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843314c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51f6580",
   "metadata": {},
   "outputs": [],
   "source": [
    "## XGB BOOSTER\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "xgb_param_grid = {\n",
    "    'classifier__n_estimators': [50, 75, 100, 150],\n",
    "    'classifier__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'classifier__max_depth': [3, 5, 7],\n",
    "    'classifier__subsample': [0.6, 0.8, 1.0],\n",
    "    'classifier__colsample_bytree': [0.5, 0.7, 1.0],\n",
    "    'classifier__gamma': [0, 1, 5],  # min split loss\n",
    "    'classifier__reg_lambda': [1, 5, 10],  # L2\n",
    "    'classifier__reg_alpha': [0, 0.5, 1],  # L1\n",
    "}\n",
    "\n",
    "\n",
    "random_search_xgb = RandomizedSearchCV(\n",
    "    estimator=pipeline_xgb,\n",
    "    param_distributions=xgb_param_grid,\n",
    "    n_iter=50,  # try 50 combinations\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "random_search_xgb.fit(X_train, y_train)\n",
    "print(f\"Best XGBoost Parameters: {random_search_xgb.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99d5253",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(pipeline_xgb, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-Validation Accuracy: {np.mean(scores):.2f} ± {np.std(scores):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ab506a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to your existing classifier/pipeline section\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Add to your pipeline definitions\n",
    "pipeline_knn = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Add to your model combinations list\n",
    "models = [\n",
    "    # ... existing models ...\n",
    "    ('knn', pipeline_knn)\n",
    "]\n",
    "\n",
    "# Then you can run grid search with your shown parameters\n",
    "knn_param_grid = {\n",
    "    'classifier__n_neighbors': [15, 20, 25],\n",
    "    'classifier__weights': ['uniform', 'distance'],\n",
    "    'classifier__metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "grid_search_knn = GridSearchCV(estimator=pipeline_knn, \n",
    "                              param_grid=knn_param_grid, \n",
    "                              cv=3, \n",
    "                              scoring='accuracy',\n",
    "                              n_jobs=-1)\n",
    "grid_search_knn.fit(X_train, y_train)\n",
    "print(f\"Best k-NN Parameters: {grid_search_knn.best_params_}\")\n",
    "\n",
    "# Preprocessor remains the same as in your existing code\n",
    "transformers = [\n",
    "    ('num', Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ]), numeric_columns)\n",
    "]\n",
    "if categorical_columns:\n",
    "    transformers.append(('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns))\n",
    "preprocessor = ColumnTransformer(transformers)\n",
    "\n",
    "# Define all model pipelines\n",
    "pipeline_rf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        n_estimators=90,\n",
    "        max_depth=14,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=3,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipeline_lr = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(\n",
    "        C=0.1,\n",
    "        solver='liblinear',\n",
    "        penalty='l1',\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipeline_svm = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(\n",
    "        C=1.0,\n",
    "        kernel='rbf',\n",
    "        gamma='auto',\n",
    "        probability=True,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipeline_lgbm = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LGBMClassifier(\n",
    "        n_estimators=197,\n",
    "        learning_rate=0.24,\n",
    "        max_depth=11,\n",
    "        num_leaves=24,\n",
    "        subsample=0.62,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipeline_et = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ExtraTreesClassifier(\n",
    "        n_estimators=35,\n",
    "        max_depth=8,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=3,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipeline_nb = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GaussianNB())\n",
    "])\n",
    "\n",
    "pipeline_ridge = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RidgeClassifier(\n",
    "        alpha=13.15,\n",
    "        solver='auto',\n",
    "        random_state=42\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafd163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##pipeline_lgbm\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Clean and prepare data\n",
    "X_train = X_train.replace({\n",
    "    'False': False, \n",
    "    'True': True,\n",
    "    'None': np.nan,\n",
    "    'nan': np.nan\n",
    "})\n",
    "\n",
    "# Convert all boolean columns to float (0.0/1.0)\n",
    "bool_cols = X_train.select_dtypes(include=['bool']).columns\n",
    "X_train[bool_cols] = X_train[bool_cols].astype(float)\n",
    "\n",
    "# 2. Identify column types\n",
    "numeric_cols = X_train.select_dtypes(include=['number']).columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "print(f\"Numeric columns: {len(numeric_cols)}\")\n",
    "print(f\"Categorical columns: {len(categorical_cols)}\")\n",
    "\n",
    "# 3. Create transformers\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# 4. Create column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# 5. LightGBM pipeline\n",
    "pipeline_lgbm = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LGBMClassifier(\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 6. Minimal parameter grid for testing\n",
    "lgbm_param_grid = {\n",
    "    'classifier__n_estimators': [220, 260],  # \n",
    "    'classifier__learning_rate': [0.09, 0.11],  # Wider range\n",
    "    'classifier__max_depth': [-1],  # -1 means no limit\n",
    "    'classifier__num_leaves': [23, 24],  # Should be <= 2^max_depth\n",
    "    'classifier__subsample': [0.6,0.7],  # More reasonable fractions\n",
    "    'classifier__colsample_bytree': [0.6],  # Added feature subsampling\n",
    "    'classifier__reg_alpha': [0.2, 0.25],  # L1 regularization\n",
    "    'classifier__reg_lambda': [0.1]  # L2 regularization\n",
    "}\n",
    "\n",
    "# 7. Run grid search\n",
    "grid_search_lgbm = GridSearchCV(\n",
    "    estimator=pipeline_lgbm,\n",
    "    param_grid=lgbm_param_grid,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    error_score='raise'\n",
    ")\n",
    "\n",
    "# 8. Fit the model\n",
    "print(\"\\nStarting LightGBM hyperparameter tuning...\")\n",
    "try:\n",
    "    grid_search_lgbm.fit(X_train, y_train)\n",
    "    print(f\"\\nBest Parameters: {grid_search_lgbm.best_params_}\")\n",
    "    print(f\"Best Accuracy: {grid_search_lgbm.best_score_:.3f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError during fitting: {str(e)}\")\n",
    "    print(\"\\nFinal debugging steps:\")\n",
    "    print(\"1. Check for any remaining non-numeric values:\")\n",
    "    print(X_train.apply(lambda x: pd.api.types.is_numeric_dtype(x)).value_counts())\n",
    "    print(\"\\n2. Check sample of each column type:\")\n",
    "    print(\"\\nNumeric columns sample:\")\n",
    "    print(X_train[numeric_cols].head(2))\n",
    "    print(\"\\nCategorical columns sample:\")\n",
    "    print(X_train[categorical_cols].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d02708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(pipeline_lgbm, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-Validation Accuracy: {np.mean(scores):.2f} ± {np.std(scores):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b77b048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a5fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##EXTRA TREES\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Clean and prepare data\n",
    "X_train = X_train.replace({\n",
    "    'False': False, \n",
    "    'True': True,\n",
    "    'None': np.nan,\n",
    "    'nan': np.nan\n",
    "})\n",
    "\n",
    "# Convert all boolean columns to float (0.0/1.0)\n",
    "bool_cols = X_train.select_dtypes(include=['bool']).columns\n",
    "X_train[bool_cols] = X_train[bool_cols].astype(float)\n",
    "\n",
    "# 2. Identify column types\n",
    "numeric_cols = X_train.select_dtypes(include=['number']).columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "print(f\"Numeric columns: {len(numeric_cols)}\")\n",
    "print(f\"Categorical columns: {len(categorical_cols)}\")\n",
    "\n",
    "# 3. Create transformers\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())  # Optional but can help feature importance\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# 4. Create column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# 5. Extra Trees pipeline\n",
    "pipeline_et = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ExtraTreesClassifier(\n",
    "        random_state=42,\n",
    "        n_jobs=-1  # Use all cores\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 6. Parameter grid - expanded version\n",
    "et_param_grid = {\n",
    "    'classifier__n_estimators': [120, 130],\n",
    "    'classifier__max_depth': [None],  # None for unlimited depth\n",
    "    'classifier__min_samples_split': [4, 5],\n",
    "    'classifier__min_samples_leaf': [2],\n",
    "    'classifier__max_features': [0.5],  # Feature selection\n",
    "    'classifier__bootstrap': [False]  # Whether to bootstrap samples\n",
    "}\n",
    "\n",
    "# 7. Run grid search with error handling\n",
    "grid_search_et = GridSearchCV(\n",
    "    estimator=pipeline_et,\n",
    "    param_grid=et_param_grid,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,  # Parallel processing\n",
    "    verbose=1,\n",
    "    error_score='raise'\n",
    ")\n",
    "\n",
    "# 8. Fit the model\n",
    "print(\"\\nStarting Extra Trees hyperparameter tuning...\")\n",
    "try:\n",
    "    grid_search_et.fit(X_train, y_train)\n",
    "    print(f\"\\nBest Parameters: {grid_search_et.best_params_}\")\n",
    "    print(f\"Best Accuracy: {grid_search_et.best_score_:.3f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    if hasattr(grid_search_et.best_estimator_.named_steps['classifier'], 'feature_importances_'):\n",
    "        print(\"\\nTop 10 Features:\")\n",
    "        importances = grid_search_et.best_estimator_.named_steps['classifier'].feature_importances_\n",
    "        feature_names = (\n",
    "            numeric_cols.tolist() + \n",
    "            grid_search_et.best_estimator_.named_steps['preprocessor']\n",
    "                .named_transformers_['cat']\n",
    "                .named_steps['onehot']\n",
    "                .get_feature_names_out(categorical_cols).tolist()\n",
    "        )\n",
    "        for feat, imp in sorted(zip(feature_names, importances), \n",
    "                              key=lambda x: x[1], reverse=True)[:10]:\n",
    "            print(f\"{feat}: {imp:.4f}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"\\nError during fitting: {str(e)}\")\n",
    "    print(\"\\nDebugging steps:\")\n",
    "    print(\"1. Check for remaining non-numeric values:\")\n",
    "    print(X_train.apply(lambda x: pd.api.types.is_numeric_dtype(x)).value_counts())\n",
    "    print(\"\\n2. Check sample data:\")\n",
    "    print(\"\\nNumeric columns:\")\n",
    "    print(X_train[numeric_cols].head(2))\n",
    "    print(\"\\nCategorical columns:\")\n",
    "    print(X_train[categorical_cols].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c70aeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(pipeline_et, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-Validation Accuracy: {np.mean(scores):.2f} ± {np.std(scores):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afa3636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37860617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb_param_grid = {\n",
    "    'classifier__var_smoothing': [1e-9, 1e-8, 1e-7]\n",
    "}\n",
    "\n",
    "grid_search_nb = GridSearchCV(estimator=pipeline_nb, param_grid=nb_param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_nb.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Naive Bayes Parameters: {grid_search_nb.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122c83f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(pipeline_nb, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-Validation Accuracy: {np.mean(scores):.2f} ± {np.std(scores):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf135ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ee7f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "ridge_param_grid = {\n",
    "    'classifier__alpha': [13.15, 13.25, 13.05],\n",
    "    'classifier__solver': ['auto', 'svd', 'cholesky']\n",
    "}\n",
    "\n",
    "grid_search_ridge = GridSearchCV(estimator=pipeline_ridge, param_grid=ridge_param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_ridge.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Ridge Classifier Parameters: {grid_search_ridge.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eef31c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(pipeline_ridge, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-Validation Accuracy: {np.mean(scores):.2f} ± {np.std(scores):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c5f458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c0bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_param_grid = {\n",
    "    'classifier__n_neighbors': [15, 20, 25],\n",
    "    'classifier__weights': ['uniform', 'distance'],\n",
    "    'classifier__metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "grid_search_knn = GridSearchCV(estimator=pipeline_knn, param_grid=knn_param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_knn.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best k-NN Parameters: {grid_search_knn.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec445580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(pipeline_knn, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-Validation Accuracy: {np.mean(scores):.2f} ± {np.std(scores):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c9905e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702d450c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4069da87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# List of models\n",
    "models = [\n",
    "    ('random_forest', pipeline_rf),\n",
    "    ('logistic_regression', pipeline_lr),\n",
    "    ('gradient_boosting', pipeline_gb),\n",
    "    ('catboost', pipeline_catboost),\n",
    "    ('xgboost', pipeline_xgb),\n",
    "    ('svm', pipeline_svm),\n",
    "    ('lightgbm', pipeline_lgbm),\n",
    "    ('extra_trees', pipeline_et),\n",
    "    ('naive_bayes', pipeline_nb)\n",
    "#    ('ridge', pipeline_ridge),\n",
    "#    ('knn', pipeline_knn)  # Added KNN here\n",
    "]\n",
    "\n",
    "# Generate all combinations (choose subsets of length 2 to all)\n",
    "for r in range(2, len(models) + 1):  # Minimum of 2 models\n",
    "    for combo in combinations(models, r):\n",
    "        print(f\"Evaluating combination: {[name for name, _ in combo]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e74792",
   "metadata": {},
   "source": [
    "Optimize ensemble model selection with Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ae2ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "best_score = 0\n",
    "best_combination = None\n",
    "\n",
    "# Generate and evaluate each combination\n",
    "for r in range(2, len(models) + 1):\n",
    "    for combo in combinations(models, r):\n",
    "        # Create a VotingClassifier with the current combination\n",
    "        ensemble = VotingClassifier(estimators=list(combo), voting='soft')\n",
    "        \n",
    "        # Fit the model\n",
    "        ensemble.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate on the test set\n",
    "        y_pred = ensemble.predict(X_test)\n",
    "        score = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Combination: {[name for name, _ in combo]} | Accuracy: {score:.4f}\")\n",
    "        \n",
    "        # Track the best score and combination\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_combination = combo\n",
    "\n",
    "print(f\"Best Combination: {[name for name, _ in best_combination]} | Accuracy: {best_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106c64c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = []\n",
    "\n",
    "for r in range(2, len(models) + 1):\n",
    "    for combo in combinations(models, r):\n",
    "        ensemble = VotingClassifier(estimators=list(combo), voting='soft')\n",
    "        ensemble.fit(X_train, y_train)\n",
    "        y_pred = ensemble.predict(X_test)\n",
    "        score = accuracy_score(y_test, y_pred)\n",
    "        results.append({'combination': [name for name, _ in combo], 'accuracy': score})\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by='accuracy', ascending=False)\n",
    "\n",
    "# Display the best combinations\n",
    "print(results_df.head(10))\n",
    "#Combination: ['gradient_boosting', 'catboost'] | Accuracy: 0.7042"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e43d313",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = []\n",
    "\n",
    "for r in range(2, len(models) + 1):\n",
    "    for combo in combinations(models, r):\n",
    "        ensemble = VotingClassifier(estimators=list(combo), voting='soft')\n",
    "        ensemble.fit(X_train, y_train)\n",
    "        y_pred = ensemble.predict(X_test)\n",
    "        score = accuracy_score(y_test, y_pred)\n",
    "        results.append({'combination': [name for name, _ in combo], 'accuracy': score})\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by='accuracy', ascending=False)\n",
    "\n",
    "# Display the best combinations\n",
    "print(results_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050586c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_combination_names = [name for name, _ in best_combination]\n",
    "print(f\"Best Models: {best_combination_names}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912abd9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856cfcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 172        [random_forest, svm, lightgbm, extra_trees]\n",
    "# 65   [logistic_regression, gradient_boosting, xgboost]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130d3076",
   "metadata": {},
   "source": [
    "# Meta-Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83382a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First ensemble\n",
    "ensemble_1 = VotingClassifier(\n",
    "    estimators=[\n",
    "#        ('gradient_boosting', pipeline_gb),\n",
    "        ('randomforest', pipeline_rf),\n",
    "        ('svm', pipeline_svm),\n",
    "        ('lightgbm', pipeline_lgbm),\n",
    "        ('extratrees', pipeline_et)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Second ensemble\n",
    "ensemble_2 = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('gradient_boosting', pipeline_gb),\n",
    "        ('logistic_regression', pipeline_lr),\n",
    "#        ('lightgbm', pipeline_lgbm)\n",
    "#        ('catboost', pipeline_catboost),\n",
    "        ('xgboost', pipeline_xgb)\n",
    "#        ('svm', pipeline_svm)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2768d80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the first-level ensembles with progress bar\n",
    "print(\"Training First-Level Ensembles...\")\n",
    "with tqdm(total=2, desc=\"First-Level Ensembles\") as pbar:\n",
    "    ensemble_1.fit(X_train, y_train)  # Fit ensemble_1\n",
    "    pbar.update(1)\n",
    "    ensemble_2.fit(X_train, y_train)  # Fit ensemble_2\n",
    "    pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f34ea95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the meta-ensemble\n",
    "meta_ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('ensemble_1', ensemble_1),\n",
    "        ('ensemble_2', ensemble_2)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Fit the meta-ensemble\n",
    "print(\"Training Meta-Ensemble...\")\n",
    "meta_ensemble.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00a4274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the meta-ensemble\n",
    "y_pred_meta = meta_ensemble.predict(X_test)\n",
    "accuracy_meta = accuracy_score(y_test, y_pred_meta)\n",
    "print(f'Meta-Ensemble Accuracy: {accuracy_meta:.2f}')\n",
    "print(\"\\nMeta-Ensemble Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_meta))\n",
    "print(\"\\nMeta-Ensemble Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_meta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f8f925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc5fbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32da49e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129400c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f47ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba91fac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ✅ Deduplicate predictions before saving\n",
    "predictions_df = predictions_df.sort_values(by='confidence', ascending=False)\n",
    "predictions_df = predictions_df.drop_duplicates(subset=['date', 'term'], keep='first')\n",
    "\n",
    "# Optional sanity check\n",
    "assert predictions_df[['date', 'term']].duplicated().sum() == 0, \"Still has duplicates!\"\n",
    "\n",
    "# Save cleaned predictions\n",
    "predictions_df.to_csv(\"predictions_log.csv\", index=False)\n",
    "print(\"✅ Cleaned predictions_log.csv saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
