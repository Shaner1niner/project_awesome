{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "946923ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07-Jun-25 17:47:58 - NumExpr defaulting to 4 threads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shane\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from ntscraper import Nitter\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Roberta Pretrained Model\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "\n",
    "#natural language toolkit\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "import psycopg2\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# picking a specific model that has been pretrained on data for sentiment analysis\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\" # Define the pre-trained modelt rained specifically on Twitter data\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL) # Load the tokenizer from the pre-trained model to convert text into tokens suitable for the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL) # Load the pre-trained sentiment classification model that will be used to classify text into different sentiment categories\n",
    "\n",
    "def refresh_twt_aggregated():\n",
    "    conn = psycopg2.connect(\"dbname=twt_snt user=postgres password=Ilpmnl!69gg\")\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"REFRESH MATERIALIZED VIEW twt_aggregated_tbl;\")\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "# create a function to run our Roberta model on the entire dataset\n",
    "def polarity_scores_roberta(example):\n",
    "    # Encode the input text into the appropriate format for the model (PyTorch tensors)\n",
    "    encoded_text = tokenizer(example, return_tensors='pt')\n",
    "    # Pass the encoded text through the Roberta model to get sentiment logits (raw model outputs)\n",
    "    output = model(**encoded_text)\n",
    "    # Extract the output tensor and convert it to a NumPy array for further processing\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    # Apply the softmax function to get normalized probabilities for negative, neutral, and positive sentiments\n",
    "    # Using probabilities is important for interpretation. The softmax output tells you how likely the model thinks the input text belongs to each of the sentiment categories\n",
    "    scores = softmax(scores)\n",
    "    # Create a dictionary to hold the sentiment scores: negative, neutral, and positive\n",
    "    scores_dict = {\n",
    "        'roberta_neg' : scores[0],\n",
    "        'roberta_neu' : scores[1],\n",
    "        'roberta_pos' : scores[2]\n",
    "    }\n",
    "    \n",
    "    return scores_dict\n",
    "\n",
    "# create a function to address funky date format of import tweets\n",
    "def clean_date(date_str):\n",
    "    try:\n",
    "        # Clean up the date string\n",
    "        cleaned_date_str = date_str[:-4].replace(' ¬∑ ', ' ')\n",
    "        # Use pandas to parse the date and convert to a standard format\n",
    "        dt = pd.to_datetime(cleaned_date_str, format='%b %d, %Y %I:%M %p')\n",
    "        return dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    except Exception as e:\n",
    "        print(f\"Date parsing error for {date_str}: {e}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "        \n",
    "def safe_value(value):\n",
    "    if isinstance(value, dict):\n",
    "        return json.dumps(value)\n",
    "    elif isinstance(value, str):\n",
    "        try:\n",
    "            # Attempt to load as JSON to ensure it's valid\n",
    "            json.loads(value)\n",
    "            return value\n",
    "        except json.JSONDecodeError:\n",
    "            # Return the raw string if it's not valid JSON\n",
    "            return None\n",
    "    elif isinstance(value, pd.Timestamp):  # Convert pandas datetime to string\n",
    "        return value.strftime('%Y-%m-%d') if not pd.isnull(value) else None\n",
    "    elif value is None:  # Explicitly handle None values\n",
    "        return None\n",
    "    else:\n",
    "        return None if pd.isnull(value) else value\n",
    "\n",
    "        \n",
    "# Function to capture current time\n",
    "def print_current_time():\n",
    "    now = datetime.datetime.now()\n",
    "    print(f\"Current Time: {now.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68c741d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "\n",
    "# Twitter API credentials\n",
    "bearer_token = 'AAAAAAAAAAAAAAAAAAAAAJ%2BFvAEAAAAATIyA4cKQKD76g8ve04n4J5Igono%3Dlak30GBoA3qa0ZH8rYLF879i6N2W11OX8fqGxRGFRfRRtdjy0s'  # Replace with your actual bearer token\n",
    "\n",
    "# Initialize the Tweepy client\n",
    "client = tweepy.Client(bearer_token=bearer_token)\n",
    "\n",
    "# List of terms to search for (including top 10 cryptocurrencies and top 10 stocks by volume)\n",
    "search_terms = [\n",
    "    \n",
    "    'SOL', 'KAS', 'LINK', 'ADA', 'MATIC', 'AVAX', 'DOGE', 'BTC', 'ETH', 'POPCAT', 'SUI', 'HNT', 'WIF'\n",
    "#         'GOOGL','TSLA', 'TSMC', 'CVX', 'COIN', 'NFLX', 'DIS', 'AMZN', 'MSFT', 'AAPL', 'GME', 'NVDA', 'JPM'#  Top stocks by volume plus my interests\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Prepare an empty DataFrame to collect all tweets\n",
    "all_tweets_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each term\n",
    "for term in search_terms:\n",
    "    query = f'#{term} -is:retweet'  # Exclude retweets with '-is:retweet'\n",
    "\n",
    "    try:\n",
    "        # Retrieve tweets\n",
    "        tweets = client.search_recent_tweets(query=query, max_results=10, tweet_fields=['created_at', 'public_metrics', 'entities', 'referenced_tweets', 'geo', 'lang', 'source'])\n",
    "\n",
    "        if tweets.data is None:\n",
    "            print(f\"No tweets found for term: {term}\")\n",
    "        else:\n",
    "            # Extract tweets data into a list of dictionaries\n",
    "            tweets_data = []\n",
    "            for tweet in tweets.data:\n",
    "                created_at = tweet.created_at\n",
    "                tweet_info = {\n",
    "                    'term': term,\n",
    "                    'web_link': f\"https://twitter.com/twitter/status/{tweet.id}\",\n",
    "                    'twt_text': tweet.text,\n",
    "                    'user_name': tweet.author_id,\n",
    "                    'username': tweet.author_id,\n",
    "                    'date': created_at.date(),\n",
    "                    'time': created_at.strftime('%H:%M:%S'),\n",
    "                    'retweets': tweet.public_metrics.get('retweet_count', 0),\n",
    "                    'quotes': tweet.public_metrics.get('quote_count', 0),\n",
    "                    'likes': tweet.public_metrics.get('like_count', 0),\n",
    "                    'hashtags': ','.join([hashtag['tag'] for hashtag in tweet.entities.get('hashtags', [])]) if tweet.entities else '',\n",
    "                    'mentions': ','.join([mention['username'] for mention in tweet.entities.get('mentions', [])]) if tweet.entities else '',\n",
    "                    'urls': ','.join([url['expanded_url'] for url in tweet.entities.get('urls', [])]) if tweet.entities else '',\n",
    "                    'reply_to_tweet_id': tweet.referenced_tweets[0]['id'] if tweet.referenced_tweets and tweet.referenced_tweets[0]['type'] == 'replied_to' else None,\n",
    "                    'reply_to_username': None,  # Placeholder for reply_to_username\n",
    "                    'location': tweet.geo['place_id'] if tweet.geo else None,\n",
    "                    'twt_language': tweet.lang,\n",
    "                    'twt_source': tweet.source,\n",
    "                }\n",
    "                tweets_data.append(tweet_info)\n",
    "\n",
    "            # Convert list of dictionaries to DataFrame\n",
    "            term_df = pd.DataFrame(tweets_data)\n",
    "\n",
    "            # Append to the main DataFrame\n",
    "            all_tweets_df = pd.concat([all_tweets_df, term_df], ignore_index=True)\n",
    "\n",
    "    except tweepy.TweepyException as e:\n",
    "        print(f\"An error occurred while retrieving tweets for term {term}: {e}\")\n",
    "\n",
    "# After collecting all tweets, assign it to raw_df\n",
    "raw_df = all_tweets_df\n",
    "\n",
    "# Display the collected DataFrame\n",
    "# print(raw_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1af31e73",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    term                                           web_link  \\\n",
      "0    SOL  https://twitter.com/twitter/status/19315136381...   \n",
      "1    SOL  https://twitter.com/twitter/status/19315136323...   \n",
      "2    SOL  https://twitter.com/twitter/status/19315135544...   \n",
      "3    SOL  https://twitter.com/twitter/status/19315135492...   \n",
      "4    SOL  https://twitter.com/twitter/status/19315135449...   \n",
      "..   ...                                                ...   \n",
      "125  WIF  https://twitter.com/twitter/status/19315027985...   \n",
      "126  WIF  https://twitter.com/twitter/status/19315027540...   \n",
      "127  WIF  https://twitter.com/twitter/status/19314909096...   \n",
      "128  WIF  https://twitter.com/twitter/status/19314875858...   \n",
      "129  WIF  https://twitter.com/twitter/status/19314816486...   \n",
      "\n",
      "                                              twt_text user_name username  \\\n",
      "0            üîó https://t.co/ahhHVNi0Hd\\n#SOL #Outlight      None     None   \n",
      "1    @RoundtableSpace Happy SUNDAY! ‚òÄÔ∏è\\n\\nTime to r...      None     None   \n",
      "2    #Trump #BTC #SOL #Airdrop #CryptoAttacks #Cryp...      None     None   \n",
      "3    #Trump #BTC #SOL #Airdrop #CryptoAttacks #Cryp...      None     None   \n",
      "4    üèÜBest Online CasinoüèÜ\\n\\nü•áÔºëÔºöStake\\nhttps://t.co...      None     None   \n",
      "..                                                 ...       ...      ...   \n",
      "125  FORGET #BITCOIN, #ALTCOINS are MOVING! With ht...      None     None   \n",
      "126  üîç **memecoins Index Performance** For 07/06/20...      None     None   \n",
      "127  #UFD #FartCoin #Bonk #Wif #Pengu #House #Chill...      None     None   \n",
      "128  When the history books are written‚Ä¶\\n\\n$MGF wi...      None     None   \n",
      "129  $WIF (All targets reached)\\n\\nüìà #WIFUSDT New S...      None     None   \n",
      "\n",
      "           date      time  retweets  quotes  likes  \\\n",
      "0    2025-06-08  00:48:31         0       0      0   \n",
      "1    2025-06-08  00:48:30         0       0      0   \n",
      "2    2025-06-08  00:48:11         0       0      0   \n",
      "3    2025-06-08  00:48:10         0       0      0   \n",
      "4    2025-06-08  00:48:09         0       0      0   \n",
      "..          ...       ...       ...     ...    ...   \n",
      "125  2025-06-08  00:05:27         1       0      0   \n",
      "126  2025-06-08  00:05:16         0       0      0   \n",
      "127  2025-06-07  23:18:12         0       0      0   \n",
      "128  2025-06-07  23:05:00         0       0      1   \n",
      "129  2025-06-07  22:41:24         0       0      0   \n",
      "\n",
      "                                              hashtags  \\\n",
      "0                                         SOL,Outlight   \n",
      "1                 SOL,Crypto,Memes,BlockchainCommunity   \n",
      "2           Trump,BTC,SOL,Airdrop,CryptoAttacks,Crypto   \n",
      "3           Trump,BTC,SOL,Airdrop,CryptoAttacks,Crypto   \n",
      "4    Bitcoin,Crypto,Ethereum,BTC,SOL,XRP,ETH,ALPACA...   \n",
      "..                                                 ...   \n",
      "125  BITCOIN,ALTCOINS,BTC,ETH,AB,INJ,SPX,ENA,LDO,PE...   \n",
      "126                      WIF,BONK,PEPE,memecoins,memes   \n",
      "127  UFD,FartCoin,Bonk,Wif,Pengu,House,Chillguy,Pop...   \n",
      "128  Solana,DOGE,srchafreen,ETH,BNB,CryptoGems,pump...   \n",
      "129               WIFUSDT,WIF,Crypto,AiTrading,Signals   \n",
      "\n",
      "                      mentions  \\\n",
      "0                                \n",
      "1    RoundtableSpace,nftlxcoin   \n",
      "2                                \n",
      "3                                \n",
      "4                                \n",
      "..                         ...   \n",
      "125                              \n",
      "126                              \n",
      "127                              \n",
      "128                              \n",
      "129                              \n",
      "\n",
      "                                                  urls      reply_to_tweet_id  \\\n",
      "0                                https://outlight.fun/  1931513637409059072.0   \n",
      "1    https://t.me/nftlxcoin,https://x.com/maxzyblac...  1931497651599462400.0   \n",
      "2    https://x.com/MaxCryptoAttack/status/193120650...                    NaN   \n",
      "3    https://x.com/MaxCryptoAttack/status/193120650...                    NaN   \n",
      "4    http://tgr.jp/stake,http://tgr.jp/roobet,http:...                    NaN   \n",
      "..                                                 ...                    ...   \n",
      "125  https://bit.ly/3mMsJZ4,https://x.com/kellysiqi...                    NaN   \n",
      "126  https://x.com/crypto_indices/status/1931502754...  1931502744751177984.0   \n",
      "127                                                     1931490827362246656.0   \n",
      "128  https://mtgoxf.com,https://x.com/MtGoxf/status...                    NaN   \n",
      "129  https://t.me/+PebYpgMYXyk2ZmQ0,https://x.com/c...                    NaN   \n",
      "\n",
      "    reply_to_username location twt_language twt_source  \n",
      "0                None     None          qme       None  \n",
      "1                None     None           en       None  \n",
      "2                None     None          qme       None  \n",
      "3                None     None          qme       None  \n",
      "4                None     None           en       None  \n",
      "..                ...      ...          ...        ...  \n",
      "125              None     None           en       None  \n",
      "126              None     None           en       None  \n",
      "127              None     None          qht       None  \n",
      "128              None     None           en       None  \n",
      "129              None     None           en       None  \n",
      "\n",
      "[130 rows x 18 columns]\n"
     ]
    }
   ],
   "source": [
    "print(raw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f997845",
   "metadata": {},
   "source": [
    "#### Sentiment Analyzers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0967e97",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c337ec6a4b740b194035b11c66f978f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis completed.\n",
      "Current Time: 2025-06-07 18:40:24\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# Initialize a SentimentIntensityAnalyzer instance for VADER sentiment analysis\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Create an empty dictionary to store sentiment results\n",
    "res = {}\n",
    "\n",
    "# Iterate over each row in the raw DataFrame using tqdm to provide a progress bar\n",
    "for i, row in tqdm(raw_df.iterrows(), total=len(raw_df)):\n",
    "    try:\n",
    "        # Extract the text and unique identifier from the current row\n",
    "        text = row['twt_text']\n",
    "        myid = row['web_link']\n",
    "        \n",
    "        # Check if the tweet ID (web_link) has already been processed\n",
    "        # If it exists in `res`, skip to the next iteration to avoid duplicates\n",
    "        if myid in res:\n",
    "            continue\n",
    "        \n",
    "        # Calculate VADER sentiment scores for the text\n",
    "        vader_result = sia.polarity_scores(text)\n",
    "        \n",
    "        # Rename the keys in VADER results to avoid naming conflicts (e.g., 'vader_neg', 'vader_neu', 'vader_pos')\n",
    "        vader_result_rename = {f\"vader_{key}\": value for key, value in vader_result.items()}\n",
    "        \n",
    "        # Get sentiment scores using Roberta model\n",
    "        roberta_result = polarity_scores_roberta(text)\n",
    "        \n",
    "        # Combine both VADER and Roberta sentiment results into a single dictionary\n",
    "        both = {**vader_result_rename, **roberta_result}\n",
    "        \n",
    "        # Store the combined results in `res` dictionary using weblink `myid`, as the key\n",
    "        res[myid] = both\n",
    "    except RuntimeError:\n",
    "        # If a RuntimeError occurs (e.g., due to an error in text encoding or other issues), print the ID that caused the problem\n",
    "        print(f'Broke for id {myid}')\n",
    "\n",
    "# End of sentiment analysis\n",
    "print(\"Sentiment analysis completed.\")\n",
    "\n",
    "# Print the current time to show when the analysis was completed\n",
    "print_current_time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2fb8170",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              web_link  vader_neg  vader_neu  \\\n",
      "125  https://twitter.com/twitter/status/19315027985...      0.070      0.887   \n",
      "126  https://twitter.com/twitter/status/19315027540...      0.260      0.636   \n",
      "127  https://twitter.com/twitter/status/19314909096...      0.000      1.000   \n",
      "128  https://twitter.com/twitter/status/19314875858...      0.036      0.964   \n",
      "129  https://twitter.com/twitter/status/19314816486...      0.000      1.000   \n",
      "\n",
      "     vader_pos  vader_compound  roberta_neg  roberta_neu  roberta_pos term  \\\n",
      "125      0.043         -0.2789     0.012002     0.699183     0.288815  WIF   \n",
      "126      0.104         -0.7506     0.160476     0.729054     0.110470  WIF   \n",
      "127      0.000          0.0000     0.078001     0.858396     0.063602  WIF   \n",
      "128      0.000         -0.1027     0.182382     0.639415     0.178203  WIF   \n",
      "129      0.000          0.0000     0.020538     0.899631     0.079831  WIF   \n",
      "\n",
      "                                              twt_text  ... quotes likes  \\\n",
      "125  FORGET #BITCOIN, #ALTCOINS are MOVING! With ht...  ...      0     0   \n",
      "126  üîç **memecoins Index Performance** For 07/06/20...  ...      0     0   \n",
      "127  #UFD #FartCoin #Bonk #Wif #Pengu #House #Chill...  ...      0     0   \n",
      "128  When the history books are written‚Ä¶\\n\\n$MGF wi...  ...      0     1   \n",
      "129  $WIF (All targets reached)\\n\\nüìà #WIFUSDT New S...  ...      0     0   \n",
      "\n",
      "                                              hashtags mentions  \\\n",
      "125  BITCOIN,ALTCOINS,BTC,ETH,AB,INJ,SPX,ENA,LDO,PE...            \n",
      "126                      WIF,BONK,PEPE,memecoins,memes            \n",
      "127  UFD,FartCoin,Bonk,Wif,Pengu,House,Chillguy,Pop...            \n",
      "128  Solana,DOGE,srchafreen,ETH,BNB,CryptoGems,pump...            \n",
      "129               WIFUSDT,WIF,Crypto,AiTrading,Signals            \n",
      "\n",
      "                                                  urls      reply_to_tweet_id  \\\n",
      "125  https://bit.ly/3mMsJZ4,https://x.com/kellysiqi...                    NaN   \n",
      "126  https://x.com/crypto_indices/status/1931502754...  1931502744751177984.0   \n",
      "127                                                     1931490827362246656.0   \n",
      "128  https://mtgoxf.com,https://x.com/MtGoxf/status...                    NaN   \n",
      "129  https://t.me/+PebYpgMYXyk2ZmQ0,https://x.com/c...                    NaN   \n",
      "\n",
      "     reply_to_username location twt_language twt_source  \n",
      "125               None     None           en       None  \n",
      "126               None     None           en       None  \n",
      "127               None     None          qht       None  \n",
      "128               None     None           en       None  \n",
      "129               None     None           en       None  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "2025-06-08\n"
     ]
    }
   ],
   "source": [
    "# Append results of sentiment analysis to create a new DataFrame\n",
    "# Convert the dictionary `res` (which contains sentiment analysis results) into a DataFrame and transpose it\n",
    "results_df = pd.DataFrame(res).T\n",
    "results_df  # Display the resulting DataFrame\n",
    "\n",
    "# Reset the index of `results_df` and rename the index column to 'web_link'\n",
    "# This helps to ensure that 'web_link' is treated as a regular column rather than an index\n",
    "results_df = results_df.reset_index().rename(columns={'index': 'web_link'})\n",
    "results_df  # Display the updated DataFrame\n",
    "\n",
    "# Merge the sentiment results (`results_df`) with the original raw DataFrame (`raw_df`)\n",
    "# The merge is done using a 'right' join, meaning all rows from `raw_df` will be retained,\n",
    "# and sentiment results will be appended where a match is found by the 'web_link' key\n",
    "df_submit = results_df.merge(raw_df, how='right')\n",
    "\n",
    "\n",
    "\n",
    "# results_df.head()\n",
    "# res\n",
    "print(df_submit.tail()) \n",
    "print(df_submit['date'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff8a7ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b005022",
   "metadata": {},
   "source": [
    "#### Insert into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "333fb6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 40 rows at 2025-06-07 18:40:26.980383\n",
      "Inserted 40 rows at 2025-06-07 18:40:27.057737\n",
      "Inserted 40 rows at 2025-06-07 18:40:27.128563\n",
      "Inserted final batch of 10 rows at 2025-06-07 18:40:27.147214\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    'dbname': 'twt_snt',\n",
    "    'user': 'postgres',\n",
    "    'password': 'Ilpmnl!69gg',\n",
    "    'host': 'localhost',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "# Function to ensure data types are compatible with SQL\n",
    "def safe_value(value):\n",
    "    if isinstance(value, str):\n",
    "        return value\n",
    "    elif isinstance(value, pd.Timestamp):  # Convert pandas datetime to string\n",
    "        return value.strftime('%Y-%m-%d') if not pd.isnull(value) else None\n",
    "    elif value is None:  # Explicitly handle None values\n",
    "        return None\n",
    "    else:\n",
    "        return None if pd.isnull(value) else value\n",
    "\n",
    "# Create a connection to the database\n",
    "conn = psycopg2.connect(**db_params)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Define the insert query with ON CONFLICT DO NOTHING\n",
    "insert_query = \"\"\"\n",
    "    INSERT INTO twt_tbl (web_link, term, twt_text, user_name, username, date, time, retweets, quotes, likes, hashtags, mentions, urls, reply_to_tweet_id, reply_to_username, location, twt_language, twt_source, vader_neg, vader_neu, vader_pos, vader_compound, roberta_neg, roberta_neu, roberta_pos)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    ON CONFLICT (web_link) DO NOTHING\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 40\n",
    "batch_values = []\n",
    "\n",
    "try:\n",
    "    df_submit['date'] = pd.to_datetime(df_submit['date']).dt.date  # Ensure date format\n",
    "\n",
    "    for index, row in df_submit.iterrows():\n",
    "        # Check if twt_text length exceeds the limit\n",
    "        if len(row['twt_text']) > 1001:\n",
    "            print(f\"Skipping row {index} due to twt_text length: {len(row['twt_text'])}\")\n",
    "            continue\n",
    "\n",
    "        # Prepare values for insertion\n",
    "        values = (\n",
    "            safe_value(row['web_link']),\n",
    "            safe_value(row['term']),\n",
    "            safe_value(row['twt_text']),\n",
    "            safe_value(row['user_name']),\n",
    "            safe_value(row['username']),\n",
    "            safe_value(row['date']),\n",
    "            safe_value(row['time']),\n",
    "            safe_value(row['retweets']),\n",
    "            safe_value(row['quotes']),\n",
    "            safe_value(row['likes']),\n",
    "            safe_value(row['hashtags']),\n",
    "            safe_value(row['mentions']),\n",
    "            safe_value(row['urls']),\n",
    "            safe_value(row['reply_to_tweet_id']) if not pd.isna(row['reply_to_tweet_id']) else None,\n",
    "            safe_value(row['reply_to_username']),\n",
    "            safe_value(row['location']),\n",
    "            safe_value(row['twt_language']),\n",
    "            safe_value(row['twt_source']),\n",
    "            safe_value(row['vader_neg']),\n",
    "            safe_value(row['vader_neu']),\n",
    "            safe_value(row['vader_pos']),\n",
    "            safe_value(row['vader_compound']),\n",
    "            safe_value(row['roberta_neg']),\n",
    "            safe_value(row['roberta_neu']),\n",
    "            safe_value(row['roberta_pos']),\n",
    "        )\n",
    "        \n",
    "        batch_values.append(values)\n",
    "\n",
    "        # Commit in batches\n",
    "        if len(batch_values) >= batch_size:\n",
    "            try:\n",
    "                cursor.executemany(insert_query, batch_values)\n",
    "                conn.commit()\n",
    "                print(f\"Inserted {len(batch_values)} rows at {datetime.now()}\")\n",
    "                batch_values = []  # Clear the batch list\n",
    "            except Exception as e:\n",
    "                print(f\"Error inserting batch: {e}\")\n",
    "                conn.rollback()\n",
    "\n",
    "    # Insert remaining rows\n",
    "    if batch_values:\n",
    "        try:\n",
    "            cursor.executemany(insert_query, batch_values)\n",
    "            conn.commit()\n",
    "            print(f\"Inserted final batch of {len(batch_values)} rows at {datetime.now()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error inserting final batch: {e}\")\n",
    "            conn.rollback()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the cursor and connection\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    \n",
    "# Call this function after inserting tweets\n",
    "refresh_twt_aggregated()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b9cf3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b94db77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614320a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52a3845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91888738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99e375ed",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to expand username tweet pull</summary>\n",
    "\n",
    "```python\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "\n",
    "# Twitter API credentials\n",
    "bearer_token = 'AAAAAAAAAAAAAAAAAAAAAJ%2BFvAEAAAAATIyA4cKQKD76g8ve04n4J5Igono%3Dlak30GBoA3qa0ZH8rYLF879i6N2W11OX8fqGxRGFRfRRtdjy0s'  # Replace with your actual bearer token\n",
    "\n",
    "# Initialize the Tweepy client\n",
    "client = tweepy.Client(bearer_token=bearer_token)\n",
    "\n",
    "# List of usernames to search for (example usernames)\n",
    "usernames = [\n",
    "    'elonmusk',  # Replace with desired usernames\n",
    "    'jack'\n",
    "]\n",
    "\n",
    "# Prepare an empty DataFrame to collect all tweets\n",
    "all_tweets_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each username\n",
    "for username in usernames:\n",
    "    query = f'from:{username} -is:retweet'  # Exclude retweets with '-is:retweet'\n",
    "\n",
    "    try:\n",
    "        # Retrieve tweets\n",
    "        tweets = client.search_recent_tweets(query=query, max_results=10, tweet_fields=['created_at', 'public_metrics', 'entities', 'referenced_tweets', 'geo', 'lang', 'source'])\n",
    "\n",
    "        if tweets.data is None:\n",
    "            print(f\"No tweets found for username: {username}\")\n",
    "        else:\n",
    "            # Extract tweets data into a list of dictionaries\n",
    "            tweets_data = []\n",
    "            for tweet in tweets.data:\n",
    "                created_at = tweet.created_at\n",
    "                tweet_info = {\n",
    "                    'username': username,\n",
    "                    'web_link': f\"https://twitter.com/twitter/status/{tweet.id}\",\n",
    "                    'twt_text': tweet.text,\n",
    "                    'user_name': tweet.author_id,\n",
    "                    'date': created_at.date(),\n",
    "                    'time': created_at.strftime('%H:%M:%S'),\n",
    "                    'retweets': tweet.public_metrics.get('retweet_count', 0),\n",
    "                    'quotes': tweet.public_metrics.get('quote_count', 0),\n",
    "                    'likes': tweet.public_metrics.get('like_count', 0),\n",
    "                    'hashtags': ','.join([hashtag['tag'] for hashtag in tweet.entities.get('hashtags', [])]) if tweet.entities else '',\n",
    "                    'mentions': ','.join([mention['username'] for mention in tweet.entities.get('mentions', [])]) if tweet.entities else '',\n",
    "                    'urls': ','.join([url['expanded_url'] for url in tweet.entities.get('urls', [])]) if tweet.entities else '',\n",
    "                    'reply_to_tweet_id': tweet.referenced_tweets[0]['id'] if tweet.referenced_tweets and tweet.referenced_tweets[0]['type'] == 'replied_to' else None,\n",
    "                    'reply_to_username': None,  # Placeholder for reply_to_username\n",
    "                    'location': tweet.geo['place_id'] if tweet.geo else None,\n",
    "                    'twt_language': tweet.lang,\n",
    "                    'twt_source': tweet.source,\n",
    "                }\n",
    "                tweets_data.append(tweet_info)\n",
    "\n",
    "            # Convert list of dictionaries to DataFrame\n",
    "            term_df = pd.DataFrame(tweets_data)\n",
    "\n",
    "            # Append to the main DataFrame\n",
    "            all_tweets_df = pd.concat([all_tweets_df, term_df], ignore_index=True)\n",
    "\n",
    "    except tweepy.TweepyException as e:\n",
    "        print(f\"An error occurred while retrieving tweets for username {username}: {e}\")\n",
    "\n",
    "# After collecting all tweets, assign it to raw_df\n",
    "raw_df = all_tweets_df\n",
    "\n",
    "# Display the collected DataFrame\n",
    "print(raw_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "533909bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tweepy\n",
    "#import pandas as pd\n",
    "#import time\n",
    "\n",
    "# Twitter API credentials\n",
    "# bearer_token = 'AAAAAAAAAAAAAAAAAAAAAJ%2BFvAEAAAAATIyA4cKQKD76g8ve04n4J5Igono%3Dlak30GBoA3qa0ZH8rYLF879i6N2W11OX8fqGxRGFRfRRtdjy0s'  # Replace with your actual bearer token\n",
    "\n",
    "# Initialize the Tweepy client\n",
    "# client = tweepy.Client(bearer_token=bearer_token)\n",
    "\n",
    "# List of terms to search for (including top 10 cryptocurrencies and top 10 stocks by volume)\n",
    "#search_terms = [\n",
    "#    'BTC', 'ETH', 'BNB', 'AVAX', 'DOGE', 'ADA', 'SOL', 'MATIC', 'LINK', 'KAS',  # Top cryptocurrencies\n",
    "#    'AAPL', 'TSLA', 'AMZN', 'MSFT', 'GOOGL', 'GME', 'NVDA' #, 'JPM', 'V',  # Top stocks by volume\n",
    "#    #'DXY'  # Forex\n",
    "#]\n",
    "\n",
    "# Prepare an empty DataFrame to collect all tweets\n",
    "#all_tweets_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each term\n",
    "#for term in search_terms:\n",
    "#    query = f'#{term} -is:retweet'  # Exclude retweets with '-is:retweet'\n",
    "\n",
    "#    try:\n",
    "#        # Retrieve tweets\n",
    "#        tweets = client.search_recent_tweets(query=query, max_results=20, tweet_fields=['created_at', 'public_metrics', 'entities', 'referenced_tweets', 'geo', 'lang', 'source'])\n",
    "\n",
    "#        if tweets.data is None:\n",
    "#            print(f\"No tweets found for term: {term}\")\n",
    "#        else:\n",
    "#            # Extract tweets data into a list of dictionaries\n",
    "#           tweets_data = []\n",
    "#            for tweet in tweets.data:\n",
    "#$                created_at = tweet.created_at\n",
    "#                tweet_info = {\n",
    "#                    'term': term,\n",
    "#                    'web_link': f\"https://twitter.com/twitter/status/{tweet.id}\",\n",
    "#                    'twt_text': tweet.text,\n",
    "#                    'user_name': tweet.author_id,\n",
    "#                    'username': tweet.author_id,\n",
    "#                    'date': created_at.date(),\n",
    "#                    'time': created_at.strftime('%H:%M:%S'),\n",
    "#                    'retweets': tweet.public_metrics.get('retweet_count', 0),\n",
    "#                    'quotes': tweet.public_metrics.get('quote_count', 0),\n",
    "#                    'likes': tweet.public_metrics.get('like_count', 0),\n",
    "#                    'hashtags': ','.join([hashtag['tag'] for hashtag in tweet.entities.get('hashtags', [])]) if tweet.entities else '',\n",
    "#                    'mentions': ','.join([mention['username'] for mention in tweet.entities.get('mentions', [])]) if tweet.entities else '',\n",
    "#                    'urls': ','.join([url['expanded_url'] for url in tweet.entities.get('urls', [])]) if tweet.entities else '',\n",
    "#                    'reply_to_tweet_id': tweet.referenced_tweets[0]['id'] if tweet.referenced_tweets and tweet.referenced_tweets[0]['type'] == 'replied_to' else None,\n",
    "#                    'reply_to_username': None,  # Placeholder for reply_to_username\n",
    "#                    'location': tweet.geo['place_id'] if tweet.geo else None,\n",
    "#                    'twt_language': tweet.lang,\n",
    "#                    'twt_source': tweet.source,\n",
    "#                }\n",
    "#                tweets_data.append(tweet_info)\n",
    "#\n",
    "#           # Convert list of dictionaries to DataFrame\n",
    "#            term_df = pd.DataFrame(tweets_data)\n",
    "#\n",
    "#            # Append to the main DataFrame\n",
    "#            all_tweets_df = pd.concat([all_tweets_df, term_df], ignore_index=True)#\n",
    "\n",
    "#        # Delay to avoid hitting rate limits\n",
    "#        time.sleep(15)  # Adjust the delay as needed\n",
    "\n",
    "#    except tweepy.TweepyException as e:\n",
    "#        print(f\"An error occurred while retrieving tweets for term {term}: {e}\")\n",
    "\n",
    "# After collecting all tweets, assign it to raw_df\n",
    "#raw_df = all_tweets_df\n",
    "\n",
    "# Display the collected DataFrame\n",
    "# print(raw_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2334e839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d4fec1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8792028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7625e320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
